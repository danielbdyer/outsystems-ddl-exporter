# The OutSystems Developer's Playbook for SQL Server DevOps

## Executive Summary

**Our Goal:** Empower OutSystems developers to professionally manage and deploy external SQL Server database schemas using modern **"database-as-code" DevOps practices**[\[1\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=schema%20of%20the%20target%20database,schema%20reach%20the%20latest%20version). This means treating database schema like application code-under version control, continuously integrated, and safely deployed through automation.

**The Problem:** OutSystems excels at managing its own internal database, but integrating with external, complex SQL Server databases often relies on manual SQL scripts and ad-hoc changes. This old approach is slow, error-prone, and doesn't scale in high-change environments. In fast-paced ("high churn") projects with tight deployment windows, manual schema updates become a serious risk.

**The Solution:** Adopt a **professional DevOps workflow for databases**. You will learn to use **SQL Server Data Tools (SSDT)** in Visual Studio to define and version your database schema as code (a database project). Changes will be compiled into a **DACPAC** artifact - a packaged schema - which can be automatically built and deployed via CI/CD pipelines. The DACPAC deployment process will **compare the desired schema to the target database and apply only the differences** needed[\[2\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=excels,schema%20reach%20the%20latest%20version), greatly reducing human error. This guide provides a roadmap from fundamentals to advanced topics like multi-table refactoring and automated testing, so you can handle anything from simple column additions to complex schema overhauls with confidence.

**Who This Is For:** OutSystems developers (and tech leads) who need to modify an external SQL Server 2019 database schema and want a reliable, repeatable process. No prior database DevOps experience is assumed - we'll cover activities from entry-level tasks to expert-level responsibilities. By following this playbook, even those unfamiliar with SQL Server administration or SSDT will be able to contribute and deploy changes without constant hand-holding.

**Core Tooling:** We will use a set of industry-standard tools: - **Visual Studio 2019/2022 + SSDT:** Visual Studio with the SQL Server Data Tools workload is your primary IDE for database development. SSDT lets you manage a database schema as a Visual Studio project composed of .sql files[\[3\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=But%20SSDT%20is%20in%20a,as%20very%20robust%20and%20powerful). You get offline editing, IntelliSense, error checking, and a _declarative_ approach (you define the end state, not the manual ALTER scripts)[\[4\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=Offline%20Database%20Development). - **SSDT Database Project & Schema Compare:** You will import the existing database into an SSDT project. The project structure organizes objects into folders and a .sqlproj file. SSDT's Schema Compare tool allows you to compare your project to a live database (or DACPAC) and visualize differences, similar to a code diff[\[5\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=The%20schema%20comparison%20tooling%20enables,that%20has%20the%20same%20effect)[\[6\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=Functionality). This is invaluable for review and ensuring your changes are correct before deployment. - **DACPAC:** A Data-tier Application Package (_.dacpac_) is the compiled output of the SSDT project - essentially a single file containing the entire database schema model[\[7\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=What%20is%20DACPAC%3F). Think of it as the DLL of your database. Deploying a DACPAC uses a **state-based** approach: the deployment engine (e.g. SqlPackage or Azure DevOps task) compares the DACPAC's schema to the target database and **automatically generates a script of necessary changes** to make the target match the source[\[1\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=schema%20of%20the%20target%20database,schema%20reach%20the%20latest%20version). This differential script deployment is efficient and consistent across environments, ensuring no drift and no missed steps[\[8\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Key%20feature%20of%20DACPAC%20deployment,and%20error%20free%20as%20possible). - **Azure DevOps Pipelines (CI/CD):** We leverage Azure DevOps for automation. A Continuous Integration (CI) pipeline will build your database project on each commit (producing a DACPAC artifact), and a Continuous Deployment (CD) pipeline will deploy that DACPAC to target databases (Dev, Test, etc.) in an automated, controlled manner. Azure DevOps has built-in tasks to deploy DACPACs, making this integration seamless[\[9\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=,a%20form%20and%20you%E2%80%99re%20done). _(Note: If your team uses other CI/CD platforms, similar automation can be achieved, but this playbook will illustrate with Azure DevOps for concreteness.)_ - **OutSystems Integration Studio:** This OutSystems tool remains part of the workflow, but _only after_ the database schema is deployed. You will use Integration Studio to refresh the external database entities (bringing in any new columns/tables into the OutSystems extension) and then publish the updated extension. This ensures OutSystems knows about the latest schema changes so your applications can use them. _The key principle:_ **Database changes are deployed first via the DACPAC, then OutSystems is refreshed to adapt to those changes** - never the reverse.

By the end of this guide, you will be treating database schemas with the same rigor as application code: version-controlled, peer-reviewed, automatically built, and safely deployed. This enables faster changes (even multiple database iterations per day if needed) with far less risk, which is crucial for high-churn projects. Let's dive into the roadmap that will take you from novice to pro in SQL Server DevOps. ðŸš€

## Learning Roadmap: From Novice to Pro

We've outlined a three-stage learning path. Each stage represents a progression of skills and responsibilities, from the fundamentals a junior developer might handle up to the advanced practices a senior developer or team lead would adopt. **Focus on mastering each stage's concepts and tasks before moving on.** In a high-change environment, solid fundamentals are critical - rushing ahead without them can lead to mistakes. Use this roadmap to gradually build your confidence and capability.

### Stage 1: Foundation (Weeks 1-2) âœ”ï¸

**Goal:** Grasp core concepts and set up your first database project in SSDT. By the end of Stage 1, you will have a database project in source control, a DACPAC built, and a basic schema change deployed manually to a dev environment. This is the groundwork upon which everything else is built.

**Key Activities and Skills:** - **Set Up Your Tools:** Install Visual Studio (2019 or 2022) with the **SQL Server Data Tools (SSDT)** workload. Also ensure you have access to a SQL Server 2019 instance (it can be a local Developer Edition or a shared dev server) and appropriate permissions to create and modify a database. _If you're new to SSDT, Microsoft's tutorial on creating a SQL database project is a great starting point_[_\[10\]_](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=The%20development%20cycle%20of%20a,local%20development%20without%20additional%20effort)_._ - **Create Your First SQL Database Project:** In Visual Studio, use _File > New > Project > SQL Server Database Project_. Give it a name (it doesn't have to match the DB name) and set the target platform to **SQL Server 2019** (or the version of your target database) in the project properties. SSDT will create a new project with a .sqlproj file. At first, it will be empty aside from the Properties and References folders. - **Import an Existing Schema:** Since we're dealing with an existing external database, use SSDT's **Schema Import** or Schema Compare feature to bring the current database schema into your project. Right-click the project and choose _Import_ > _Database_, or start a Schema Compare (set the source as the live dev database and the target as your empty project)[\[11\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=We%20start%20our%20project%20by,using%20the%20schema%20comparison%20tools)[\[12\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=Select%20File%2C%20New%2C%20then%20Project). This will generate .sql files in your project for each schema object (tables, views, stored procedures, etc.). Organize these files in a logical folder structure (e.g., by schema and object type) for easier navigation - SSDT can do this automatically during import (choose the option to organize by _Schema\\ObjectType_)[\[13\]](https://joeplumb.com/blog/sql-server-data-tools-a-deeper-look/#:~:text=import%20a%20data%20model%20from,I%20prefer%20the%20Schema%2FObject%20option)[\[14\]](https://joeplumb.com/blog/sql-server-data-tools-a-deeper-look/#:~:text=Schema%20name%20%28e.g.%20dbo%29%20%7C,Security%20Policies). - **Understand the Project Structure:** Open the Solution Explorer to see the imported objects. Each table will have its own .sql file, as will other objects. There's also a central .sqlproj file that lists all contents and settings. Note that no actual data is included - only schema. The project is **declarative**: the .sql files describe the desired end state of each object (e.g., a table definition with all columns). You do **not** write ALTER statements here; SSDT figures out how to alter the real database to match your definitions. - **Build the Project:** Hit **Build** (or press F6). SSDT will validate all the scripts and produce a DACPAC file (check the bin\\Debug or bin\\Release folder for a .dacpac). Building ensures that your schema scripts are syntactically correct and that references between objects resolve. For example, if a view references a table that doesn't exist in the project, you'll get an error (or a warning if it's a cross-database reference). Fix any errors (such as unresolved references by adding database references or dummy definitions as needed) until the project builds clean. Once built, congratulations - you have your first DACPAC, which is a portable schema snapshot. - **Source Control - Single Source of Truth:** Initialize a new Git repository for this database project (if one doesn't exist already). Commit **all** the files: .sqlproj, the .sql files for objects, any reference .dacpac files if you added them, etc. This repository now becomes **the source of truth for the database schema**. Going forward, _no schema changes should be made directly to the database_. Every change goes through the project and Git. This discipline is the cornerstone of database DevOps: the repo represents what the database _should_ look like. (If an emergency fix is made on the live DB, it must be reverse-engineered into the project ASAP - but try to avoid that situation.) - **Make a Simple Schema Change:** As a practice, perform a basic schema modification through SSDT: - Open a table's .sql file (e.g., a Users table). Add a new column, say Nickname VARCHAR(50) NULL (making it nullable so it's a non-breaking change for now). - Save the change and **Build** the project again. SSDT's build will catch any mistakes (e.g., if you added a NOT NULL column without a default, build would warn about possible data issues). - **Schema Compare & Review the Change:** Use Visual Studio's Schema Compare to see what your change would do in the database. Set the source as your project and the target as the dev database, then compare[\[5\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=The%20schema%20comparison%20tooling%20enables,that%20has%20the%20same%20effect)[\[15\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=The%20differences%20between%20source%20and,any%20of%20the%20following%20options). The Schema Compare result will show an action to add the new column on the target. This GUI diff is a critical safety step - it lets you review the exact script SSDT would apply. You should see an ALTER TABLE ... ADD \[Nickname\] ... statement. If instead it showed a drop-and-recreate of the table (which would be alarming because it would risk data loss), you'd know something is wrong. In our simple add-column case, it should be a safe alter. (We'll later cover how SSDT handles more complex changes like drops or renames safely using options and refactor logs.) - **Deploy the Change (Publish from VS):** Now deploy your change to the development database. In Solution Explorer, right-click the project and choose **Publish** (or use Schema Compare's _Update_ option). You'll get a Publish dialog where you configure the target connection (dev DB) and can view the generated script. Review and then execute the publish. SSDT will apply the new column to the dev database. Verify in SQL Server (using SQL Server Management Studio or VS's SQL Object Explorer) that the column appeared. Alternatively, you can generate a publish script and run it manually if you prefer. Either way, you've just performed your first schema change via SSDT! - **Refresh OutSystems Integration:** Now that the external database schema (Dev environment) has the new column, open **OutSystems Integration Studio**. Locate the Extension that corresponds to this external database. Use the _Refresh Entities_ wizard to fetch the updated schema. You should see the new Nickname column detected for the Users entity. Complete the wizard to update the entity definition. Then **Publish** the extension. This updates the OutSystems platform with the new database schema metadata. Finally, in OutSystems Service Studio, refresh the dependencies in any applications that use this extension so the new column is available in those app modules. (If you skip this, your OutSystems app might break or not see the new column - always update the extension after a DB change. OutSystems 11 allows refreshing an entity's definition easily in Integration Studio[\[16\]](https://success.outsystems.com/documentation/11/integration_with_external_systems/extend_logic_with_your_own_code/managing_extensions/define_extension_entities/refresh_an_entity/#:~:text=Refresh%20an%20Entity%20,table%20information%20from%20the%20database).) - **Celebrate and Communicate:** You've gone through the cycle of making a change in code, deploying to DB, and syncing OutSystems. Commit your project changes (the added column script) to Git with a message like "Add Nickname column to Users table." This is a small but important victory - you are now controlling the schema in a maintainable way. At this stage, a junior developer should feel comfortable making and deploying simple changes under guidance. A dev lead should ensure the newbie's changes are reviewed (maybe via a pull request) and that they follow standards (e.g., naming conventions, correct data types, etc.).

**By the end of Stage 1**, you have a foundational understanding of SSDT and database-as-code: - You know how to **import an existing database** into an SSDT project. - You can **navigate the project structure** and understand that each object is represented by a script. - You have created a **DACPAC** by building the project, and understand it's a versioned package of the schema. - You practiced a full cycle: edit schema in VS â†’ build â†’ compare â†’ publish to DB â†’ refresh OutSystems. - Your schema is now in **Git**. This means you can track changes over time, revert if needed, and collaborate with others without stepping on each other's toes (merging text files is easier and safer than merging manual SQL scripts!).

_Study Resources for Stage 1:_ To reinforce these concepts, consider the following free resources: - **Official Tutorial - "Create and deploy a SQL project" (Microsoft Docs):** A step-by-step walkthrough that covers creating a new project, adding objects, building, and even setting up a simple pipeline[\[10\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=The%20development%20cycle%20of%20a,local%20development%20without%20additional%20effort). - **Video - "SSDT Tutorial - SQL Server Data Tools" by Extern Code:** A 2-hour YouTube course covering SSDT setup, creating projects, importing a database, publishing changes, previewing differences, refactoring, and more[\[17\]](https://www.classcentral.com/course/youtube-ssdt-tutorial-sql-server-data-tools-140991#:~:text=Coursera%20Plus%20Annual%20Sale%3A%20All,Off%21%20Grab%20it). Great for seeing the tools in action. - **Documentation - "What Are SQL Database Projects?" (MS Learn):** Overview of the benefits of project-oriented database development, explaining offline schema editing and how it fits into CI/CD[\[4\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=Offline%20Database%20Development). - **OutSystems Documentation - "Integrate with an External Database":** OutSystems guide on using Integration Studio to connect to external DBs and how to refresh entities after schema changes. This ties the database-dev process back into the OutSystems platform.

With the fundamentals in place, you're ready to remove the _manual_ element from deployments. Stage 2 will focus on automation - making deployments repeatable and hands-off.

### Stage 2: Automation (Weeks 3-4) ðŸ¤–

**Goal:** Eliminate _"right-click deploy"_ and other manual steps by introducing automated build and deployment pipelines. In Stage 2, you'll set up Continuous Integration (CI) to build the DACPAC on each commit, and Continuous Delivery (CD) to deploy it to environments (Dev, Test, etc.). You'll also implement environment-specific configurations and approvals to ensure safe promotions. The outcome: any developer's committed change can flow to a database reliably, and no one is running SQL scripts by hand anymore.

**Key Activities and Skills:** - **Set Up CI Pipeline (Build Automation):** In Azure DevOps (or your chosen CI tool), create a build pipeline for the database project. This pipeline should trigger on pushes to your main branch (or whichever branch you integrate to). Its job: **compile the .sqlproj and produce a DACPAC artifact** on each change. In Azure DevOps, you can use a YAML pipeline or the Classic editor: - Include a step to use MSBuild or the VisualStudioBuild task to build the project. Make sure to include the argument /p:NetCoreBuild=true if using Azure DevOps hosted agents (this ensures the SSDT build targets run correctly on the build server). - After building, add a step to publish the DACPAC as a pipeline artifact (so it can be downloaded or used in release pipelines). For example, copy the \*.dacpac from the build output folder to the staging area, then use the PublishBuildArtifacts task. Name the artifact something like "DatabaseDacpac". - The result: whenever a change is pushed, the pipeline will compile the project. If there's a syntax error (say a dev committed a broken script), the build will fail - catching the problem early. If it succeeds, you get an updated DACPAC file that represents the latest schema. This becomes the input to deployment.

_Why CI?_ This guarantees that your project is always in a _buildable state_. It also means the DACPAC is consistently produced in a controlled environment, eliminating "it works on my machine" issues. Every team member sees quickly if their commit broke the database build, and they can fix it before it ever gets deployed.

- **Set Up CD Pipeline (Deployment Automation):** Now create a release (CD) pipeline that takes the DACPAC artifact and deploys it to a target database. In Azure DevOps, set it to trigger automatically after the CI pipeline succeeds (for the Dev environment), or use YAML multi-stage pipelines to combine build and deploy. Key steps and practices:
- Use the **SqlAzureDacpacDeployment** task (despite the name, it works for on-prem SQL Server too, or use the generic SqlPackage command-line) to deploy the DACPAC. You'll supply connection details for the target database. For on-prem SQL, you might use a SQL authentication (username/password) or integrated security via a build agent-store credentials securely in the pipeline.
- **Variable Groups for Secrets:** Configure a Variable Group in Azure DevOps to hold environment-specific settings like SqlServerName, DatabaseName, SqlUser, SqlPassword. Mark secrets as hidden. The pipeline can use these variables so that, for example, in Dev stage it deploys to DEVSERVER\\SQLINSTANCE and in Test stage to TESTSERVER\\SQLINSTANCE, with appropriate credentials, without hard-coding anything.
- **Dev Deployment:** The first stage of CD deploys to Dev automatically on each build. This keeps the dev database in sync with the latest code without manual intervention. After deployment, you might include a script or manual step to notify the OutSystems devs to refresh the extension if needed (in practice, as a developer you'll probably refresh it yourself as you work).
- **Test (or QA) Deployment with Approval:** Set up a second stage in the pipeline for the Test environment. This stage should _not_ auto-deploy without oversight - typically you'd put a _pre-deployment approval_ on it. That means after Dev deployment is successful, someone (e.g., the tech lead or QA) reviews the changes and approves the promotion to Test. The pipeline then uses the same DACPAC artifact to update the Test database. Using the same artifact ensures the schema is identical to what was in Dev.
- You can extend this pattern to a Prod stage with even stricter controls (approvals, scheduled deployment windows, etc.), depending on your release processes.

**Important DACPAC Deployment Options:** When configuring the deployment task (or SqlPackage.exe), there are several parameters (publish profile settings) you should use for safety: - **BlockOnPossibleDataLoss = True:** This setting **fails the deployment** if it detects a change that could cause data loss[\[1\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=schema%20of%20the%20target%20database,schema%20reach%20the%20latest%20version)[\[8\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Key%20feature%20of%20DACPAC%20deployment,and%20error%20free%20as%20possible). For example, dropping a column or shrinking a column's data type might orphan or truncate data. With this on, the pipeline will stop and report an error instead of blindly executing a destructive change. This is your most important safety net in automation. If a deployment fails with this error, it's telling you _"Hey, this change could lose data - handle it carefully (perhaps with a migration script) before proceeding."_ In practice, you'd then add a pre- or post-deployment script (see Stage 3) or do a phased change to avoid data loss, and redeploy. - **DropObjectsNotInSource = False:** This ensures the DACPAC deployment **will not drop objects** in the target that aren't in your project. In other words, if someone manually added a table in the database that isn't in source, the deployment by default won't drop it. This is a safer default to prevent accidental deletions of data. Only set this to True (allow drops) if you explicitly want to clean up deprecated objects _and_ you're sure they are safe to remove. Often teams keep it False to err on caution. - **IncludeCompositeObjects = True:** This includes things like users and permissions in the deployment if they are in the project. Set as needed, but if you manage users/roles in SSDT, this ensures they deploy.

In Azure DevOps, these can be supplied in the Additional Arguments of the DACPAC task, for example:  
/p:BlockOnPossibleDataLoss=True /p:DropObjectsNotInSource=False /p:IncludeCompositeObjects=True

- **Pipeline Outcome:** After these are set up, try the full automated path:
- Developer pushes a commit (e.g., adding a new table or modifying a procedure) to the main branch.
- CI pipeline triggers, builds the SSDT project, and outputs a DACPAC (or fails and alerts if there's a build error).
- CD pipeline automatically takes the DACPAC, deploys to Dev. The developer (or pipeline) can then verify the Dev database is updated. Integration Studio can be refreshed to pull in changes to OutSystems if needed.
- When ready (maybe after some testing in Dev or just immediately if following trunk-based development), approve the Test stage. The pipeline deploys the same DACPAC to Test.
- Eventually, with proper change management, the same artifact can be deployed to Production, ensuring what you tested is exactly what you release.

At this point, **manual deployments are eliminated**. No more applying scripts by hand or developers directly updating tables in SSMS. Everything goes through the pipeline which is consistent and logged. This dramatically reduces errors and speeds up the process - e.g., a new column addition goes from taking days of coordination to maybe an hour from commit to deployed.

- **Integration Studio Post-Deployment:** It's worth reiterating the OutSystems part in automation. You might automate a reminder or even a step to update the OutSystems extension. Full automation of Integration Studio is not straightforward (there's no simple API to refresh and publish an extension, it's a manual step). The typical process: after the pipeline updates the external DB, a developer opens Integration Studio to refresh & publish. In a future improvement, you could script Integration Studio via CLI (it has some automation capabilities) or use OutSystems Lifetime Deployment if the extension is packaged as part of an application - but those are advanced. For now, the key is to **always refresh the extension after a DB deployment** before users access new schema changes in OutSystems. Forgetting this will cause runtime errors in OutSystems since its cached entity definitions are out-of-date (e.g., your OutSystems module might try to insert into a table without the new column unless you refresh).
- **Security and Permissions:** Make sure the service account running the pipeline (or the credentials you provided) have the necessary rights on the target databases. It should have _db_owner_ or similar privileges for the deployment to create/alter objects. If you hit permission issues (e.g., the pipeline logs show a failure executing CREATE TABLE due to lack of rights), you'll need to grant the appropriate role to that user on the databases. Store credentials securely (Azure DevOps Variable group secrets or a service connection if targeting Azure SQL).
- **Branching Strategy and Collaboration:** At this stage, multiple developers might be making changes. Decide on a branching model. A common approach is **feature branches** and a **Pull Request (PR) workflow** into main. Developers create feature branches for their changes, test locally (perhaps even have a personal dev DB to publish and ensure it works), then push and open a PR. The CI pipeline can run on PRs too for early feedback. A senior developer or DBA reviews the PR (checking for correctness and best practices as we'll outline in Governance). Once approved and merged to main, the CI/CD will deploy to shared environments. This ensures every change is reviewed and no one is directly committing to main without oversight. In a high-churn setting, good coordination is needed to avoid merge conflicts in the schema (e.g., two people editing the same table file). Git can handle simultaneous edits, but communicate within the team to avoid stepping on each other's migrations. Using work item links or ticket numbers in commit messages can help trace _why_ a change was made when reviewing history.

**By the end of Stage 2**, you achieve a significant DevOps milestone: - Your database schema changes are **built and tested automatically** - ensuring quality (at least build quality) on every commit. - Deployments to dev/test are **fully automated**. This means faster iteration (developers can focus on coding, not deployment logistics) and more reliable releases. - The groundwork for multi-environment continuity is laid: the schema in each environment is derived from the same source, reducing "It works in dev but not in test" issues. - Developers start to think of database changes in the same way as code changes: something that goes through commit, CI, test, deploy. This cultural shift is important. It also means if a deployment fails, you examine it like a failing build - figure out the issue (maybe a breaking change or a needed data migration) and fix it in code, then redeploy. No hotfixing the DB outside the pipeline.

_Study Resources for Stage 2:_ - **Official Docs - "Deploy to Azure SQL Database" (Azure Pipelines):** Although targeted at Azure SQL, this Microsoft Learn article shows using DACPAC in pipelines[\[18\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=Deploy%20to%20Azure%20SQL%20Database,deploy%20schema%20changes%20and%20data), which is analogous to on-prem. It provides examples of pipeline definitions. - **Blog - "CI/CD of SSDT Projects" series by Gavin Matthews:** A practical guide to setting up Azure DevOps pipelines for SSDT projects (Part 2 focuses on pipelines)[\[19\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=CI%2FCD%20of%20SSDT%20Projects%3A%20Part,project%20template%20and%20Azure%20DevOps). It offers insights on YAML configuration. - **Eitan Blumin's Webinar Part 3 - SSDT Tools and Features for DB DevOps:** Covers how to use Visual Studio and Azure DevOps together, with demos of pipeline setup for SSDT. (Refer to the recorded sessions listed on Eitan's blog[\[20\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=In%20case%20you%20didn%E2%80%99t%20know,CI%20and%20CD%20with%20SSDT)). - **YouTube - "Using Azure DevOps for SQL Databases with SSDT":** A video walkthrough on setting up build and release pipelines for SSDT database projects[\[21\]](https://www.youtube.com/watch?v=ObgY4XB0hHo#:~:text=Using%20Azure%20DevOps%20for%20Microsoft,further%20environments%20while%20maintaining), helpful to visualize the process end-to-end.

Having automated the pipeline, you are now deploying changes faster and more safely. But automation alone doesn't cover complex scenarios - like what if a change _does_ involve data movement or a risky refactor? How do we handle review and governance at scale? That's where the mastery stage comes in.

### Stage 3: Mastery (Month 2+ onward) ðŸ†

**Goal:** Tackle complex schema evolution and establish robust team practices. In Stage 3, you build upon the automation foundation to handle scenarios that require more finesse - e.g., changes that need data migrations, multi-step refactorings across many tables, or coordinating big bang releases. You'll also implement quality gates like automated testing and formalize governance (code review standards, drift monitoring, etc.). This is about ensuring **long-term, scalable operations** of your database DevOps process, even as the team and database grow.

**Key Activities and Skills:** - **Pre-Deployment and Post-Deployment Scripts (Data Motion):** SSDT projects allow you to include special scripts that run **before or after** the main deployment changes. These are crucial for handling data migrations or any content adjustments that pure schema comparison can't handle. For example: - _Scenario:_ You need to drop a column that's no longer used. If you simply remove it from the project, the DACPAC deployment (with BlockOnPossibleDataLoss=True) will refuse to drop it because it could have data. The safe approach might be to migrate that data or handle it in steps. - _Scenario:_ Splitting a single column into two, or a table into multiple tables (a **multi-table refactor**). The deployment can add new tables/columns, but moving the existing data into the new structure is up to you. - To address these, you write a **Pre-Deployment script** (runs before the DACPAC schema diff) or **Post-Deployment script** (runs after). In SSDT, you add these via _Add > Script > Pre-Deployment Script_ (or Post) which creates (for example) a Script.PreDeployment.sql file in the project[\[22\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17#:~:text=In%20Solution%20Explorer%2C%20right,Deployment%20Script). This file will be packaged into the DACPAC and executed at deploy time. - Use Transact-SQL in these scripts to manipulate data. **Important:** make these scripts **idempotent and safe to rerun**. The deployment may be done multiple times (especially in dev/test, or if something fails and you rerun). So, check for conditions before doing actions. For instance, in a post-deploy script for a column split, you might do:

PRINT 'Migrating FullName into FirstName and LastName';  
IF EXISTS (SELECT 1 FROM sys.columns  
WHERE Name = N'FullName' AND Object_ID = Object_ID(N'dbo.Users'))  
BEGIN  
\-- Only do this if the old FullName still exists (meaning this is the first run)  
UPDATE dbo.Users  
SET  
FirstName = LEFT(FullName, CHARINDEX(' ', FullName) - 1),  
LastName = SUBSTRING(FullName, CHARINDEX(' ', FullName) + 1, 8000)  
WHERE FullName LIKE '% %'; -- simple split on first space  
PRINT 'FullName data migrated to FirstName, LastName.';  
END

This example migrates data from a FullName column into new FirstName and LastName columns[\[19\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=CI%2FCD%20of%20SSDT%20Projects%3A%20Part,project%20template%20and%20Azure%20DevOps). The SSDT schema changes (in the project) would be: add FirstName and LastName columns, and remove FullName. When deploying, because FullName was removed, the DACPAC will try to drop that column. With BlockOnPossibleDataLoss, it would block _unless_ we handle it. Our post-deploy script moves the data out and after that, dropping the column is acceptable. The order of execution is: _Pre-deployment script -> Schema changes -> Post-deployment script_. But note, the schema comparison happens before pre-deploy script runs[\[23\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17#:~:text=Pre,after%20the%20deployment%20plan%20completes). In this case, since FullName is removed in the model, the deploy plan is "drop FullName". BlockOnPossibleDataLoss would block that if data exists. One strategy is to use _Pre_\-deploy to temporarily save data or disable block, but a simpler method if possible is: deploy in two steps - first deployment adds new columns (keeping old one), migrate data via post-script, second deployment (after verifying data moved) removes old column with block off or after making it safe. - **Multi-Table Refactor Example:** Suppose you want to split a large UserProfiles table into two tables, Users and Profiles, for better normalization. This is a multi-table refactor. You would likely: 1. Create the new Users and Profiles tables in the project (initially existing alongside UserProfiles). 2. Write a post-deployment script to migrate data from UserProfiles into Users and Profiles (in a transaction, perhaps). 3. Maybe in the same deployment or a subsequent one, drop UserProfiles. Dropping might be done in a pre-deploy script of the second deployment or by re-modeling the project to remove it once you've confirmed the new tables are in use. 4. Update any references (OutSystems or other) to use the new tables. This likely requires downtime or at least careful coordination if done in one shot, so often such changes are done over multiple releases (step 1-2 in one release, then apps switch to new structure, then drop old table in a later release). The key is: **SSDT can handle the _schema_ creation part easily; the _data migration_ is on you via scripts.** By planning phased deployments, you can achieve complex restructurings with minimal impact. - **Versioning Static Data:** Some tables are essentially reference data (lookup tables) that you want under source control (e.g., a list of countries or product types). SSDT doesn't inherently deploy data, but you can include INSERT/MERGE statements in post-deploy to ensure certain rows exist. For example, a post-deploy script can MERGE into a Countries table to insert new countries or update names without deleting existing ones. Keep these data scripts idempotent (e.g., use MERGE or IF NOT EXISTS checks). This way, every deployment can enforce that "seed data" is present. (Be cautious with large static data sets - hundreds of thousands of rows - including those in the DACPAC can bloat it. In such cases, consider bulk import or a separate data package. But for small lookup lists, a script is fine[\[24\]](https://dba.stackexchange.com/questions/265475/what-is-the-best-way-to-include-a-large-amount-of-configuration-data-in-a-sql-pr#:~:text=Your%20intuition%20is%20right%20here,SSDT%20project%20in%20source%20control).) - **Pre- vs Post-Deploy:** Use pre-deployment scripts for actions that must happen _before_ schema changes are applied. For example, if you plan to drop a column that has important data, a pre-deploy script could copy that data to a staging table or temporary storage. Then the deployment drops the column, then a post-deploy could move it somewhere else or clean up. Most often, post-deploy is sufficient for backfilling data into new structures after they've been created by the deployment.

- **Advanced Governance - Pull Requests & Code Reviews:** By now, your team should adopt **mandatory code reviews** for any database changes. A senior developer or designated "database custodian" should review each PR for:
- **Correctness & Safety:** Does the change do what the user story says? Are there any potential data loss or performance issues? For example, if a column is made NOT NULL, was a default provided or a data migration included for existing records? If an index is dropped, is it truly unused?
- **Standards:** Are naming conventions followed (e.g., primary key constraints named PK_Table etc.)? Are new tables normalized properly? Are data types appropriate (using DATETIME2 instead of DATETIME, etc.)?
- **Impact on OutSystems:** If OutSystems aggregates or SQL queries are known to rely on certain fields, ensure changes won't break them (or are coordinated with OutSystems application changes). For instance, adding a NOT NULL column will break OutSystems if not given a default - because when refreshing the entity, OutSystems will expect to handle those new non-null fields. Ideally, new columns in external DB should be nullable or have defaults, so they don't require immediate changes in every consumer.
- **Indexing and Performance:** In OutSystems, heavy use of aggregates might mean adding a column used in filters requires an index. Make sure any new columns that will be frequently queried are indexed or part of an existing index. Likewise, if someone adds a huge index, review the necessity (indexes speed reads but slow writes, so balance is key).
- **Migration scripts:** If BlockOnPossibleDataLoss was disabled for a specific deployment (rare, but say we allow a drop), is there a documented migration? For example, dropping a table - was its data migrated? The reviewer should verify that pre/post scripts cover data movement where needed.
- **Testing Plan:** For complex deployments, the PR should mention how the change will be tested or any special deployment order (e.g., "run this deployment with app version X, then deploy app version Y").

By enforcing these reviews, you ensure the **bus factor** is addressed - knowledge is shared as others critique and understand the changes, and mistakes are caught early. It's especially important in databases because a mistake can mean lost data or downtime.

- **Automated Testing for Databases:** Introduce automated tests for your database logic. One popular framework is **tSQLt** - a unit testing framework for SQL Server. You can write tests in T-SQL to verify that stored procedures or functions behave as expected (for example, a proc calculates a total correctly given sample data). These tests can be included in a separate project or the same project. You can then configure your CI pipeline to run these tests against a temporary database after building the DACPAC (e.g., deploy DACPAC to a throwaway local DB, run tSQLt tests via command line). This way, if someone makes a change that breaks a business logic assumption, the pipeline catches it. Automated tests become a quality gate just like they are for application code. Start with basic tests on critical stored procs or data integrity checks. Over time build a suite. This gives developers more confidence to change the schema because they get immediate feedback if something important broke.
- **Release Management & Production Deployments:** At mastery level, you are likely involved in planning production deployments. Since Prod might have tight windows (say only Sunday night or very short downtime allowance), you need to be adept at:
- **Deployment dry-runs:** Use a staging or pre-prod environment identical to Prod to test the pipeline and scripts on realistic data. Always generate a **Deployment Report** (SqlPackage can produce an XML or JSON report of the changes) and a **deployment script** from the DACPAC for Prod, and have the DBAs or leads review it. Azure DevOps can generate this report during a release - you might include a step to generate the script and publish it as an artifact for inspection. This helps catch any surprises (like "Oh, it's going to rebuild this huge index, which might take 2 hours - we need to plan for that or adjust the change.").
- **Backups and Recovery:** Ensure backups are taken before any Prod deployment. Even with all safeguards, things can go wrong (maybe a script miscomputed something). A best practice is to do a quick backup or have point-in-time restore enabled on the database so you can rollback if needed. In automated pipelines, you might not automatically backup, but your runbook should include "DBA will take snapshot/backup before deployment".
- **Monitoring Deployments:** For production, consider enabling **data-tier application versioning**. This is a feature where you "register" a DACPAC version with the database. It can help detect drift (unapproved changes) and also allows the system to know which version is currently deployed. There's even a property /p:BlockWhenDriftDetected=True that can stop a deployment if someone made a manual change in Prod outside the pipeline[\[25\]](https://github.com/microsoft/DacFx/issues/18#:~:text=Unable%20to%20read%20data,Then%20the%20%2FAction%3ADriftReport). That forces you to reconcile that drift first. In high governance environments, this is useful. Alternatively, maintain a manual version table or use the DACPAC's built-in version (the DACPAC carries a version number you can set).
- **Schedule and Coordinate:** Work with your team to schedule the deployment during the allowed window. Communicate clearly what changes are included. Involve any stakeholders (e.g., the OutSystems ops team if they need to publish anything or take the app offline). Even though everything is automated, human coordination ensures everyone's ready to validate once it's done.
- **Drift Detection and Continuous Improvement:** Set up a process to regularly ensure that what's in source control matches what's in Prod (no drift). One approach is to run a _Schema Compare_ of the main branch DACPAC to the Prod database on a schedule (say weekly) and alert if differences are found. This could be a small pipeline that generates a drift report (using SqlPackage.exe /Action:DriftReport if the database is registered as a data-tier app, or a standard schema compare)[\[26\]](https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-deploy-drift-report?view=sql-server-ver17#:~:text=SqlPackage%20Deploy%20Report%20and%20Drift,since%20it%20was%20last). If drift is detected (meaning someone made a change in Prod outside of the normal process, or a change failed to be back-ported), treat it as a high-priority item to fix - either by applying that change to source control or resetting the database to the expected state. The ideal state: **0 drift** - all changes go through the pipeline so what's in Git and what's in Prod are in sync.
- **Team Knowledge Sharing:** Finally, at mastery level, part of your role is to **spread knowledge** and make the process self-sustaining. Create internal documentation or lunch-and-learn sessions on using SSDT, interpreting schema compare results, writing safe migration scripts, etc. Encourage less experienced devs to shadow on a complex schema change. Over time, every developer on the team should be comfortable making a schema change and understanding the pipeline feedback. This playbook itself is an example of such documentation - it should evolve as the team learns lessons. Consider creating a checklist for schema changes or a template for designing changes (including how to migrate data if needed). The more you bake good practices into the team's routine, the less reliant the process is on any one "hero" to catch issues.

**By the end of Stage 3**, you are operating at a professional level with your database DevOps: - No schema change is "too scary" - even complex refactors are planned and executed with a strategy (and possibly phased deployments). You've essentially tamed the database complexity with tooling and process. - The team has **confidence** in deployments: metrics like deployment lead time drop (you might get changes out same day) and deployment failures are near zero. And if something does fail, you have procedures to handle it (e.g., pipeline fails on data loss - you implement the fix and redeploy quickly). - There is a strong **governance model**: every change is reviewed, audited, and traceable to a requirement. Production is stable because changes are vetted. Surprises are rare. - Developers and leads now **speak a common language** about database changes, similar to application code. This cultural shift means the database is no longer a mysterious black box managed by a select few, but a transparent part of the development lifecycle. - The organization can measure improvement: for example, you might achieve 100% of external DB schema changes being deployed via the pipeline (no more direct DB edits), release frequency could increase, and outages due to schema issues drop to zero.

_Study Resources for Stage 3:_ - **tSQLt Documentation & Tutorials:** The official tSQLt site and its quickstart guide are essential for learning database unit testing. There are also Pluralsight courses and blogs on using tSQLt in CI pipelines. - **Eitan Blumin's Webinar Part 4 - Troubleshooting SSDT Errors:** Advanced session covering common issues (like circular references, dependency issues, etc.) and how to resolve them[\[27\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=Diving%20even%20deeper%20into%20SSDT%2C,how%20best%20to%20resolve%20them). A must-watch when you start hitting edge cases. - **Redgate's "DevOps & Database" articles:** While Redgate's tools differ (migration-based), their blog and free ebooks on Database DevOps discuss cultural and process considerations that are relevant even with SSDT. For example, how to get DBAs on board, how to version static data, etc. - **O'Reilly Book - _DevOps for Databases_ (2023):** A comprehensive look at applying DevOps to database management (not specific to SSDT, but covers version control, automation, backup strategies, etc.). If you have access via O'Reilly Learning, it's a good read for broader context. - **Udemy Courses:** There are courses like "SQL Server DevOps with SSDT and Azure DevOps" that simulate real-world scenarios of implementing these practices. These can be useful if you prefer a guided, project-based learning (availability may vary, and they might not be free).

Now, with the roadmap covered, let's summarize some core concepts in a concise way and then provide quick-reference guides for common tasks.

## Core Concepts Explained

Before we dive into reference guides and best practices, let's ensure the key concepts are crystal clear.

### What is SSDT (SQL Server Data Tools)?

Think of **SQL Server Data Tools** as **Visual Studio for databases**. It's an extension (or workload) for Visual Studio that enables **offline, declarative development** of SQL Server databases[\[4\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=Offline%20Database%20Development). Key points: - **Project-Based:** Instead of editing a live database, you define your schema in a Visual Studio **Database Project** (a .sqlproj). All database objects (tables, views, functions, etc.) are represented as create scripts in the project. For example, Users.sql contains the CREATE TABLE for Users. You edit these files just like you'd edit C# code - with full IntelliSense and error highlighting. - **Declarative Approach:** You specify _what_ the end state should be (e.g., "a Users table with these columns"). You do not manually write the migration script (ALTER statements) to get from the old state to new. When you build or publish, SSDT figures out the difference and builds the migration script for you. This saves time and avoids human error, because the tooling is doing a diff rather than you writing potentially complex scripts for each change. - **Offline Development & Validation:** Since everything is in files, you can build and even do a sort of "compile check" without touching a database. SSDT will catch things like syntax errors or invalid references at build time. For instance, if you rename a table in one script but a view in another script still references the old name, the build will flag an unresolved reference - preventing you from deploying broken code. This is akin to catching a compile error in code early. Working offline also means you can experiment in a sandbox without risk, then deploy once things are ready. - **Refactor and Design Tools:** SSDT has additional features like a **Refactor** menu that can rename objects safely (this creates a refactor log to instruct the deployment engine that e.g. TableA -> TableB is a rename, not a drop/create)[\[28\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=7,the%20changes%20in%20this%20session)[\[29\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=11.%20Right,regardless%20of%20the%20rename%20operation). This ensures data is preserved during renames - existing data stays intact when the table is renamed[\[30\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=10,table%20was%20renamed). There's also a table designer, schema compare tool, data generation, etc., integrated into VS. - **Free and SQL Server-Specific:** SSDT is completely free to use (comes with VS Community) and is very powerful, arguably on par with pricey third-party DB tools[\[31\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=There%20are%20numerous%20database%20development,Liquibase%20or%20DBGeni). The trade-off is it only supports SQL Server/Azure SQL. But for our use case (SQL Server 2019 on-prem), it's perfect. It's the same tech Microsoft itself uses for Azure Data Studio's SQL projects and other SQL tools, which means it's supported and regularly updated. - **State-Based vs. Migration-Based:** SSDT uses a **state-based deployment** model[\[32\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=As%20of%20today%2C%20SSDT%20is,based%20deployments%20exclusively). This means you store the desired state and let the tool compute the delta at deployment time. This is different from a migration-script approach (where you manually write "upgrade steps" for each change). State-based (DACPAC) is great for catching drift and simplifying the workflow - the downside is you rely on the tool to handle changes correctly. In practice, SSDT's engine is quite mature and handles most changes well. In cases where a direct diff would be destructive (like dropping a column), you use the pre/post deployment script as we discussed to intervene. Many teams love state-based because it's simpler to manage than a long history of incremental scripts, and it guarantees the deployment script is always for the exact target vs source (no missing scripts). It's a matter of preference, but since OutSystems itself abstracts DB changes in a state-like manner, SSDT fits that mindset (describe the end schema, not how to get there). - **Comparison and Publishing:** The SSDT project can publish directly to a database or produce a DACPAC to deploy later. Under the hood, publishing does a schema comparison (project vs target DB) and generates the necessary T-SQL statements to synchronize. If you want to see those statements, you can generate a publish script instead of applying - this is often done for review or to execute manually with DBAs. The schema compare GUI we used is essentially the same diff algorithm but with a nice interface[\[5\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=The%20schema%20comparison%20tooling%20enables,that%20has%20the%20same%20effect). - **Parallel Development:** Multiple team members can work on the project concurrently thanks to Git. However, be mindful that if two people edit the same object file, you'll have to merge like any code conflict. Strategies like feature branching and frequent integration help here. Also, using PRs with reviewers (as mentioned) ensures that one person's changes are incorporated at a time for a given object to avoid conflict.

In summary, SSDT brings the benefits of modern IDEs and version control to database development: **you edit, refactor, and validate your database code just like application code**, enabling integration into DevOps workflows smoothly[\[33\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=There%20are%20many%20migration,based%20deployments%20only).

### What is a DACPAC?

A **DACPAC (Data-Tier Application Package)** is the **compiled output** of an SSDT database project. It's a single .dacpac file that essentially contains a snapshot of your database schema (tables, columns, constraints, everything) in a binary model form[\[7\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=What%20is%20DACPAC%3F). Key things to know: - **Contents:** The DACPAC includes the definitions of all objects in a database (and optionally, can include some static data if you explicitly add data insertion scripts - though bulk data isn't included by default). It does _not_ include the actual data stored in tables (that would be a BACPAC). Think of it as a blueprint of the database. - **Analogy:** A DACPAC is to a database what a DLL or JAR is to application code. It's a **deployable unit**. You can version it (e.g., v1.3 of the schema corresponds to DACPAC file version 1.3). It can be stored and moved around as one file. In fact, you could email it or store it in artifact storage, and anyone with the right tools could deploy that database schema exactly. - **Deployment Magic:** The real power of a DACPAC is in its deployment behavior. When you deploy a DACPAC to a SQL Server instance (using **SqlPackage.exe** or Azure DevOps tasks), the tool: 1. Reads the target database's current schema. 2. Compares it to the schema inside the DACPAC (the desired state). 3. Figures out the _differences_ and creates a script to apply those differences[\[2\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=excels,schema%20reach%20the%20latest%20version). For example, if the DACPAC has a new table that the target doesn't, the script will have a CREATE TABLE for it. If a column's definition changed, it will include an ALTER for that column. Only the necessary changes are included - unchanged objects are left alone[\[1\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=schema%20of%20the%20target%20database,schema%20reach%20the%20latest%20version). 4. It then executes that script on the target (unless you ran it in "generate script only" mode). - **Safe Deployments:** Because of this diff approach, deploying a DACPAC to a slightly different target will bring it up to date without redoing things. If target already had a previous change applied, the diff sees no change needed. This means you can re-deploy the same DACPAC and it will usually report "no differences" after the first time (idempotent). It's great for ensuring consistency across multiple environments - the same DACPAC to Dev, Test, Prod will put the same schema everywhere (assuming each was at the previous version to start with). It also means if someone _did_ manually add something in one environment, the DACPAC deployment (with default options) might flag it as an extra and drop it or ignore it depending on settings. So it's another reason to avoid sneaky manual changes. - **Version Control & History:** We keep the DACPACs for traceability. For instance, you might store each release's DACPAC in a build artifact feed or a folder. That way, you can always compare version N vs N+1 or rollback if needed by re-deploying an old DACPAC (note: rollback via DACPAC only works for pure schema changes that are reversible; data migrations aren't simply reversed, so rollback is usually restore from backup or forward-fix). - **Usage with Tools:** You can deploy a DACPAC using: - **Visual Studio** publish (for one-off or dev use). - **SqlPackage.exe** - a command-line provided by Microsoft, very flexible, supports numerous parameters and actions (Publish, Script, DeployReport, DriftReport, etc.). - **Azure DevOps Pipeline Task** - which essentially wraps SqlPackage for you. - **SSMS** - SQL Server Management Studio has a Deploy Data-tier Application wizard where you can import a DACPAC. - **Third-party** - e.g., Octopus Deploy or others support DACPAC deployment. - **Limitations:** DACPACs handle schema great, but any **data changes** (beyond simple adds that can have defaults) need to be handled via the pre/post scripts or separate SQL scripts as we've discussed. Also, certain SQL Server objects like jobs or server-level objects are outside the scope of DACPAC (it's database-scoped). If your schema relies on cross-database references, you have to manage those via references or allow unresolved references (with SQLCMD variables for DB names). That's a more advanced topic: you can add "Database References" in SSDT to other databases' dacpacs to inform the build about cross-db queries. It's doable, as described in Microsoft Q&A forums[\[34\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=Under%20the%20database%20project%2C%20there,in%20your%20cross%20databases%20queries)[\[35\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=As%20you%20have%20discovered%2C%20cross,in%20SSDT%20can%20be%20complicated), but ideally minimize cross-database dependencies, or use synonyms etc., to keep things self-contained. - **DACPAC vs BACPAC:** Just to avoid confusion: a **BACPAC** is a data-tier package that includes the schema _and data_. It's used for moving databases with data (like exporting a copy of a database). We are not using BACPACs in this process because we only need schema. All our application data remains in the database and we manage it via OutSystems or SQL scripts, not source controlled as part of schema (except lookup seed data we handle as noted).

In short, a DACPAC is **your database delivered in a box**. It enables consistent, repeatable deployments and environment parity[\[36\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Some%20of%20the%20key%20benefits,SQL%20schema%20are%20as%20follows). Instead of running a bunch of SQL scripts in order, you deploy one DACPAC and it brings the database to the desired state, while respecting safety options and catching potential problems (like data loss) before they happen.

### How OutSystems Fits In

OutSystems and external databases have a specific interplay that's important to get right in this DevOps setup. Here's the typical coordinated dance between the two:

- **Deploy the Database Changes First:** Always apply schema changes to the external SQL Server _before_ updating anything in OutSystems. This is because OutSystems (via Integration Studio) will read the new schema to update its metadata. If you tried to refresh OutSystems _before_ the DB change, the DB wouldn't have the new structure and there'd be nothing to read. So, the pipeline or manual process should ensure the DACPAC is published to (for example) the Dev database _prior_ to refreshing the OutSystems extension for that DB.
- **Refresh in Integration Studio:** Once the target database (Dev, Test, etc.) is updated by the DACPAC deployment, open **Integration Studio** and load the extension (XTD) that corresponds to that external database. Use the **"Refresh Database Objects"** (or "Refresh Entities") feature. This will connect to the database and detect any schema changes - new tables, new columns, changed data types, etc. You can pick which changes to import (usually all relevant changes). Essentially, this updates the definition of the external entities in OutSystems to match the actual DB. OutSystems 11 documentation emphasizes that this step fetches updated table info from the database to update the entity definition[\[16\]](https://success.outsystems.com/documentation/11/integration_with_external_systems/extend_logic_with_your_own_code/managing_extensions/define_extension_entities/refresh_an_entity/#:~:text=Refresh%20an%20Entity%20,table%20information%20from%20the%20database).
- **Publish the Extension:** After refreshing, you must **publish** the extension module. This is like deploying a new version of your OutSystems extension that now knows about the new schema. OutSystems will then sync that information to its own system (but note: OutSystems does _not_ copy the data, it just updates the metadata of entity definitions and any CRUD logic in the extension). If the extension has logic (like stored procedures or custom queries defined), you'd also update those if needed to adapt to the new schema.
- **Update Consumer Apps:** Finally, any OutSystems applications that use that extension should be republished or at least have their references refreshed. In Service Studio, you'd open the module that depends on the extension, see that there's an updated version of the extension, refresh dependencies, and then fix any impacts (for instance, maybe a screen that showed FullName now needs to use FirstName + LastName, etc.). Then publish the application. Ideally, if the changes were additive (like adding a new column), the existing app might not break at all, but if you removed or renamed something, the OutSystems modules will show errors until fixed. This is why we coordinate: e.g., you wouldn't drop a column that OutSystems uses without simultaneously updating the OutSystems module to not use it. That often means you do a two-step: add new replacements first, get OutSystems using them, then remove the old in a later release.

In summary, the workflow per environment is: - **DB Schema deployment (via pipeline) -> Integration Studio refresh & publish -> OutSystems app adjustments & publish.**

For Dev, a developer usually does the refresh right after their change is deployed, since they control both sides (DB and app) in development. For Test or Prod, you might have different people doing the refresh, but it's usually quick (a matter of minutes). Ensure the team responsible for OutSystems deployments is aware when a DB deployment happens so they can perform the refresh ASAP. If there's an OutSystems deployment pipeline, consider adding a manual step "Refresh Extension after DB deploy" in your checklist before deploying the OutSystems application update.

One more consideration: **timing and downtime**. If you deploy a schema change that, say, drops a column that OutSystems currently expects, any OutSystems functionality using it will error out until the extension is refreshed (because the physical column is gone, and the platform doesn't know). To avoid issues: - In non-critical environments, it's usually fine (some automated tests in OutSystems might fail until refresh, but you handle it quickly). - In production, plan to have a short maintenance window where you can deploy the DB changes and immediately do the refresh and app publish. OutSystems users might see an error in the interim if they hit a changed entity. Usually we mitigate by making breaking changes in a backward-compatible way: e.g., for a rename, we _add_ new columns, update OutSystems to use them, then drop old ones later. That way, at no point does OutSystems try to use a missing field. This goes back to multi-step deployments for refactors.

In essence, OutSystems is another consumer of the database. Our DevOps process updates the database like any other application would. OutSystems doesn't magically handle external DB changes - we do, using SSDT. OutSystems just provides tools (Integration Studio) to sync up and use the new schema. **Always deploy DB first, OutSystems second** is the golden rule here.

To tie it back to goals: by following this pattern, we remove the need for manual SQL scripts in OutSystems deployments. Instead of writing ad-hoc SQL to alter external tables when an app needs a change, we incorporate it into the professional pipeline. OutSystems developers can then focus on app logic, knowing the database changes will be carried out consistently. This leads to faster delivery and fewer runtime errors (because nothing is out of sync between the platform and the database).

## Just-in-Time Task Guides

This section provides quick "how-to" guides for common tasks you will perform. These are like mini runbooks or checklists for specific scenarios, which you can refer to until the process becomes second nature. They assume you have set up the basics from the earlier sections.

### Guide A: Making a Standard Schema Change (Feature Workflow)

This guide covers the typical steps a developer (or team lead) would take to implement a new schema change - for example, adding a new table or modifying a column - and propagate it through the pipeline with proper review. This is the **day-to-day routine** in a database DevOps environment.

- **Pull Latest & Create a Branch:** Start by pulling the latest changes from the main branch of your database repo to ensure you're up to date. Then create a new **feature branch** for your work (e.g., feature/add-user-profile-table). Naming branches by feature or ticket number helps tracking. Working in a branch ensures isolation until your change is ready and reviewed.
- **Develop the Schema Change in SSDT:** Open the database project in Visual Studio. Make the necessary schema changes:
- If you're adding a new table, right-click the project (or a folder) -> **Add > New Item > Table**. SSDT will add a new .sql file with a stub CREATE TABLE statement. Define the columns, primary key, etc. (Pro tip: set up default properties like ANSI nulls as needed; SSDT uses defaults from project settings).
- If modifying an existing object, open its .sql file and edit. For example, to change a column's data type or add a new column.
- If you need to rename something, **use the Refactor** (right-click -> Refactor -> Rename) so that a refactor log entry is created[\[37\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=1.%20Right,SQL%20Editor)[\[28\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=7,the%20changes%20in%20this%20session). Check the .refactorlog file gets updated - this will inform the DACPAC deployment to treat it as a rename operation (which uses sp_rename behind scenes) rather than drop and add, thus preserving data[\[30\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=10,table%20was%20renamed).
- If your change is complex (e.g., splitting tables), consider whether you need a pre/post script as described earlier. Add those scripts and write the T-SQL logic as part of your change.
- **Build and Run Local Tests:** Build the project (F6) to catch errors. If you have a local dev database available, you can even publish the project there to try the change in isolation (optional but useful for testing a data migration script or ensuring the change works). If using tSQLt tests, run them locally to ensure nothing broke.
- **Use Schema Compare (Optional but Recommended):** Before committing, it's wise to double-check what your change would do. Schema Compare your project to a representative database (like your local copy or dev)[\[15\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=The%20differences%20between%20source%20and,any%20of%20the%20following%20options)[\[38\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=When%20the%20comparison%20is%20complete%2C,different%2C%20and%20the%20source%20name). Review the delta. This helps you catch mistakes, e.g., "Oops, I dropped and recreated a table because I renamed it without refactor - data would be lost." If anything unexpected appears, adjust your approach (maybe use refactor rename, or break the change into two deployments).
- **Update Reference Data or Other Dependencies:** If your change requires certain data present (like new lookup values for a new column's foreign key), don't forget to update those either via seed scripts or coordinating with the team that handles data. Also, if OutSystems or other apps query changed objects (like a stored proc signature change), notify or update those in tandem.
- **Commit to Git with a Clear Message:** Stage the changed files (the .sql files, any .refactorlog or .sqlproj changes, etc.) and commit with a message like "Add Profiles table and migrate User data to Profiles (Fixes #1234)." Push your feature branch to the remote.
- **Create a Pull Request:** On your Git server (Azure Repos, GitHub, etc.), open a PR to merge your feature branch into the main branch. Add reviewers (e.g., the tech lead or a colleague). In the PR description, explain the change briefly, including any **special deployment considerations**. For example: "This adds a Profiles table and splits it from Users. It includes a post-deploy script to move existing profile info. After deployment, we need to refresh OutSystems and update the Profiles screen in the app to use the new table." This context helps reviewers and ops folks.
- **Continuous Integration Kicks In:** As soon as you opened the PR (or pushed commits), the CI pipeline should run (if configured for PRs) to build the project. Ensure the pipeline passes. If it fails (maybe you forgot to add a new file to the .sqlproj, or a syntax error), fix those and push an update.
- **Address PR Feedback:** Reviewers will look at your changes. They might point out improvements (e.g., "Make that new column nullable, OutSystems might not handle non-null without default" or "Add an index for the FK"). Discuss and make additional commits to your branch as needed. The CI pipeline will re-run for each update, giving confidence that each iteration still builds.
- **Merge to Main:** Once approved, merge the PR. The main branch now contains your change. This triggers the **CI build** which produces a new DACPAC artifact, and then the **CD pipeline** which deploys it to the Dev environment automatically.
- **Verify in Dev Environment:** After the pipeline runs, check the Dev database (and maybe the pipeline logs or the Deployment Report artifact) to confirm the change was applied. For example, see that the new table exists and data was migrated if applicable. Also open Integration Studio, refresh the extension to catch the new/changed elements, and publish it. Then test in OutSystems (Dev environment) that everything works - e.g., the app can see the new table's data, no errors from missing fields, etc.
- **Deployment to Higher Environments:** If Dev tests are good, promote the change to Test (via pipeline approval) and subsequently to Prod following your release process. Often, multiple changes are bundled in a release, so this might happen at a later scheduled time. But if you're doing continuous delivery, it could be soon after. The important part for you: ensure any **manual steps** (refreshing OutSystems, updating app logic) are repeated in those environments. For instance, when deployed to Test, the OutSystems extension in Test must be refreshed/published too.
- **Post-Deployment Checks:** Monitor for any issues. Did the pipeline flag any warnings? (E.g., maybe it logged that it had to drop and recreate a default constraint - which is normal). Are there any errors in OutSystems logs due to the change? If something went wrong, you might need a hotfix or to rollback via a backup restore (if it's data related). Usually with thorough testing and pipeline gating, this is rare.

This guide ensures that a typical schema change flows through the system in a controlled, reviewable way. Over time, these steps become routine. The PR review acts as the choke-point for quality - as a lead, you'd focus attention there to catch anything the contributor might have missed.

**Key reminders for standard changes:** - Always sync with OutSystems after deploying. - Make sure every change is documented (even if just in commit/PR) for future reference. - Keep changes small and incremental when possible. Many small safe deployments are better than a giant one with huge risk. - If a deployment fails due to BlockOnPossibleDataLoss, do not blindly disable it - instead, address the root cause (e.g., write a migration script or plan a two-step change). This often turns a "failed" pipeline run into a guidance to improve your change. Embrace it, fix, and try again.

With this, most day-to-day additions and modifications are handled smoothly.

### Guide B: CI Pipeline Setup (Azure DevOps YAML)

If you are configuring the Continuous Integration build pipeline for the database project (or need to adjust it), here's an example using Azure DevOps YAML. This pipeline will compile the SSDT project and produce a DACPAC artifact on each commit to main. We include this as a reference - if your pipeline is already set up, you might not need to create it from scratch, but understanding it helps in maintenance.

\# azure-pipelines.yml for CI (Build) pipeline  
<br/>trigger:  
\- main # Run on commits to main branch (adjust if using different branch strategy)  
<br/>pool:  
vmImage: 'windows-latest' # Use a Windows agent that has Visual Studio and SSDT  
<br/>variables:  
solution: '\*\*/\*.sqlproj' # Path to the SQL project file (pattern search in repo)  
buildPlatform: 'Any CPU' # Platform and config for MSBuild, typically Any CPU/Release for SSDT  
buildConfiguration: 'Release'  
<br/>steps:  
\- task: VSBuild@1  
displayName: 'Build Database Project'  
inputs:  
solution: '\$(solution)'  
platform: '\$(buildPlatform)'  
configuration: '\$(buildConfiguration)'  
msbuildArgs: '/p:NetCoreBuild=true' # Important for building SSDT projects on Azure DevOps  
<br/>\- task: CopyFiles@2  
displayName: 'Copy DACPAC to Staging Directory'  
inputs:  
SourceFolder: '\$(Build.SourcesDirectory)'  
Contents: '\*\*\\bin\\\$(buildConfiguration)\\\*.dacpac' # find the DACPAC from build output  
TargetFolder: '\$(Build.ArtifactStagingDirectory)'  
<br/>\- task: PublishBuildArtifacts@1  
displayName: 'Publish DACPAC Artifact'  
inputs:  
PathtoPublish: '\$(Build.ArtifactStagingDirectory)'  
ArtifactName: 'DatabasePackage'  
publishLocation: 'Container'

**Explanation:** - The pipeline triggers on the main branch. You might also include PR triggers or feature branch triggers if you want CI on those. - It uses the Microsoft-hosted agent image windows-latest which includes Visual Studio. Make sure the VS version on the agent has SSDT installed. Microsoft's hosted agents typically have VS with Data Tools, so it should work. If not, you might need to add a step to install the SSDT extension or use an agent that has it. - We set variables for the solution path and config. \*\*/\*.sqlproj will find the first .sqlproj file in the repo. If you have multiple, specify the exact path. - **VSBuild@1** task runs MSBuild on the .sqlproj. The NetCoreBuild=true property is crucial nowadays as SSDT has some MSBuild targets that require it on build servers. - CopyFiles step takes the output DACPAC (found under bin\\Release typically) and copies it to the artifact staging. - PublishBuildArtifacts publishes that folder as an artifact named "DatabasePackage". After the build, you can go to the pipeline run and see the artifact (the DACPAC file inside). - This artifact is what the CD pipeline will consume to deploy.

Once this pipeline is set up and running, every commit gives you a built DACPAC. If the build fails, developers will get immediate feedback to fix their changes.

_(If using a Classic pipeline UI, the steps correspond to: Use VSBuild task -> Copy Files -> Publish Artifact with similar settings.)_

### Guide C: CD Pipeline Setup (Release Deployment)

This guide illustrates a deployment pipeline that takes the DACPAC from the CI build and deploys it to a SQL Server. In Azure DevOps, you might implement this as a Release pipeline with stages or a YAML multi-stage pipeline. Here's a conceptual example for one stage (Dev deployment), using Azure DevOps tasks:

\# Example stage in a multi-stage YAML pipeline (or use classic release)  
\# This stage deploys the DACPAC to DEV environment.  
<br/>\- stage: DeployDev  
jobs:  
\- job: DeployToDev  
displayName: "Deploy DACPAC to DEV"  
steps:  
\- download: current  
artifact: DatabasePackage # download the artifact from CI pipeline  
displayName: "Download DACPAC artifact"  
<br/>\- task: SqlAzureDacpacDeployment@1  
displayName: 'Deploy DACPAC to DEV Server'  
inputs:  
\# If targeting Azure SQL, specify an Azure service connection and remove SQL auth.  
\# For on-prem SQL, you can skip azureSubscription and use connection string authentication.  
azureSubscription: '' # (Leave blank or omit for on-prem)  
AuthenticationType: 'sqlAuthentication'  
ServerName: '\$(Dev.SqlServerName)' # use pipeline variables or variable group  
DatabaseName: '\$(Dev.DatabaseName)'  
SqlUsername: '\$(Dev.SqlUser)'  
SqlPassword: '\$(Dev.SqlPassword)'  
\# Path to the DACPAC file we downloaded  
DacpacFile: '\$(Pipeline.Workspace)/DatabasePackage/YourDatabaseName.dacpac'  
AdditionalArguments: |  
/p:BlockOnPossibleDataLoss=True  
/p:DropObjectsNotInSource=False  
/p:IncludeCompositeObjects=True

**Explanation:** - First, we **download** the artifact named "DatabasePackage" that was published by the CI stage. This gives us access to the DACPAC file in the pipeline agent's workspace. - We then use the **SqlAzureDacpacDeployment@1** task. Despite "Azure" in its name, it can deploy to any SQL Server; if not using an Azure subscription, you provide direct connection details. - In this example, we use SQL Authentication (username/password). These values (Dev.SqlServerName, etc.) would be defined in a Variable Group scoped to the Dev environment and stored securely (the password marked secret). - DacpacFile path points to the DACPAC from the artifact. \$(Pipeline.Workspace)/DatabasePackage is a default path after downloading artifact. Replace "YourDatabaseName.dacpac" with the actual DACPAC filename (it will be named after your project by default). - We pass the important AdditionalArguments as discussed: - /p:BlockOnPossibleDataLoss=True - prevents data loss changes from running automatically. - /p:DropObjectsNotInSource=False - don't drop objects that the project doesn't know about, as a safety. - /p:IncludeCompositeObjects=True - include things like permissions if present (optional, based on your project). - If deploying to Azure SQL Database, you'd use azureSubscription (service connection) and possibly a different task version or connection string. But for on-prem, the above works using SQL auth.

You would create similar stages for Test and Prod: - For **Test**, you might have: - Another job that uses \$(Test.SqlServerName) etc. for that environment's variables. - Likely a manual approval before this stage (set in environment approvals or release triggers). - For **Prod**, you'd definitely have an approval and maybe only run it on a manual trigger or when you create a release specifically.

Azure DevOps also allows using Environments and deploying via an "approve before deploy" gate, which is great for Prod.

After deployment tasks, you could add other steps, for example: - A script to run tSQLt tests (if you deploy them with the DACPAC). - A step to generate a deployment report: SqlPackage.exe /Action:DeployReport which outputs XML describing what it did. That file could be kept as evidence or attached to a release artifact. This is helpful for DBAs to review exactly what happened especially in Prod. - Notification steps (email/Teams message) to inform that deployment happened.

**Reminder:** The pipeline does _not_ handle the OutSystems Integration Studio part. That remains a manual step. You might, however, add a **Pause** or **Manual Intervention** task after DB deploy in Prod that basically reminds the operator "Refresh OutSystems Integration Studio and publish the extension now, then resume." This can be part of your runbook.

After setting up the CD pipeline, test it on a non-critical environment. Intentionally introduce a benign change and see it flow through Dev and Test. Practice an approval, etc. Make sure credentials work and the task is deploying properly. Once verified, you've automated the heavy lifting of deployment.

_(For non-Azure DevOps users, equivalent steps exist in other CI/CD tools: e.g., GitHub Actions has community actions for DACPAC or you can use a Powershell script to run SqlPackage; Jenkins can call SqlPackage; GitLab CI can use a Docker image with SqlPackage CLI, etc.)_

### Guide D: Handling Data Migrations with Pre/Post-Deployment Scripts

When your schema change involves transforming or migrating data (not just schema structure), SSDT's pre and post deployment scripts are your friends. Let's walk through a scenario and how to implement it:

**Scenario:** We have a FullName column in the Users table that we want to replace with FirstName and LastName columns. We need to **split the data** currently in FullName into the new columns.

**Plan:** This change can be done in one deployment as follows - Add new columns, migrate data, then drop old column. However, dropping the old column in the same deployment will trigger BlockOnPossibleDataLoss. We have options: (a) do it in two deployments (one to add & migrate, one to drop), or (b) temporarily disable BlockOnPossibleDataLoss for this operation if we're absolutely sure it's safe after migration. Generally, the two-step approach is safer. We'll illustrate one-step here for simplicity, but note in real life you might decide to remove the drop from the DACPAC and do it in a later release after verifying everything.

**Steps:**

- **Schema Changes in Project:**
- Open Users.sql (the table create script). Add the new columns:

- \[FirstName\] NVARCHAR(100) NULL,  
    \[LastName\] NVARCHAR(100) NULL,
- perhaps right after FullName or at the end. Do _not_ remove FullName yet if doing a single deployment (because if you remove it in model, SSDT will try to drop it during deploy).

- Alternatively, if doing a phased approach, you leave FullName in the model for now (so no drop will occur), and just add FirstName, LastName. We'll go with that: keep FullName for now.
- (If you were confident, you could mark FullName for removal - but let's keep it, drop it next time).
- **Add Post-Deployment Script:**
- In Solution Explorer, **Add > Script > Post-Deployment Script**. Name it something like Script.PostDeployment.sql (it's usually the default). SSDT will add it and set its Build Action to PostDeploy automatically[\[39\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17#:~:text=Add%20pre).
- Write the migration SQL inside this post-deploy script. For our example:

- PRINT 'Starting data migration for FullName into FirstName/LastName';  
    <br/>\-- Only migrate if FullName column exists and FirstName is currently null, to avoid repeated runs affecting already migrated data  
    IF EXISTS (  
    SELECT 1  
    FROM sys.columns  
    WHERE Name = N'FullName' AND Object_ID = Object_ID(N'dbo.Users')  
    )  
    BEGIN  
    UPDATE dbo.Users  
    SET  
    FirstName = CASE  
    WHEN FullName LIKE '% %'  
    THEN LEFT(FullName, CHARINDEX(' ', FullName) - 1)  
    ELSE FullName END,  
    LastName = CASE  
    WHEN FullName LIKE '% %'  
    THEN SUBSTRING(FullName, CHARINDEX(' ', FullName) + 1, 4000)  
    ELSE '' END  
    \-- Only update users that have not yet been split (perhaps mark by FirstName IS NULL initially)  
    WHERE FirstName IS NULL AND FullName IS NOT NULL;  
    <br/>PRINT 'User names split into FirstName and LastName.';  
    END  
    ELSE  
    BEGIN  
    PRINT 'Skipping migration: FullName column no longer exists (already migrated in a previous run).';  
    END;
- A few things to note here:
  - We check if FullName exists in the target. If someone runs this script after FullName was dropped, it will just skip (so it's safe to rerun).
  - We update each user's FirstName, LastName by splitting on the first space. This is simplistic (and you'd handle edge cases in reality), but serves the idea.
  - We only do it where FirstName is NULL to avoid overwriting if the script ran before. So multiple executions won't duplicate work.
  - We output some PRINT messages for the deployment log for traceability.
  - We didn't drop FullName here; we are handling drop via model (or in next deployment).

- Save this script. On build, SSDT will include it in the DACPAC. Note that the post-deploy script runs **after** all schema changes in the deployment.
- **Build and Test Locally:**
- Build the project. Fix any typos. Deploy to a local dev database (or the dev environment if you're confident) to test the script. Watch the output - see that the PRINTs appear and that names got split. If you run it again, ensure it doesn't do weird things (our condition on FirstName IS NULL should make it a no-op second time).
- Check that after deployment, both FullName and FirstName/LastName exist (since we didn't remove FullName yet in model). Data is duplicated (which is fine).
- This is where a decision: If you want to drop FullName in this same release, you could have removed it from the model. The deployment plan would then include "DROP COLUMN FullName". Because BlockOnPossibleDataLoss=True, the deployment would fail _unless_ you disable that or do something. One trick: you could disable BlockOnPossibleDataLoss for this deployment only (via AdditionalArguments on pipeline for this release). But that's risky if any other potentially harmful change is in there. Alternatively, do a pre-deploy script to drop FullName after copying (but pre-deploy runs _before_ adding new columns, so not ideal).
- Safer route: plan to remove FullName in a **future deployment** (Stage 2 of this refactor) once you've confirmed everything working with new columns. That future deployment would simply be a model change (delete FullName from Users table script) and maybe a pre-deploy script to clean any straggling data or just rely on block (since by then presumably data is moved, you could drop with block off).
- **Commit and Deploy via Pipeline:**
- Commit the changes (the Users.sql modified, and the new PostDeployment.sql). Push and PR as usual. The pipeline will run the post-deploy script after applying schema changes (adding FirstName, LastName). Review the deployment report - it should show actions: Add FirstName, Add LastName (and no drop of FullName since we left it), and then the script's output in logs.
- In Dev, verify data: Users table now has both FullName and the new columns filled in. OutSystems: refresh the entity - now OutSystems sees FirstName and LastName (they were added). It also still sees FullName (since we haven't removed it). At this point, OutSystems devs can start changing the app to use FirstName/LastName instead of FullName. They have both available which is good (backward compatibility for now).
- **Follow-Up (Next Deployment to Remove FullName):**
- After the app has been updated to use FirstName/LastName exclusively, you can remove the old FullName. That would be done by deleting it from the project and deploying (which will trigger the actual ALTER TABLE DROP COLUMN FullName). You would want to set /p:BlockOnPossibleDataLoss=False just for that deployment _or_ drop the column in a pre or post script explicitly once it's safe. Since you know data is migrated and no code uses it, it's safe to drop. In PR review, double-check nothing references it.
- Once dropped, don't forget to refresh OutSystems again to remove it from the entity model.

This guide demonstrates using post-deployment for one common pattern: backfill data into new structures. Pre-deployment scripts are similarly added if you ever need to, for example, modify data _before_ schema changes happen. (Less common, but imagine you were about to make a column NOT NULL and you want to fill default values into all rows before the alteration occurs - you could do that in a pre-deploy script so that when the alter runs, it doesn't fail on nulls. The deploy plan is calculated before pre-deploy runs, but the pre-deploy script executes before the alter actually executes[\[23\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17#:~:text=Pre,after%20the%20deployment%20plan%20completes), so in this scenario it wouldn't help because the plan might still see nulls - a safer method is two-step deployment or allow a default constraint to handle it.)

**Tips for Pre/Post-Deployment Scripts:** - Keep them idempotent. Use checks like IF EXISTS or track progress in a table if complex. This ensures if a pipeline re-runs or if you redeploy the same DACPAC, you don't mess up data or get errors. - These scripts are not automatically tested by SSDT (they don't run until deployment), so test them in a dev database manually. - Remember they execute in the same transaction context as the rest of the deployment by default. If a statement fails, the whole deployment may roll back. So, be careful with long-running or risky operations. You can wrap parts in transactions or use TRY/CATCH as needed. For very large migrations, sometimes breaking it into chunks or offloading outside the deployment might be necessary (this gets into advanced territory). - Document what the script does in comments - future you or others will appreciate it when maintaining the project.

By mastering pre and post deployment scripts, you gain the ability to handle **any kind of change**, no matter how complex, in a controlled release. Schema changes and data migrations become two sides of the same coin in your deployment process.

With these guides, you have play-by-play instructions for common scenarios: everyday feature work, setting up pipelines, deploying through environments, and handling tricky changes. Next, we'll solidify some governance best practices and success metrics to aim for, which will help ensure the process we've built remains healthy and effective.

## Governance & Best Practices

Adopting database DevOps is not just a technical implementation; it's also about enforcing the right practices consistently. Below are **golden rules and best practices** that your team should follow to maintain order and quality as you manage SQL Server schema changes in an OutSystems context:

- **Single Source of Truth - The Git Repository:** Perhaps the most important rule: **the database project in source control is the only official representation of the schema.** No one should make ad-hoc changes directly on the database, no matter how minor, without going through the SSDT project and pipeline. If an emergency fix is absolutely needed on Prod, treat it as a temporary hotfix and ensure the change is reverse-integrated into the project immediately (otherwise the next deployment will overwrite or drop the hotfix). This discipline prevents "configuration drift" where the code and actual DB diverge.
- **Pull Request Reviews are Mandatory:** Every schema change should be done via Pull Request, with at least one other knowledgeable developer or DBA reviewing. This is your chance to catch issues:
- _Validate the business need:_ Is this change tied to a user story or ticket? Unexplained changes can indicate someone tinkering without proper requirement.
- _Verify correctness:_ Is the change doing what is intended and only that? No accidental extra changes (like unintended drops because someone removed an object from the project by mistake).
- _Nullability and Defaults:_ If new columns are added, are they nullable? If not nullable, was a default value provided or a script to populate existing rows? We don't want deployments failing or, worse, adding NOT NULL columns that break inserts.
- _Indexing:_ For any new table or column likely to be filtered or joined on, does it have an index or foreign key index as needed? OutSystems often will create queries that might benefit from indexing on foreign keys or aggregate filters. Conversely, check that any new index isn't redundant or overly expensive.
- _Constraints & Relationships:_ Ensure foreign keys and other constraints are included as appropriate, and named consistently. Also check that any changes to constraints (like making a FK cascade on delete) are intentional.
- _Security & Permissions:_ If your project includes users/roles or uses SQL security objects, review those changes carefully. In an external DB, OutSystems typically connects with one user, but if you have other apps, make sure permissions exist for new objects as needed.
- _Data Migration Plan:_ For destructive or complex changes, is there a clear plan (and script) for handling data? E.g., if dropping a column, is it truly unused or did we migrate needed data elsewhere? If splitting tables, did we cover migrating the data and updating the app logic accordingly?
- _OutSystems Impact:_ Think through how the change affects the OutSystems environment. Will this require immediate changes in eSpace logic? Is the team prepared to update the OutSystems side at deployment time? For example, if we rename a table, the integration studio will treat it as a drop and add of a new entity - which will break all references in OutSystems. We might decide to instead add new and deprecate old to phase it out. These considerations should be discussed in the PR.
- _Standards Compliance:_ Ensure naming conventions and coding standards for SQL are followed (like PascalCase or snake_case as your project uses, abbreviation guidelines, etc.). Consistency helps maintainability.
- **Use Branching Strategy (e.g., GitFlow or Trunk-Based) Appropriately:** Decide as a team how you handle releases. If you use a long-lived main branch that's always production-ready, then feature branches -> PR -> main works well (possibly with release tags). Some teams might use a dev branch and production branch, etc. The key is to integrate frequently to avoid huge merges and to test changes in an integrated environment early (continuous integration). Also, avoid having one developer's changes sitting undeployed for too long; stale changes can cause merge conflicts and surprises.
- **Maintain an Updated** README **or Docs:** Document how to set up the environment (which VS version, etc.), how to run the pipeline, and any conventions specific to your project. New team members will benefit from this. Include instructions on common tasks (some of which we covered in the guides).
- **Enforce Pipeline Quality Gates:** We've already implemented some: build must succeed or no deployment. You can add others:
- **Database Unit Tests:** If using tSQLt, configure the CI to fail if any tests fail.
- **Static Code Analysis:** Consider using a SQL code analysis tool or SSDT's Code Analysis (if available) to catch things like SELECT \* usage, or deprecated syntax. This can be part of build.
- **Deployment Report Review for Prod:** Have a rule that someone (DBA or lead) must review the Prod deployment script if it's a major release. This isn't an automated gate but a procedural one.
- **Regularly Monitor for Drift:** As mentioned, run a **schema drift check** at least weekly or before any release on Prod. SqlPackage's DriftReport action requires the database to be registered as a data-tier application (which you can do via SSMS one-time). Alternatively, run a schema compare between the last known good DACPAC (from last release) and the Prod database. If you find drift (someone changed something in Prod outside the pipeline), investigate immediately. The golden rule is no manual changes - drift indicates a breach of process or possibly an external actor change. In either case, reconcile it.
- **Backups and Recovery Plans:** This is more ops-focused but absolutely part of best practices. Ensure there's a **backup strategy**. For example, if deploying to Prod, have a fresh full backup or confirm point-in-time restore is available. That way if something catastrophic and unforeseen happens, you can restore. Also test the restore process occasionally. Nothing is worse than needing a backup and finding it doesn't work. This ties into DevOps as well - maybe even automate snapshotting the database before a release as part of a runbook (even if not as a pipeline task).
- **Performance Best Practices:** With frequent schema changes, keep an eye on query performance:
- Update statistics after big data changes (maybe part of post-deploy script if new indexes were added and data loaded).
- If a deployment significantly changes query plans (new indexes or removed indexes), monitor the effect. SQL Server 2019 has Query Store - use it to see if any regressed after changes.
- For large tables, adding columns with default values can lock the table; when possible do such changes off-peak or use techniques to add as nullable then update in batches.
- If you have very tight deployment windows, consider features like **Online Index Rebuilds** or **Resumable index operations** if index changes are part of deployments, to reduce impact.
- **Security in Pipelines:** Make sure sensitive info (like connection strings, passwords) are secured. Use Azure Key Vault integration if possible for prod secrets. Also, limit who can approve or run prod deployments. This prevents unauthorized schema deployments.
- **Team-wide Education:** Encourage all developers (even primarily OutSystems ones) to learn basic SQL and SSDT usage. Perhaps pair a less experienced dev with a mentor for a database task so they learn. The goal is to distribute knowledge - no single point of failure in understanding the DB.
- **Incremental Rollout:** If possible, test changes in smaller subsets of data before the whole production. For example, if there's an archive or a way to test on a subset (maybe your system allows deploying to one tenant's data first), do it. OutSystems might not easily support partial deploys because the external DB is usually one for all, but for very large changes you could feature-flag at the app level (ignore new columns until ready, etc.).
- **No Shared Dev Databases without Coordination:** If multiple devs share a dev DB, coordinate changes to avoid conflict. Alternatively, each dev can have their own local or containerized SQL instance to deploy and test in isolation, then integrate on a shared QA. This prevents stepping on toes. SSDT can publish to any, so use that flexibility.
- **Stay Current with Tooling:** Keep an eye on SSDT updates or Azure Data Studio SQL Project extension improvements. For example, Microsoft recently introduced SDK-style SQL projects (as of VS2022) that integrate with .NET CLI. These might offer new capabilities (like cross-platform build of DACPACs). Evaluate and adopt if beneficial. Also, OutSystems versions update - ensure your approach still fits if OutSystems introduces something like direct DACPAC integration (not likely yet, but who knows).

Following these best practices will ensure that your database DevOps process is **sustainable and reliable**. It's not just about getting a pipeline running once; it's about running a marathon - continuously delivering changes without degrading quality. Governance might sound strict, but it pays off when you have smooth releases and everyone trusts the process.

## Success Metrics

How do we know this new approach is working and delivering value? It's good to define some **Key Performance Indicators (KPIs)** or success criteria. Here are some concrete metrics and outcomes we aim to achieve by implementing this SQL Server DevOps playbook:

- **âœ… 100% Pipeline Deployment Rate:** All external database schema changes are deployed via the automated pipeline (no direct manual deployments). This can be measured by checking deployment logs or simply by policy adherence. When every change goes through CI/CD, you have traceability and consistency. Success is when we can say "We haven't run a manual SQL script in production in months."
- **âœ… Greatly Reduced Lead Time for Changes:** The time from a requested change (or a developer starting on it) to it being deployed is shortened significantly. For example, if previously it took a week of coordination to get a DB change live, with automation it might be down to a day or even hours (depending on approvals). We can track an average lead time: e.g., "Schema changes now deploy in <1 day after code complete, down from 3-5 days."
- **âœ… Increased Deployment Frequency:** We can safely deploy small increments more often. Perhaps you were doing weekly or biweekly DB releases before. Success could be the ability to deploy _on demand_ or with every OutSystems sprint release without extra hassle. Basically, deployment is no longer the bottleneck.
- **âœ… Zero (0) Incidents from Schema Changes:** A big one - with proper process, the number of production incidents caused by database changes should drop to near zero. Incidents might include downtime, failed deployments that cause service outage, or bugs due to schema mismatches (like an OutSystems app breaking because a column was wrong). By using this approach, things like "schema drift" or "missing columns" should be eliminated. We'll know we're succeeding when releases go out without fire-fighting afterward.
- **âœ… Developer Confidence and Morale:** This one is softer but important. Developers (and tech leads) should feel **confident** making database changes. In the past, a dev might be nervous to alter a table that could affect many modules, so they avoid changes or hack around it. Now, with a safety net (source control, pipeline validation, review), they can proactively improve the database schema as needed. You can gauge this via feedback or simply seeing an uptick in contributions to the database project from more team members. Essentially, the database becomes a part of the codebase everyone is comfortable with, rather than a scary shared resource.
- **Process Compliance:** Another metric could be **compliance to process** - e.g., 100% of changes have an associated work item and PR review. This ensures the practice is ingrained.
- **Faster Recovery from Issues:** In case something does slip through, how quickly can we fix it? With automated deploy, if a bug is found, a fix can be committed, and pipeline redeployed quickly. So Mean Time To Restore (MTTR) improves. Instead of hours of manual SQL correction, a pipeline can roll out a correction in minutes (for example, add back a column that was dropped accidentally by reverting commit and redeploying).
- **Quality of Schema** (long-term): Over iterations, the schema should improve, or at least not degrade, because of better practices. For instance, you might measure index usage or run static analysis. If you had issues with missing foreign keys or inconsistent data, after adopting DevOps, those should reduce as every change is reviewed for quality.
- **Team Velocity:** Not a direct metric of the pipeline, but if previously integration with external DB was a blocker that slowed OutSystems feature development, we expect velocity to increase. Features that need DB changes can be delivered in the same sprint, rather than being deferred due to DB coordination issues.

To sum up, success is when the external database is no longer a pain point but a well-oiled part of the development process: - Releases are **faster** and **smoother**. - The business gets updates with less wait and risk. - The team spends more time on building features, and less on deployment firefights or writing boilerplate scripts.

We should periodically review these metrics. If deployments are still causing issues, we adjust our process or add tests. If lead time is still long, identify the bottleneck (is it approval? is it a slow test suite?). Continuous improvement is part of DevOps: use data from our process to keep refining it.

## Appendix: Common Issues & Solutions

Even with the best setup, you'll encounter occasional issues. Here's a table of some common problems and how to address them, which can serve as a quick troubleshooting reference:

| **Issue** | **Solution & Explanation** |
| --- | --- |
| **Pipeline Deployment Fails due to "Possible data loss"** &lt;br&gt;_Error:_ _"Deployment blocked by BlockOnPossibleDataLoss."_ | This is the /p:BlockOnPossibleDataLoss=True flag doing its job! It means the DACPAC detected a change that would drop or truncate data (e.g., dropping a column or decreasing column size). Review the deployment report or error message to identify which object caused it[\[1\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=schema%20of%20the%20target%20database,schema%20reach%20the%20latest%20version). The solution is to implement a safe migration: e.g., if you intended to drop a column, write a pre- or post-deployment script to migrate or archive the data, or deploy a version that makes it unused first. After handling data, you can drop the column in a subsequent release (or re-run with data loss allowed if you are absolutely sure it's safe). Never force a deployment with data loss without understanding the impact. |
| **Build Fails with "Unresolved Reference" Errors** &lt;br&gt;_Error:_ _"Invalid object dbo.OtherTable" or "Unresolved reference to Login XYZ"._ | An unresolved reference means the project is referring to something not defined within it or its references. For cross-database or server objects, you need to add a **Database Reference** or external reference. For example, if a view selects from another database, add a DACPAC of that other database as a reference (or a placeholder reference)[\[40\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=2021)[\[41\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=Harish%20A%20Anandarao%20%20%E2%80%A2,Follow%20%201%20Reputation%20point). If the reference is a SQL server object like a login, you might use a dummy create in a "pre-deployment" or a special folder with NOT IN BUILD setting. Another common one is references to OutSystems internal tables (if any) - usually avoid doing that; but if necessary, you may need a reference DACPAC for the OutSystems system (not generally recommended). Once the reference is resolved (or you suppress it via project settings), the build will succeed. Essentially, tell SSDT about all external dependencies. |
| **OutSystems App Breaks Right After Deployment** &lt;br&gt;_Symptom:_ Screens or aggregates in OutSystems throw errors (e.g., "column not found") after a DB change. | Likely you forgot to refresh and publish the Integration Studio extension **before** users hit the app. OutSystems is still using the old entity definitions. The fix: Immediately go to Integration Studio, refresh the external entities, and publish. Then in Service Studio, refresh dependencies in the consuming modules and republish those. To avoid this, always schedule a coordinated deployment: DB first, then Integration Studio, then any app republish if needed. In a pinch, you can roll back by redeploying the previous DACPAC or restoring a backup, but usually just updating OutSystems to match solves it. |
| **Permissions Issues During Deployment** &lt;br&gt;_Error:_ _"User does not have permission to alter table" or deployment task fails to log in._ | The account used by the pipeline doesn't have sufficient rights on the target DB. Ensure the **SQL user has db_owner or db_ddladmin** on the database (for schema changes). If using integrated security via an agent, ensure the agent's AD account has rights. Also, if the DB is on-prem behind firewall, ensure the agent can reach it. Sometimes the deployment can fail if there are pending transactions by other users locking things - that's rare. If it happens, coordinate so no open transactions during deploy. |
| **SQL Package Path Issues** &lt;br&gt;_Error:_ _"SqlPackage.exe not found"_ or task version issues. | Azure DevOps hosted agents usually have SqlPackage installed (as part of VS). If using a custom agent or GitHub Actions, you might need to install it. Microsoft offers it as a .NET Core global tool now. The error indicates the pipeline couldn't invoke it. Solution: Ensure your build agent has Visual Studio Data Tools installed or manually add a step to install SqlPackage (for example, use dotnet tool install -g Microsoft.SqlPackage and then use that path). On Windows, the default path is something like C:\\Program Files (x86)\\Microsoft SDKs\\â€¦\\SqlPackage.exe. |
| **Handling Static Data (Lookup tables)** &lt;br&gt;_Question:_ _"How do we source control and deploy reference data changes?"_ | Treat static data like part of schema when it's small and relatively static. Use a **post-deployment script with MERGE** statements to insert/update key lookup rows[\[24\]](https://dba.stackexchange.com/questions/265475/what-is-the-best-way-to-include-a-large-amount-of-configuration-data-in-a-sql-pr#:~:text=Your%20intuition%20is%20right%20here,SSDT%20project%20in%20source%20control). For example, if adding a new entry to a StatusType table, you'd write a MERGE that ensures that entry exists (matching by some natural key). This script runs every deployment, but since MERGE is upsert, it will only insert if not present, or update if changed. This way, your lookup tables are kept in sync with what your code expects. Do **not** try to include large volumes of reference data in the DACPAC - it will slow down deployment. For large static data, consider maintaining it externally or using a separate "data seed" DACPAC as \[6\] suggests, but that's rarely needed unless hundreds of MBs of data. In summary: for typical small lookups, put it in source via scripts and let the pipeline manage it. |

Keep this appendix handy when facing issues. Most of these are speed bumps that once solved, tend not to recur frequently. The combination of SSDT and DACPAC is quite robust, and the community has seen most problems before - often a quick search of the error will point to solutions (forums, Stack Overflow, MSDN blogs, etc., many of which we referenced above).

Finally, remember that implementing DevOps for databases is a journey. Start with the fundamentals, iterate on the process, and continuously improve by learning from each deployment. By adhering to this playbook and leveraging the recommended resources, you will significantly level-up the team's capability to deliver changes to external SQL Server databases efficiently and reliably.

Good luck, and happy database DevOps! ðŸš€[\[42\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=robust%20error,downtime%20and%20deployment%20difficulties)[\[43\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=,Methodologies%20for%20DB%20DevOps%3A%20Diving)

[\[1\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=schema%20of%20the%20target%20database,schema%20reach%20the%20latest%20version) [\[2\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=excels,schema%20reach%20the%20latest%20version) [\[7\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=What%20is%20DACPAC%3F) [\[8\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Key%20feature%20of%20DACPAC%20deployment,and%20error%20free%20as%20possible) [\[36\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Some%20of%20the%20key%20benefits,SQL%20schema%20are%20as%20follows) [\[42\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=robust%20error,downtime%20and%20deployment%20difficulties) SQL Schema Versioning, Build, and Deployment with DACPAC | by Firas Sboui | Medium

<https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1>

[\[3\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=But%20SSDT%20is%20in%20a,as%20very%20robust%20and%20powerful) [\[4\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=Offline%20Database%20Development) [\[9\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=,a%20form%20and%20you%E2%80%99re%20done) [\[20\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=In%20case%20you%20didn%E2%80%99t%20know,CI%20and%20CD%20with%20SSDT) [\[27\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=Diving%20even%20deeper%20into%20SSDT%2C,how%20best%20to%20resolve%20them) [\[31\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=There%20are%20numerous%20database%20development,Liquibase%20or%20DBGeni) [\[32\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=As%20of%20today%2C%20SSDT%20is,based%20deployments%20exclusively) [\[33\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=There%20are%20many%20migration,based%20deployments%20only) [\[43\]](https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/#:~:text=,Methodologies%20for%20DB%20DevOps%3A%20Diving) T-SQL Tuesday 177: Managing database code - Eitan Blumin's blog

<https://eitanblumin.com/2024/08/13/t-sql-tuesday-177-managing-database-code/>

[\[5\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=The%20schema%20comparison%20tooling%20enables,that%20has%20the%20same%20effect) [\[6\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=Functionality) [\[15\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=The%20differences%20between%20source%20and,any%20of%20the%20following%20options) [\[38\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17#:~:text=When%20the%20comparison%20is%20complete%2C,different%2C%20and%20the%20source%20name) Schema Comparison Overview - SQL Server | Microsoft Learn

<https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/schema-comparison?view=sql-server-ver17>

[\[10\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=The%20development%20cycle%20of%20a,local%20development%20without%20additional%20effort) [\[11\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=We%20start%20our%20project%20by,using%20the%20schema%20comparison%20tools) [\[12\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=Select%20File%2C%20New%2C%20then%20Project) Create and Deploy a SQL Project - SQL Server | Microsoft Learn

<https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17>

[\[13\]](https://joeplumb.com/blog/sql-server-data-tools-a-deeper-look/#:~:text=import%20a%20data%20model%20from,I%20prefer%20the%20Schema%2FObject%20option) [\[14\]](https://joeplumb.com/blog/sql-server-data-tools-a-deeper-look/#:~:text=Schema%20name%20%28e.g.%20dbo%29%20%7C,Security%20Policies) Joe Plumb | SQL Server Data Tools - A deeper look

<https://joeplumb.com/blog/sql-server-data-tools-a-deeper-look/>

[\[16\]](https://success.outsystems.com/documentation/11/integration_with_external_systems/extend_logic_with_your_own_code/managing_extensions/define_extension_entities/refresh_an_entity/#:~:text=Refresh%20an%20Entity%20,table%20information%20from%20the%20database) Refresh an Entity - OutSystems 11 Documentation

<https://success.outsystems.com/documentation/11/integration_with_external_systems/extend_logic_with_your_own_code/managing_extensions/define_extension_entities/refresh_an_entity/>

[\[17\]](https://www.classcentral.com/course/youtube-ssdt-tutorial-sql-server-data-tools-140991#:~:text=Coursera%20Plus%20Annual%20Sale%3A%20All,Off%21%20Grab%20it) Free Video: SSDT Tutorial - SQL Server Data Tools from Extern Code | Class Central

<https://www.classcentral.com/course/youtube-ssdt-tutorial-sql-server-data-tools-140991>

[\[18\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=Deploy%20to%20Azure%20SQL%20Database,deploy%20schema%20changes%20and%20data) Deploy to Azure SQL Database - Azure Pipelines | Microsoft Learn

<https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops>

[\[19\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=CI%2FCD%20of%20SSDT%20Projects%3A%20Part,project%20template%20and%20Azure%20DevOps) CI/CD of SSDT Projects: Part 2, Creating Azure DevOps pipelines

<https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/>

[\[21\]](https://www.youtube.com/watch?v=ObgY4XB0hHo#:~:text=Using%20Azure%20DevOps%20for%20Microsoft,further%20environments%20while%20maintaining) Using Azure DevOps for Microsoft SQL Databases with SSDT

<https://www.youtube.com/watch?v=ObgY4XB0hHo>

[\[22\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17#:~:text=In%20Solution%20Explorer%2C%20right,Deployment%20Script) [\[23\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17#:~:text=Pre,after%20the%20deployment%20plan%20completes) [\[39\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17#:~:text=Add%20pre) Pre- and Post-Deployment Scripts - SQL Server | Microsoft Learn

<https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/concepts/pre-post-deployment-scripts?view=sql-server-ver17>

[\[24\]](https://dba.stackexchange.com/questions/265475/what-is-the-best-way-to-include-a-large-amount-of-configuration-data-in-a-sql-pr#:~:text=Your%20intuition%20is%20right%20here,SSDT%20project%20in%20source%20control) sql server - What is the best way to include a large amount of configuration data in a SQL project? - Database Administrators Stack Exchange

<https://dba.stackexchange.com/questions/265475/what-is-the-best-way-to-include-a-large-amount-of-configuration-data-in-a-sql-pr>

[\[25\]](https://github.com/microsoft/DacFx/issues/18#:~:text=Unable%20to%20read%20data,Then%20the%20%2FAction%3ADriftReport) Unable to read data-tier application registration after Publish using ...

<https://github.com/microsoft/DacFx/issues/18>

[\[26\]](https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-deploy-drift-report?view=sql-server-ver17#:~:text=SqlPackage%20Deploy%20Report%20and%20Drift,since%20it%20was%20last) SqlPackage Deploy Report and Drift Report - SQL Server

<https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-deploy-drift-report?view=sql-server-ver17>

[\[28\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=7,the%20changes%20in%20this%20session) [\[29\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=11.%20Right,regardless%20of%20the%20rename%20operation) [\[30\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=10,table%20was%20renamed) [\[37\]](https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17#:~:text=1.%20Right,SQL%20Editor) Rename and Refactoring to Make Changes to Your Database Objects - SQL Server Data Tools (SSDT) | Microsoft Learn

<https://learn.microsoft.com/en-us/sql/ssdt/how-to-use-rename-and-refactoring-to-make-changes-to-your-database-objects?view=sql-server-ver17>

[\[34\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=Under%20the%20database%20project%2C%20there,in%20your%20cross%20databases%20queries) [\[35\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=As%20you%20have%20discovered%2C%20cross,in%20SSDT%20can%20be%20complicated) [\[40\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=2021) [\[41\]](https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with#:~:text=Harish%20A%20Anandarao%20%20%E2%80%A2,Follow%20%201%20Reputation%20point) SSDT- How to setup SQL Server Database Project with a database that has cross database queries - Microsoft Q&A

<https://learn.microsoft.com/en-us/answers/questions/269913/ssdt-how-to-setup-sql-server-database-project-with>
