**OutSystems to SSDT DevOps Transition:**

**Onboarding & Operational Whitepaper**

# Introduction & Cutover Context

A major process change is underway: **all OutSystems external database entities are moving to a statebased SQL Server Data Tools (SSDT) workflow with Azure DevOps and Octopus CI/CD**. This means no more direct database edits or ad-hoc SQL scripts. Instead, developers will author schema changes in an SSDT project (Visual Studio), version them in Git, compile into a DACPAC, and deploy via automated pipelines 1 . Every change is peer-reviewed and traceable, and OutSystems must synchronize its model _after each deployment_ to pick up schema updates 2 . The goal is to treat the **database schema "as code"** with the same rigor as application code - ensuring a single source of truth in Git, consistent environments via automated deployments, and robust governance (no drift, clear history, easy rollback) .

3

**Why this change?** Managing the database through SSDT brings multiple benefits: all schema changes are captured under source control, builds catch errors early, and deployments become repeatable and testable across Dev, Test, and Prod 3 . Automated DACPAC deployments reduce human error by **applying only the differences needed** to update a database to the desired state 4 5 . Ultimately, the database is no longer a mysterious black box but a transparent part of the DevOps lifecycle 6 . This transition is critical for reliability and velocity in our enterprise environment - especially given our ~100 entity external SQL Server database backing OutSystems. The team is mostly OutSystems developers new to external DB workflows, so this guide will **bridge the knowledge gap for all roles and experience levels**. We will outline what to do before, during, and after cutover, with role-specific tips for Dev Leads, individual contributors (junior and senior), QA, Ops, and PMs.

**Success criteria:** By _Cutover Monday_, every developer should be equipped to perform basic SSDT-driven schema changes without blocking the team . In the first week post-cutover, we aim for zero production incidents from schema changes . Within the first month, team velocity should return to normal as everyone becomes comfortable with the new process . The following sections provide a comprehensive roadmap to achieve these goals, including a Week-0 readiness checklist, Day-1 survival tips, new workflow playbooks, role responsibilities, governance standards, and common pitfalls to avoid.

7

8

7

# Week-0 to Week-1 Transition Roadmap

## Week 0 (Pre-Cutover) Readiness Checklist

In the week leading up to cutover, preparation is key. All team members (Dev, QA, Ops) should complete the following by the Friday before cutover :

9

- **Install & Configure Tools:** Set up Visual Studio 2019/2022 with the **SQL Server Data Tools (SSDT)** workload, plus SQL Server Management Studio (SSMS) for queries and OutSystems Integration Studio for refreshing external entities 10 11 . Verify you can connect to the Dev database and have proper permissions (e.g. db_owner on Dev) 10 .
- **Import Current Schema into SSDT:** Use Visual Studio's **Import Database** or Schema Compare to import the existing external DB schema into a new SQL Database Project 12 13 . This generates CREATE scripts for all tables, views, etc., organized in the project. Build the project to produce your first DACPAC (a compiled schema snapshot) 14 . Think of the DACPAC as the "DLL" of your database - a portable package of the schema that can be deployed to any environment 15 .
- **Version Control Setup:** Put the SSDT project under Git (Azure DevOps Repo). Commit all .sql files, the .sqlproj file, and any reference DACPACs. From now on **treat this repo as the source of truth** for the database - _no schema changes outside of it_ 16 . If an emergency prod fix is made outside (avoid if possible), it must be back-ported into the project to prevent drift 16 .
- **"Hello World" Change:** Practice a trivial change end-to-end. For example, add a test column in a branch, build the project, and generate the deployment script to see the ALTER statements SSDT produces 17 . This helps demystify the process. Ensure the project builds with 0 errors in Visual Studio (warnings are okay) 18 .
- **Training & Resources:** Review any provided learning materials. If you're shaky on SQL fundamentals or schema versioning, use this time to upskill (e.g. Microsoft's SSDT tutorial on creating and deploying a SQL project 19 , Pluralsight SQL basics, OutSystems docs on external DB integrations). We surveyed the team's baseline knowledge and recommended resources accordingly 20 - take advantage of these now.
- **Communication Channels:** Join relevant Slack/Teams channels for support: e.g. **#database-help** (for how-to questions), **#database-dev** (for deployment coordination), **#database-emergency** (for urgent issues). Also bookmark this guide and any quick-reference playbooks. Being connected will ensure you can get help quickly if you hit a snag.
- **Environment Prep:** Ensure **Dev and Test environments are ready** for the new pipeline. Ops should set up any Azure DevOps pipeline definitions and Octopus project configurations in advance, and verify the service connections (e.g. Octopus connection from Azure DevOps, database connection strings) are working. Also, schedule database backups or snapshots before the cutover deploy as a safety net. _(We will cover the deployment runbook in a later section.)_

By end of Week 0, every developer should have a working local setup and a basic understanding of the workflow. This ensures we **hit the ground running on Monday** rather than scrambling to install tools under pressure 21 .

## Cutover Weekend & Day 1 ("Go-Live" Monday)

Over the cutover weekend, the new SSDT-based process will be put into effect. The general cutover playbook is:

- **Last Manual Changes Freeze:** Prior to cutover, freeze any direct DB changes. The final known schema is the one imported into SSDT. Communicate a code freeze if needed so no one applies hotfix scripts outside the process during the transition. Ops should take a final backup of the database just before deploying the new DACPAC (for rollback safety).
- **Initial DACPAC Deployment:** Using Octopus or Azure DevOps, deploy the initial DACPAC (which represents the same schema as current production) to each environment. This serves as a baseline deployment managed by the pipeline (it should result in "no changes" since it's the same schema, but it validates the pipeline). This step ensures all pipelines, Octopus configurations, and permissions are working in Production before real changes are introduced.
- **Communication & All-Hands:** On Monday (cutover day), hold an **extended morning standup**. Dev Leads should give a live demo of the new workflow - e.g. adding a column in SSDT, pushing to Git, running a deployment, and **refreshing OutSystems via Integration Studio** 22 . There will be time for Q&A to address any confusion 22 . This sets the tone and lets everyone see the end-to-end process in action.
- **Day-1 Quick Reference:** Distribute or remind the team of the "Day 1 Survival Guide" tips. Key points for Monday: _no manual DB edits_, _don't skip the OutSystems refresh_, and _always build your project before committing_. The top things **NOT** to do on Day 1 include: **no direct ALTERs** on the database (all changes go through SSDT now) 23 , **no skipping the Integration Studio refresh** after a DB deploy

24 , **no SELECT \* in OutSystems** (always select explicit columns) 25 , and **no adding NOT NULL columns without defaults** (the deployment will fail or block if existing data can't satisfy the constraint) 26 . These "anti-patterns" have caused outages elsewhere, so avoiding them will prevent immediate fire-fights.

- **Support On Standby:** During the first deploys on Monday, senior developers and Ops should be actively monitoring. Treat the initial runs as _"narrated deployments"_ - a lead or senior can walk through each step in real time (in a screen share or Slack updates) so everyone knows what's happening 27 . Any hiccups can be caught and explained. If a developer's build fails or they encounter errors, they should know where to turn: consult the troubleshooting section of this guide and ask in #database-help if needed (no one should struggle alone for more than ~15 minutes without seeking help) 28 .

**Integration Studio Refresh:** A _critical_ new habit is refreshing OutSystems after any external DB schema deployment. On Day 1, emphasize this repeatedly: **database changes first, OutSystems refresh second** - never the reverse 29 . Skipping the refresh will leave OutSystems unaware of the new schema, causing runtime errors (e.g. "Invalid column name" in aggregates) until fixed 30 . As a rule, after the DACPAC is published, open Integration Studio, load the affected extension, and use _Refresh_ to update its entities from the database, then publish the extension 31 32 . Only once the extension is updated should any

OutSystems module that uses it be republished (refresh module references in Service Studio and deploy)

33 . This sequence ensures OutSystems apps have the latest schema metadata and prevents breaking changes. _Never publish an OutSystems module expecting a DB change before that DB change is actually deployed._ If multiple developers deploy changes to the same external DB, coordinate so that **everyone refreshes at a designated time** (commonly a shared "refresh window" like 3:30 PM daily) to avoid conflicts 34 .

**Rollback Plan:** Despite best efforts, be prepared with a rollback strategy for cutover. The SSDT approach makes pure schema rollbacks non-trivial (drops can't be "undropped" without a backup). If something goes seriously wrong in production on Day 1, the primary options are: (a) **Re-deploy the previous DACPAC** (if the issue was a bad schema change that can be reversed - this works best if no destructive changes were made), or (b) **Restore the pre-cutover DB backup** and rollback the application to pre-cutover state. The team should agree on a "stop the line" threshold - e.g., if a critical outage occurs, we restore the knowngood backup _immediately_, rather than trying ad-hoc fixes. Document the rollback steps and responsibilities in the cutover runbook: who can authorize a rollback (likely the Dev Lead or EM), how to communicate to stakeholders, and how to reintroduce the changes after addressing the issue. In our case, the expectation is zero production incidents in Week 1 8 , but having a safety net ensures confidence to move forward.

## Post-Cutover Week 1: New Rhythm and Support

Once the new process is in place, the first week (Week 1) is all about reinforcing habits and ironing out kinks. We adopt a **"release train" model** for database changes to impose a smooth, predictable workflow

35 . Here's the typical **Day-in-the-Life (Week 1)**:

- **Morning Standup:** Each developer shares any planned DB changes or blockers with SSDT. Since this is a new workflow, use daily standup to surface questions. (On Cutover Monday, this standup included the extended demo as noted 36 .) Dev Leads should remind everyone of critical steps (e.g. "don't forget to refresh at 3:30PM") and gauge if anyone is stuck.
- **Daytime Development:** Developers continue building OutSystems application features, but now with the integrated DB workflow. When a user story requires a schema change, implement it in the SSDT project (on a feature branch) **alongside** the OutSystems module change. It's a good practice for devs to pair up or consult with others if uncertain - especially in the first few days, no one should hesitate to ask questions. Code reviews and pair programming can accelerate learning. Leads and seniors should be extra available on chat for quick help.
- **Pull Request by 2:00 PM:** We institute a daily **2:00 PM cutoff** for database change pull requests to be included in that day's deployment train 37 . Aim to have your branch committed and open a PR by that time. This ensures reviewers have at least ~1 hour to review changes before deployment. (Late PRs can go next day - sticking to the schedule prevents last-minute rush and mistakes.) Use the provided **PR template** to detail _what_ the change is, _why_ it's needed, any _impact_ on OutSystems (e.g. "Refresh extension X after deploy"), and link any work item or ticket 38 . **No PR should be marked "ready" if the SSDT project isn't building cleanly** - run a local build first, because the CI will reject PRs that fail to build 39 .
- **Code Review (2:00-3:00 PM):** Dev Leads and senior devs will review the incoming PRs. They check for correctness, standards, and risks (more on the Code Review Checklist in a later section). Be responsive to any review comments; if you can fix issues quickly, your change can still make the 3 PM train. Common review focus areas include: naming conventions (tables singular, PascalCase identifiers) 40 , data types (using NVARCHAR for text, proper precision for dates/numbers) 41 , and migration safety (no dropping columns without a safe migration plan, no adding NOT NULL fields without defaults) 42 . PRs that involve potentially destructive changes (drops) will get extra scrutiny, and our pipelines include automated **forbidden-change scanners** to catch them (see _Pull Request Process_ below).
- **Deployment at 3:00 PM:** Once PRs are approved and merged to main, the **CI pipeline** kicks in to build the project and produce an updated DACPAC artifact. Our Continuous Deployment pipeline (or Octopus Deploy process) then takes over to deploy that DACPAC to the _Dev_ environment around 3:00 PM. Typically a dev lead or designated "DB release captain" will initiate or oversee the deployment. This person announces in Slack that the deployment is starting and posts updates (e.g. "Deploying schema changes to Dev now…") 43 . The deployment uses a consistent publish profile to apply all merged changes as a batch. Thanks to DACPAC's state-based approach, it will compute the necessary ALTER statements and execute them within a transaction if configured 44 . The result is the Dev database schema is updated to match the latest project state. If the same DACPAC is deployed again, it should report no differences (idempotent deployment) 45 . We keep deployment logs and generated scripts for review and audit.
- **3:30 PM - Integration Studio Refresh:** After the DB deploy, **all developers participate in refreshing OutSystems**. Each developer is responsible for refreshing the external entities in Integration Studio for any modules they work with (or one person can handle all, but it's often faster in parallel). The team has a standing 3:30 PM "refresh window" when everyone knows to perform the OutSystems sync 34 . For example, if 5 tables changed, 5 corresponding OutSystems extensions need updating. Doing this together avoids the "I thought someone else refreshed it" problem. Once refreshed and published, developers should smoke test key application functions to ensure nothing broke. In practice, adding new columns or tables is seamless (the app won't use them until coded to), but removing or renaming fields requires coordinated app changes. This is why such breaking changes are planned and reviewed carefully (often with feature toggles or phased deployments).
- **Post-Deploy Wrap-Up:** After 4:00 PM, normal work continues. If a change missed the train or was held back in review, it goes into tomorrow's cycle - avoid trying to push it in late. Use the late afternoon to update any documentation or assist others who had issues. On **Friday of Week 1**, we will hold a retrospective to discuss how the process went 46 . Expect to adjust timing or rules as needed. For example, if PR reviews are too rushed, we might move the cutoff earlier; if refreshes are chaotic, we might assign an "Integration Studio champion" each day. Continuous improvement is part of the DevOps culture.

**Week-1 Support:** The first week is supported with extra precautions: daily office hours (open Q&A sessions), a "DB mentor" rotation for on-demand help, and monitoring by leads. Dev Leads should capture any frequently asked questions or repeated hiccups and update our internal wiki or this guide ("living Week-1 survival guide") with clarifications 47 . By the end of Week 1, every junior dev is expected to have independently gone through at least one full change (e.g. add a column) and be comfortable with the daily routine 48 . If anyone is still struggling, we'll pair them with a mentor in Week 2. Remember: **communication is vital**. Keep the Slack channels active with status updates ("refresh done"), questions, and even mistakes - we'd rather discuss a near-miss than have someone hide an issue until it causes a problem. With each day, the process will feel more natural.

# SSDT Workflow Usage Guide (All Levels)

## SSDT Project Structure & Declarative Schema

SSDT (SQL Server Data Tools) allows us to define the **desired state** of the database in a Visual Studio project. The project (a .sqlproj file) contains scripts for each object - tables, views, stored procedures, etc. - typically one file per object for clarity 49 50 . Key components of our project structure:

- **Schema Folders:** Objects are organized by schema (and sometimes by type). For example, all dbo tables might reside in a Tables\\dbo folder. SSDT's import can auto-organize this hierarchy 51 . This makes navigation easy and parallels the actual DB structure.
- **Create Scripts, Not Alters:** Each table or object script is written as a CREATE statement defining the end state of that object. **Do not write ALTER TABLE or incremental scripts in these files** 13 . SSDT's build and deployment process will compare the project's CREATE definitions to the target database and generate the necessary ALTER statements automatically 13 . This is the essence of _state-based_ deployment - you declare the final schema, the tooling figures out the migration steps. For example, to add a column, you just add that column to the CREATE TABLE script in the project; the DACPAC publish will emit an ALTER TABLE ADD COLUMN when deploying 13 .
- **Refactorlog:** If you rename an object, SSDT uses a _refactor log_ to track it as a rename operation. This prevents the deployment engine from thinking "old name dropped, new name created" (which would lose data). Use Visual Studio's **RefactorRename** feature for renames so that it generates an entry in the refactor log file. Reviewers will **check for refactorlog entries** on any rename; a missing entry

is a red flag that a rename might actually drop and recreate behind the scenes 52 53 . The refactor log essentially instructs the DACPAC deployment to treat it as sp_rename operation. _(Tip: Never manually edit the refactorlog unless you know what you're doing. If you mistakenly dropped/created instead of rename, you might need help to fix it.)_

- **Pre- and Post-Deployment Scripts:** The project can include a Script.PreDeployment.sql and

Script.PostDeployment.sql . These run before and after the main schema changes, respectively. We use **Post-Deployment** scripts for data fixes or reference data seeding (e.g. insert new lookup rows) and **Pre-Deployment** for any prep work (e.g. create a temp table to migrate data, or set session settings). Always write these scripts to be **idempotent** - meaning if you run them twice, they won't duplicate data or error out 54 . For example, use IF NOT EXISTS(...) INSERT... or MERGE statements for inserts so that if the data is already there, nothing happens

54 . Idempotent scripts allow safe re-deployments and ensure that if a deployment is partially run and re-tried, it doesn't double-add data. We also include any necessary grants or permissions in post-deploy (so that new tables have the proper rights, since DACPAC doesn't carry user permissions by default).

- **Publish Profiles:** These .publish.xml files store options for deployment. For instance, we might have a profile for Dev that allows certain operations and a stricter one for Prod. By default, we enable BlockOnPossibleDataLoss for Test/Prod to **prevent drops if they would cause data loss** 55 (the deployment will stop with an error rather than dropping a table/column with data). The publish profile also contains the target connection or can be passed in by Octopus. Ops/DBA maintain these profiles, and Octopus/DevOps uses them to execute deployments with consistent settings.
- **Source Control Considerations:** All the above files are in Git. We also set up a CODEOWNERS file so that changes in certain areas (e.g. critical tables or the publish profiles) automatically request review from specific leads. Merges to the main branch trigger an automated build pipeline (CI) to compile the SSDT project - if the build fails, the change cannot proceed 39 . This ensures no broken create scripts get into main.

For those new to Visual Studio, a quick orientation: use **Solution Explorer** to see the project tree. You can add a new table via _Add New Item Table_ . Visual Studio will scaffold a basic CREATE TABLE for you. You can also use the Table Designer GUI if preferred, but be cautious - the GUI might set defaults or choices you don't intend. It's often better to write/edit the SQL scripts directly for precision, using **IntelliSense** and error squiggles as guidance (SSDT will underline syntax errors or invalid references in red).

**Schema Compare:** A powerful SSDT feature is **Schema Compare**, which graphically compares your project to a target database or another DACPAC. We encourage developers to use Schema Compare before deploying significant changes. It's like a "dry run" - you see a diff of what's going to change (which tables will be altered, etc.) and the exact script that will run 56 5 . This is great for catching unintended changes. For example, if you see a drop statement in the diff that you didn't expect (maybe you removed a column in the project by accident), you can catch it before it ever goes to the pipeline. Schema Compare is also useful to verify that your local dev database matches the project (to detect any drift or manual changes someone might have done outside the project). Use it as a learning tool too - it helps visualize how SSDT interprets your changes.

## Common Schema Operations & Patterns in SSDT

Now let's cover _how to perform typical schema changes_ in this new system, along with best practices:

- **Adding a New Table:** To add a table, simply create a new .sql file in the **Tables** folder (or use AddTable which does this for you) and write the CREATE TABLE statement. Include a primary key (we follow the convention of an Id identity column for primary keys, or a composite key if needed) and any necessary columns 57 . Consider defaults for columns that make sense (e.g. default

GETDATE() on a CreatedDate). Adding a new table is straightforward as it doesn't affect existing data, and SSDT will just generate a CREATE TABLE on deployment. Still, follow naming conventions

(table name singular, PascalCase, etc.) and if the table has natural relationships, add the foreign keys (see below). Once deployed, don't forget to refresh OutSystems to import this new table as an external entity.

- **Adding a Column (Non-Breaking):** This is the most common change. Open the table's .sql file and add the new column definition in the CREATE TABLE. If the table already has data and the column is _optional_ (nullable), make it NULL or provide a DEFAULT if it's NOT NULL so that the deployment can succeed 26 . Generally, adding a nullable column or a column with a default is considered a **safe, additive change** - it won't break existing queries or code. After deployment, refresh OutSystems so the new column appears in aggregates/structures. Also consider if an index is needed on the new column (e.g. if it will be used in search conditions frequently). Indexes can be added in the same deployment; you'd add a CREATE INDEX statement in the table file _after_ the table definition, or as a separate index script.
- **Adding a Column (Requiring NOT NULL):** If you need a new column to be NOT NULL and there are existing rows, you have two options: (a) Add it as NOT NULL with a DEFAULT constraint in one step. SSDT will then deploy it by filling the default for existing rows 26 . This is fine if a default value makes sense (e.g. a IsActive BIT default 1). Or, (b) **Two-phase approach**: First deploy the column as NULL (or NOT NULL with default) and write a post-deployment script to backfill or compute values for existing data. Then in a later release (e.g. next sprint) alter it to NOT NULL once data is backfilled [58](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A4/) 59 . This phased tightening avoids downtime. Reviewers will flag any PR that introduces a NOT NULL on an existing table without a default or backfill plan - because it will fail deployment or potentially drop data 42 .
- **Renaming a Table or Column:** Renames are tricky in state-based deployments because the tool sees a "drop X, add Y" unless told otherwise. To safely rename, **use SSDT's rename refactoring**. For example, to rename a column, right-click it in the table script (or in Schema View) and choose Refactor Rename, then provide the new name. This will update the name in the CREATE script and record the change in the refactor log. The deployment then performs it as a true sp_rename under the hood 53 . Without the refactor log, the DACPAC would think you dropped the old column and added a new one (which would drop data!). So never manually just change the name in the script - always use refactor or coordinate a custom change. In some cases, an alternative is to do an **Add New + Deprecate Old** pattern: add the new column, copy data over, modify app to use new column, then drop the old column in a later release. This is safer for major refactorings or when renaming across many references, albeit slower. In any case, out-of-band renames (like manually running sp_rename in SSMS) are forbidden after cutover - they would break the alignment of project and DB. • **Foreign Keys & Relationships:** When adding a new table or new reference, add the **FOREIGN KEY (FK)** constraint in the SSDT project to enforce referential integrity. This can be done within the table script (after column definitions) or as a separate .sql file under a Constraints folder. We prefer to name FK constraints FK_&lt;ChildTable&gt;\_&lt;ParentTable&gt; for clarity 40 . Additionally, **always**

**create an index** on any foreign key column (if one does not already exist), as this is important for performance 40 . Reviewers will check that any new FK has a corresponding index 40 . If you forget, they'll likely comment "add an index on this FK column for performance." Also consider the ON DELETE/UPDATE behavior (usually we use NO ACTION or CASCADE depending on requirements).

Document any cascading deletes because that can surprise application logic. OutSystems will reflect FKs in aggregates (e.g. enabling join suggestions), so after refreshing, those relationships become visible in Service Studio.

- **Indexes & Performance:** Adding an index is done by including a CREATE INDEX statement targeted to the table. Follow naming conventions for indexes (e.g., IX_&lt;Table&gt;\_&lt;Column&gt; or

IX_Table_Col1_Col2 for composites) 60 . Avoid duplicate indexes or those with the same leading columns. For unique constraints, you can either use a UNIQUE index or a UNIQUE constraint (which also creates an index). In SSDT, indexes are separate from the table definition (unlike primary keys or unique constraints which are part of table DDL). Ensure new indexes are included in the project so they deploy. Monitor query performance after adding - if it's to address a specific slow query, test that query with the index in place. We will cover more performance tuning in the Month-1 advanced section.

- **Deleting or Dropping Objects: Dropping** a table or column is the highest-risk change and is tightly controlled. The SSDT project will treat a removed script as a drop on deployment. By policy, **drops are generally forbidden** in Prod without a safe migration path. If you remove a column/table from the project, the deployment to Prod will likely be blocked by the publish profile's data loss prevention

55 unless explicitly overridden. In Dev/Test, you might drop freely when experimenting, but even then, consider if it breaks OutSystems. If an OutSystems module is still using that table/column, dropping it will break things immediately. The recommended approach for deprecating a schema element is a **soft transition**: e.g., to drop a column, first ensure no code uses it (update OutSystems to stop referencing it, maybe repurpose or rename it to something like "LegacyXYZ"), then drop in a subsequent release after confirming it's truly unused. We also have an automated **forbidden-drop scanner** in our pipeline that will flag any DROP TABLE or DROP COLUMN operations in the deployment script 61 . In Production it will outright fail the deployment if such operations are detected (unless a lead approves an exception); in lower environments it might warn. The bottom line: _communicate and plan_ any destructive change well in advance.

**Pro-Tips:** Always **build locally** before pushing commits to catch errors early 62 . The Visual Studio build will surface issues like syntax errors, missing object references (e.g., you use a table that isn't in the project), or duplicate names. A common error is forgetting a GO separator between two statements - if you see _"Only one statement is allowed per batch"_ during build, you likely need a GO 63 64 . Another is an _"unresolved reference SQL71501"_ which means you referenced something the project doesn't know about 65 - maybe you misspelled an object or need to add a database reference for cross-DB queries. These errors can be puzzling at first, but the troubleshooting appendix in this guide (and SSDT documentation) offers guidance on the common ones. Don't hesitate to ask a senior if an error is unclear - it could save you hours. And remember, **never ignore a build error**; if it doesn't build on your machine, it won't build in CI and will block your teammates when they sync.

## SSDT Do's and Don'ts Summary

To wrap up the SSDT usage basics, here are the top do's and don'ts to internalize:

**DO:**

- **Treat schema as code** - plan changes, review diffs, and commit to Git. Each commit is a versioned state of the DB .

3

- **Build and test locally** before pushing - ensure the DACPAC compiles with 0 errors 62 . If possible, publish to a local or dev database to verify the script runs cleanly.
- **Write changes declaratively** - use CREATE/ALTER in the project to represent end state, and let SSDT handle the deployment logic 13 . Leverage Schema Compare to preview changes and catch mistakes 56 . - **Keep scripts idempotent** - especially post-deployment data changes. Use IF EXISTS checks or MERGE for seed data 54 . This allows repeated deployments and easier rollbacks.
- **Follow standards & naming conventions** - e.g., PascalCase names, singular table names, "Id" primary keys, foreign keys with indexes, etc. (Our code review checklist enforces this) 40 . Consistency makes the schema easier to understand and reduces errors.
- **Ask for help and pair up** - If you're unsure how to model something or run into SSDT quirks, reach out. Early in the transition, it's expected to have questions. It's better to pause and clarify than to guess and cause a broken build or bad schema.

**DON'T:**

- **Don't bypass the process** - _No direct production DB edits via SSMS, ever._ All changes must go through the SSDT project and pipeline 66 23 . This ensures auditability and consistency. In emergencies, get approval and then retro-fit the change into the project ASAP.
- **Don't skip OutSystems refresh** - Failing to update the external entities after a deploy is the #1 cause of post-deployment bugs in this setup 67 68 . Make it a reflex: after seeing "Deployment Succeeded", immediately refresh/publish the Integration Studio extension.
- **Don't use SELECT \*\*\* in OutSystems queries** - When schema changes, SELECT \* can suddenly pull in new columns and break things. Always query explicit columns so your code is stable even as the DB evolves 23 . We have static code analysis to discourage SELECT \* in our environment 69 .
- **Don't drop or alter existing schema without a plan** - No dropping tables/columns or changing data types that cause data loss without following the safe patterns (e.g. phased rollout) 70 . The system will likely block you anyway (DACPAC flags it), but even in Dev it can wreak havoc if OutSystems still expects that structure. Always consider the impact and discuss in design.
- **Don't ignore warnings** - The build might give warnings (like "possible data loss on deploy"). Treat them seriously; understand why the warning is there. For instance, altering a VARCHAR(100) to VARCHAR(50) will warn of truncation possibility. Such warnings mean you should double-check your approach.
- **Don't commit generated VS junk or user-specific settings** - Only commit the essential project files (.sql, .sqlproj, .publish profiles, etc.). No .vs folder stuff or local config overrides. Our .gitignore covers most of it, but be mindful.

By adhering to these practices, you'll avoid the most common pitfalls and help keep our database deployments smooth. The SSDT workflow might feel rigid at first, but those guardrails are what save us from costly mistakes.

# CI/CD Deployment Pipeline (Azure DevOps & Octopus)

With the database in source control and changes reviewed, how do we get those changes into each environment? This is where our **Continuous Integration/Continuous Deployment (CI/CD)** setup comes in, leveraging Azure DevOps for build automation and **Octopus Deploy** for orchestrating deployments. The pipeline ensures that every database change goes through a consistent, automated path from development all the way to production.

## Continuous Integration (CI) - Building the DACPAC

Every commit to the main branch triggers an automated **CI build** in Azure DevOps. This uses a build agent to restore the SSDT project (if needed) and execute a MSBuild on the .sqlproj . The outcome is a compiled DACPAC file (e.g. MyDatabase.dacpac ) representing the latest schema. The build pipeline will fail if the project doesn't compile (just like a code project) - this protects the main branch from syntax or reference errors 39 . We also include unit tests or static analysis steps in CI if applicable (for example, one could add SQL code analysis rules to catch things like missing primary keys or disallowed patterns). On a successful build, the DACPAC is published as a build artifact. We often retain artifacts for traceability (so we can download a particular build's DACPAC later if needed). The CI process runs in Azure DevOps, but it's configured to integrate with Octopus: once the DACPAC artifact is produced, Azure DevOps can create a release or notify Octopus to pick it up for deployment. In our setup, we use Azure DevOps to build and then hand off to Octopus Deploy for the release steps.

_Technical note:_ Azure DevOps offers a built-in **"Azure SQL Database Deployment"** task that can deploy a DACPAC, but since we use Octopus for releases, we instead publish the DACPAC and a PowerShell script (or use Octopus metadata) to trigger Octopus. The key is that **CI provides a guaranteed up-to-date DACPAC**. If the build is broken, no deployment can proceed - this gating is crucial.

## Continuous Deployment (CD) - Octopus Release Orchestration

**Octopus Deploy** is our deployment orchestrator, handling the release promotions and environment-specific configurations. We configured an Octopus project for the database schema. Here's how the CD pipeline flows:

- **Release Creation:** When CI succeeds (or on a schedule), an Octopus release is created for the new version of the DACPAC. This release knows which version of the DACPAC artifact to deploy. The versioning might follow our sprint or build numbers. The PM/Release Manager can also manually create or approve releases via Octopus's dashboard, giving control over when to push to Test/Prod.
- **Deployment to Dev:** Octopus first deploys to the Dev environment (this corresponds to our daily 3:00 PM train in Week 1). The deployment process in Octopus includes a step to **Execute DACPAC**. Under the hood, Octopus can use the SqlPackage.exe command-line or its own SQL Database Deployment step to apply the DACPAC. In essence, it runs: _"take DACPAC, compare to target Dev database, generate script, execute script"_. Octopus supports DACPAC deployments natively 71 , so it logs the SQL script and any warnings. We configure Octopus with the proper connection string to Dev and use the Dev publish profile (which might allow certain operations like data loss if we're okay in Dev). During this step, Octopus can also run **pre/post scripts** if we have any (though typically those are inside the DACPAC, but Octopus could run additional environment setup if needed).
- **Automated Checks & Guards:** As part of the Octopus deployment process, we incorporate scripts to run automated checks. For example, a **forbidden-drop scanner** runs on the generated SQL script to detect disallowed operations (DROP table/column in Prod) 72 . If such operations are found in a Prod deployment, Octopus will fail the deployment (in lower env, it may just warn). We also include an idempotency check: in Dev, after deploying, Octopus can re-run the DACPAC deployment in "reportonly" mode to ensure that a second run produces "0 changes" (proving the deployment was idempotent and no lingering issues). These checks enforce our governance model around no data loss and repeatable deploys 73 74 . Octopus logs these outcomes (pass/fail) and we treat any failure as a stop that must be addressed.
- **Promotion to Test and Prod:** After Dev, the same release can be promoted to Test (UAT) and later

Prod. Promotion is typically **manual or scheduled**, meaning a human (Dev Lead or Release

Manager) approves and kicks off the deployment to the higher environment at an appropriate time (often off-hours maintenance windows for Prod). The Octopus process is the same but uses the Test or Prod publish profile - for example, the Prod profile will have BlockOnPossibleDataLoss=True (to prevent data loss), and might have a longer timeout, etc. Octopus ensures the _same DACPAC_ is what gets deployed, so we know Dev, Test, Prod schemas are in sync except for differences applied by that release. If any step fails, Octopus can automatically create an incident, and no further promotions happen until resolved.

- **Octopus and OutSystems Coordination:** Octopus doesn't directly refresh OutSystems (as it's more of a manual step), but we integrate it into the **deployment checklist**. Octopus can be configured to send a notification (email or Teams) when a DB deployment finishes, prompting the team or an automated job to handle the OutSystems refresh. In some setups, we might include a _manual intervention step_ in Octopus after a Prod DB deploy, instructing: "Run Integration Studio refresh for X module now, then resume." This ensures no one forgets at prod time. At minimum, Octopus's runbooks for Test/Prod include a reminder to do the refresh and verification steps before considering the deployment complete. _(Future improvement:_ we are exploring scripting the OutSystems Deployment API to automate the refresh, but currently it's manual.)\*
- **Post-Deployment Verification:** Octopus supports adding custom verification scripts. After deploying to, say, Prod, we run a lightweight check (PowerShell or tsql) to verify certain critical tables exist/columns are present, and maybe query a version tracking table. Additionally, QA will run smoke tests on the application. Only after these verifications do we mark the release as successful. Octopus logs are archived for compliance, and any issues found post-deploy can trigger its rollback procedures (though as noted, rollback is not straightforward - it may involve a hotfix forward or manual restore).

The net effect of this CI/CD pipeline is **consistency and repeatability**: the _same DACPAC artifact_ built once is deployed through each stage, reducing the "works in dev, breaks in prod" syndrome 75 . The state-based deployment ensures that if each env was up to date before, applying the DACPAC yields the same result (thus maintaining environment parity) 75 . If a target environment somehow had drift, the deployment will attempt to correct it (if a table exists in Prod that was not in the project, by default DACPAC might try to drop it, which is why unauthorized changes are dangerous - but our policy and monitoring should prevent drift in the first place). We keep a history of DACPACs (you can even use them to diff versions - e.g. compare v1 vs v2 to see what changed). In case we ever needed to rollback a schema change that was purely additive, one could re-deploy the older DACPAC (Octopus can deploy a previous release) to remove it. However, truly reversing data-affecting changes usually involves manual intervention or backup restore 76 . So our strategy emphasizes **forward-fixes over rollbacks**: fix issues by deploying new changes rather than attempting to revert, except in dire cases with full restore.

Finally, because the team is new to this, **Ops will maintain runbooks** for the pipeline. These include stepby-step "Deployer Checklist" documents for the person executing deployments: e.g. **Pre-deployment:** confirm backup taken, ensure no schema drift, announce deployment in #database-dev channel; **During:** monitor Octopus logs, watch for any errors or long-running operations, be ready to abort if needed; **Post:** verify application sanity, do OutSystems refresh, communicate success [77](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=9,OPS/) . These checklists (stored in our ops wiki) are followed for each Test/Prod release to ensure nothing is missed, especially while the process is new.

# Developer Workflow: Pull Requests, Code Reviews, and Merges

All database changes must go through the **pull request (PR) process** - this is a cornerstone of our governance. Developers will work on feature branches for their changes and create PRs to merge into the main branch (usually master or main in Git) 78 79 . Let's break down how to effectively manage PRs in this new workflow:

## Preparing a Schema Change Pull Request

When your database changes are ready and tested locally, commit them with clear messages (e.g., "Add PhoneNumber to Customer - requires refresh" rather than "minor db changes"). Then open a **Pull Request in Azure DevOps** (or GitHub, depending on our repo) targeting the main branch. **Use the PR template** provided, which prompts for key details 38 :

- **Description:** Summarize what and why. E.g., "Adds PhoneNumber column to Customer table for new feature X. Backfilled with default 'N/A' for existing records." This helps reviewers understand context.
- **OutSystems Impact:** Note if OutSystems modules need changes. E.g., "Update extension

Customers after deploy to get new column." If the change is breaking (removing/renaming something OS uses), mention how you've handled it (perhaps a coordinated OutSystems change or feature toggle).

- **Backward Compatibility:** State if the change is backward-compatible (most additive changes are). If not, it's a "breaking change" and should likely be tied to a feature flag or coordinated deployment.
- **Testing Done:** Briefly note how you tested (e.g. "Built project locally and deployed to Dev sandbox, verified new column appears and default applied correctly").

Also, link the **user story or work item** (in Azure Boards/Jira) in the PR. This ensures traceability from requirement to schema change 80 . In our process, every DB change should tie to a business need - no random schema tweaks. The PR title might also include a tag like \[DB\] Feature X for clarity.

**PR Timing:** As mentioned, aim to open the PR by the daily cutoff (2:00 PM for dev environment train) if you want same-day deployment 37 . If it's after cutoff or you mark the PR as _draft_, it will go the next day. Do not mark a PR "ready" unless you are confident it's complete and build passes. No one should merge WIP (work in progress) changes to main 81 , as that can disrupt others.

When you create the PR, our CI system automatically triggers a build of your branch or the PR. The PR will show a **build status check**. If it's red , _stop right there_: fix the build errors and push an update before expecting any review. Reviewers will not look at a failing PR (and indeed we have a policy that you cannot merge if CI is failing) 39 . Common build failures at PR stage might be forgetting to include a newly added .sql file in the project (thus references break) or simple syntax issues.

## Code Review and Quality Gates

Each PR requires at least one approval from a **Dev Lead or senior developer** (our repo likely has branch policies enforcing this). The code reviewer's job is to catch issues, ensure standards, and mentor through feedback. We use a **Code Review Checklist** to guide reviews, covering: correctness, adherence to standards, and safety.

Here are highlights of what reviewers check (and thus what you _should_ self-check before PR):

- **Build and Correctness:** First, is the project build passing? (It should, or else review will be brief: "please fix build errors"). Next, does the change actually do what the user story requires? Reviewers will read through the create scripts and post-deploy scripts to mentally simulate the outcome. If you added a column, did you also handle populating it if needed? If you changed a data type, did you update related defaults or foreign keys? They'll ensure no obvious logic mistakes (like using wrong data type length, etc.).
- **Safety and Data Integrity:** This is critical. Reviewers look for any operation that might risk data loss or downtime. Examples: If a column is set to NOT NULL on an existing table, did you provide a DEFAULT or ensure the table is empty? **No PR should introduce a "NOT NULL without default on a non-empty table" situation** 42 . If renaming a column, did you use the refactorlog or is the diff showing a drop/create (which would drop data)? 52 . If a table or column is being dropped (rare in PRs, but possible in cleanup stories), is there a migration plan? Often, destructive changes would be rejected unless it's clearly safe or part of a larger plan. We have automation that flags forbidden operations - e.g., our build pipeline may generate a diff script and run a regex scanner to find DROP statements, aborting if found 61 82 . The reviewer will also manually scan for these. Essentially, **reviewers act as the last line of defense** against accidental data loss or breaking changes in Prod.
- **Standards & Best Practices:** We have a defined set of schema conventions for consistency. Reviewers will verify that: tables are named in singular form (e.g. Order , not Orders ) 40 , columns use PascalCase (no snake_case or lowercase) 40 , primary keys follow our naming (e.g.

TableNameId ) and have identity where appropriate 60 , foreign key constraints and indexes follow naming patterns ( FK_Child_Parent , IX_Table_Column ) 60 . They'll flag any deviations ("Please rename column XYZ to PascalCase" or "Use singular for table name"). They'll check data types - for instance, **use NVARCHAR for text to support Unicode** (not plain VARCHAR) 83 , use DATETIME2 instead of legacy DATETIME 84 , use appropriate numeric types (e.g. DECIMAL(10,2) for currency, avoid FLOAT) 85 . These guidelines are in our standards document and reviewers will enforce them to maintain data quality.

- **Indexes and Keys:** If you added a new table or new foreign key, did you include an index on the FK?

40 If you added a new column that will be searched or filtered on, did you consider an index? They might not reject the PR for a missing index, but will ask if it's needed. We prefer to add necessary indexes as part of the change rather than wait for a performance problem later.

- **Performance Considerations:** For more complex changes, reviewers also think about performance. A classic example: if someone writes a post-deployment script to update a million rows in one transaction, a reviewer might suggest breaking it into batches or doing it in a controlled way to avoid timeouts. They'll check if new queries (in a proc or view) are well-indexed or if they use patterns like

SELECT \* (which we ban in code) 23 . If a change might impact production performance (like adding a very wide index or changing a column to a larger size), they might ask for metrics or testing.

- **OutSystems Integration:** Since this is an OutSystems team, reviewers ensure the PR addresses the OutSystems side. They'll look at the PR description for notes on refreshing Integration Studio and any needed changes in the OutSystems application. For example, if a column is renamed, the reviewer might ask "Did you also update the OutSystems module to use the new name?" If a new table is added, "Make sure to create a new OutSystems entity for this external table and use it in your module." We treat the DB and OutSystems as an integrated unit, so a DB schema PR that has corresponding OutSystems changes should ideally be linked with those changes (if possible). In practice, that might mean the OutSystems application changes are in a separate repo - so at minimum note the dependency and ensure timing is coordinated.
- **Automated Checks (Diff & Scanner):** As mentioned, our pipeline can attach a **schema diff report** to the PR. Reviewers will open that artifact which shows the exact SQL actions that will be taken (the

publish.sql ). It's a great sanity check. If they see anything unexpected (like an ALTER dropping

a default or an unwanted collation change), they'll bring it up. Also, the **forbidden-drop scanner results** are visible - if it detected something, the PR might be auto-marked with a warning. For instance, if your change inadvertently drops a foreign key and recreates it (sometimes happens if you reorder columns in a key), it might flag that. The reviewer will help determine if that's benign or needs a fix. We also have checks for idempotency: e.g., if a post-deploy script isn't idempotent, a script analyzer might warn. The PR must pass all these checks before merge 74 .

A reviewer's feedback will typically be in comments on the PR. **Developers should address each comment**, either by making changes and pushing new commits or by replying with clarification. Our culture encourages collaborative review - ask questions if you don't understand a suggestion. The goal is not only to catch issues but to uplift everyone's knowledge. Don't take feedback personally; even senior devs will get detailed reviews early on 86 . Over time, as the team internalizes these standards, reviews will become quicker.

## Merging and Etiquette

Once the PR is approved (and all checks are green), it can be merged. We follow a **"No self-merge" rule** - you should not merge your own PR; let the reviewer or another team member merge after approval. This ensures a second set of eyes up to the last moment. We also **squash commits** on merge for database changes to keep the history clean (one cohesive commit per PR, with the PR number in the message). The main branch then gets the changes, triggering the CI build of the DACPAC as described.

After merging, _do not delete your branch immediately_ if it's a big change - occasionally we might need to refer back or do a hotfix branch off it. But do clean up stale branches eventually.

If the PR missed the train (merged after 2pm or in the evening), it will go out in the next scheduled deployment. If it's urgent, coordinate with leads - we discourage off-cycle deployments in Week 1 unless absolutely necessary (stability first), but we have procedures for urgent hotfix deploys (requires Ops and lead approval to run an unscheduled pipeline) 27 87 .

In summary, treat PRs as a learning and quality gate: they're there to protect our apps and data. By diligently going through review, we drastically reduce the chance of serious issues reaching production.

# Operational Roles & Responsibilities by Team Role

A successful DevOps transition requires clarity in each team member's role. Below we outline the "swim lanes" for various roles - from individual contributors to leads, QA, Ops, and PM. This ensures everyone knows their part in the new process and nothing falls through the cracks.

## Junior Developers (IC - Junior)

**Role:** Junior OutSystems developers with little or no prior database DevOps experience. They are the primary learners in this cutover.

- **Primary Responsibilities:** Implement schema changes for their feature work under guidance. Focus on safe, additive changes at first (e.g. adding tables or columns that don't impact others) 88 . Follow the process diligently: always create PRs, get reviews, run builds, and refresh OutSystems. By the end of Week 1, every junior should have successfully added at least one column or table via SSDT, gotten it deployed, and performed the Integration Studio refresh 48 . This builds confidence. In daily practice, juniors should attend the daily train activities (merging by cutoff, doing the refresh) and not miss critical steps (like the 3:30 refresh window) 89 .
- **Growth and Limitations:** Juniors are **not expected to handle complex or risky tasks initially**. For example, they should avoid being the point person for production deployments or tackling a tricky refactor solo 90 . If a production issue arises, their job is to escalate to seniors/leads and assist if asked, but not to attempt fixes alone 90 . They also wouldn't design major schema changes without review. Essentially, juniors stick to the playbooks and seek review on everything. Over time (first 1-3 months), they will expand their scope. We have set **milestones**: by end of Month 1, a junior should handle ~80% of common changes (like basic add/modify) with less help 91 ; by Month 3, they should be nearing mid-level capability (comfortable with not-null additions with defaults, resolving simple merge conflicts, etc.) [92](file://file_00000000173461fd85268df92fdd32e9#:~:text=,112/) . They will also start reviewing each other's small PRs as a learning exercise
  - .
- **Best Practices:** Juniors should _always ask when unsure_. They should use templates and checklists religiously (we have a **Developer PR Checklist** artifact reminding them of naming, build checks, etc.
  - ). Pair programming with a mid or senior on first few changes is encouraged. Also, juniors should spend time in Week 0/Week 1 learning - e.g., doing the "Hello SSDT" practice, reading the provided materials, and even intentionally breaking something in a safe environment to see what errors occur
  - . This hands-on learning cements the training.

## Senior Developers (IC - Senior)

**Role:** Seasoned developers (and likely some "Staff" or principal engineers) who have deeper SQL knowledge and maybe prior DevOps experience. They act as pillars of expertise and leadership without formal managerial authority (unless they are also leads).

- **Primary Responsibilities: Technical oversight and mentorship.** Seniors handle the most complex changes independently - e.g. intricate refactorings (splitting/merging tables), large data migrations, performance tuning of queries, etc. 96 97 . They are comfortable planning multi-step deployments for zero-downtime, coordinating changes that span multiple systems, and quickly troubleshooting any weird build or deploy issues. On a day-to-day basis, seniors are the primary code reviewers for database PRs, ensuring quality and standards. They provide meaningful feedback and catch subtle issues juniors might miss 98 . **Mentorship** is key: seniors should actively help upskill juniors and mids - via pair programming, answering questions, and even hosting short knowledge sessions ("brown bags" on topics like indexing or query optimization) 99 . They also contribute to refining our process and playbooks: if they see a recurring issue, they propose improvements or document a new best practice.

**Operational Duties:** Many seniors will participate in the deployment rotation. Initially, Dev Leads might run the deploys, but seniors should be capable of executing the daily 3 PM deployment to Dev by Week 2 or so, following the runbook 100 101 . Seniors typically handle **production deployments** or at least are present for them 102 103 . In our policy, mid-levels might assist, but seniors or leads are the ones to push the "Go" button in Prod because they have the experience to handle any issue that arises 104 102 . In incident management, a senior is the first on-call responder - if something goes wrong with a DB deployment or performance in Prod, seniors jump in to triage and fix or rollback as needed 103 . They then coordinate with leads on post-mortems to ensure lessons are learned 103 .

- **Decision Making:** Seniors have authority to make many technical decisions (like how to model a tricky relationship, when to introduce a new index, etc.), but they still align with the Dev Lead for bigpicture decisions. They are expected to use the **decision frameworks** we've set (e.g., whether to use a view vs direct table might be decided by a senior on a case, following guidelines in this doc). If a senior wants to introduce a new standard or deviate from one, they discuss with the Lead/Architect before proceeding 105 . This ensures consistency. Seniors also often act as translators between developers and management - they can articulate technical risks to PMs (e.g. "we need a maintenance window for this change") and guide the team on what's realistic.
- **Mentorship & Culture:** A big part of the senior role is cultural: fostering DevOps mindset in the team. They should lead by example in following processes (not bypassing reviews even though they could) and in writing high-quality documentation. When juniors see seniors rigorously testing and reviewing, it sets the norm. Seniors should also encourage blameless post-mortems - if a junior's change caused an issue, use it as a learning moment, not finger-pointing. In retrospectives, seniors contribute insights ("We saw 3 forbidden-drop attempts last week; maybe we need a training session on how to rename safely"). This continuous improvement mindset is crucial in the early stages of adoption.

## Development Leads / Architects

**Role:** Dev Leads (and any designated architects or staff engineers) oversee the technical direction and ensure the process is followed. They are accountable for the success of the transition and the health of the system.

- **Primary Responsibilities: Governance and coordination.** Leads own the **release train schedule** and make sure everyone sticks to it (e.g. reinforcing the 2pm PR deadline, managing scope of each daily release). They often serve as the "release captain" initially, running the deployment or at least supervising it 43 . In code reviews, leads typically review the more critical or complex changes and approve what seniors forward to them if needed. They ensure that all **quality gates** are met: no PR merges without approval, all scanners passing, etc. They also monitor metrics of the process - e.g. build success rates, how many issues are caught in PR vs after deploy, etc., as part of governance. If something is off (like frequent build breaks or many last-minute PRs), the lead addresses it through coaching or process tweaks.
- **Architecture & Standards:** Leads (with architects) define the **standards and design patterns**. For instance, the decision framework on when to use a view vs direct binding (discussed later) might come from the lead 106 . They set the rules like "all tables must have a clustered index" or "no more than 5 indexes per table unless justified," etc. The lead is the final say in design debates. For example, if two devs disagree on how to model something, the lead will decide the approach that aligns with our principles (like normalization vs performance trade-offs). They also ensure our naming conventions and practices stay consistent as new scenarios come.

**Mentoring & Team Enablement:** Leads mentor not just individuals but guide the whole team's progression. They organized the initial training (surveys, tool setup) 20 and will continue to identify skill gaps to fill. If many devs struggle with, say, writing post-deploy scripts, the lead might arrange a workshop on T-SQL scripting. Leads also encourage knowledge sharing: they might ask a senior to host a session or write a blog post on our internal Confluence about a tricky topic encountered. Another aspect is **removing roadblocks**: if a dev is stuck on environment access or a pipeline permission, the lead works with Ops to fix that quickly.

- **Coordination with Other Stakeholders:** Dev Leads act as the bridge to Product Management (PM) and other teams. They communicate the plan and status of the cutover to PMs, ensuring everyone knows when the system will be ready or if any downtime is needed. They also coordinate with any central DBA or IT security teams - for instance, if certain changes need DBA approval or if there are data governance rules, the lead handles that communication. In case of incidents or major schema changes, the lead might need to get sign-off from an Architecture Review Board or notify a Change Advisory Board (CAB) if one exists. We expect leads to keep the **big picture** in mind: ensuring the changes to the database don't negatively impact other systems, and aligning our OutSystems database strategy with enterprise standards.
- **Final Escalation Point:** In operations, if an issue escalates beyond the on-call senior, the Dev Lead is the final decision-maker. For example, if a production deployment has a problem and it's unclear whether to rollback or fix forward, the lead decides (in consultation with others) and owns the outcome 107 . They have the authority to approve an exceptional direct fix (with follow-up) if absolutely needed, or to delay a release if quality isn't there. The lead's focus is on managing risk while delivering value, so they'll err on the side of caution for production changes in Week 1. Leads also ensure any incident leads to a follow-up action (update guide, add a new check, etc.), thereby continuously improving our DevOps maturity.

## Quality Assurance (QA) / Testers

**Role:** QA engineers or SDETs ensure that the application still works correctly after these new DB deployment processes. Their role intersects with both testing the outcomes of DB changes and adapting the testing process to this faster DevOps cadence.

- **Primary Responsibilities: Verification and validation.** Whenever a DB change is deployed (particularly to Test or UAT environments), QA needs to verify that the change has been applied correctly and that the OutSystems application functions with it. For example, if a new column

PhoneNumber is added to Customer , QA should verify that in the application's screens or logic, the new field is accessible (if it's supposed to be used) and that nothing else broke (no regression in related functionalities). QA will likely expand their test cases to cover scenarios around the new schema: e.g., create a new customer and ensure phone number saves correctly, or if a column was renamed/removed, ensure the app no longer expects the old column.

- **Post-Deployment Testing:** After each daily **Dev deployment and refresh**, QA (if we have QA in dev) might do a quick smoke test on the dev environment to catch any obvious issues early. More formally, when changes are promoted to **Test/UAT**, QA runs the regression tests. Because DB changes are more frequent now, QA may need to perform _targeted testing_ of the changes rather than full regression each day. A good practice is the **"buddy testing"** approach: the developer who made the change can demo or explain it to a QA, and the QA designs tests specifically for that change. QA should also verify non-functional aspects: e.g., if a data migration script ran (post-deploy), did it produce the expected results? They might run queries to spot-check data (if given access) or ask developers to provide before/after counts.

**Data Quality Checks:** QA can play a role in ensuring that reference data and static data migrations are correct. For instance, if we migrated a static entity to an external lookup table, QA can compare

that all expected values exist in the new table, and the application behaves the same using the external data. They might use both the OutSystems UI and direct queries (with help from dev) to validate. If any discrepancy is found (missing values, wrong default), QA logs a defect.

- **Automation Updates:** If the team has automated tests (unit or integration tests in OutSystems, or database unit tests), QA/SDET should update those as needed for the new structure. For example, an OutSystems BDD test that was verifying an old field might need updating to the new field name. Additionally, if static data was assumed in tests, the source might change if now it's in an external table - test data setup might need adjusting. QA should work closely with dev to incorporate test adjustments in the same release train when possible.
- **Process Adaptation:** QA also has to adapt to the **faster deployment cadence**. In the old model, DB changes might have been infrequent and manually coordinated. Now with potentially daily changes, QA must be plugged into the process: following the release train schedule, knowing what changed each day (reading PR descriptions or a change log), and being ready to test those changes quickly.

We might have QA attend the daily standup or the 3:30 refresh meeting to get context.

Communication is key; QA should not be surprised by a change. The Dev Lead/PM should ensure QA is in the loop on the scope of each deployment. If a change is behind a feature toggle or should not yet be tested by end-users, QA should know that too.

- **Quality Gate in Pipeline:** In some setups, we might require QA sign-off before promoting to Prod. Practically, that means QA must finish testing in UAT and mark the story as passed. The PM/Lead will only schedule the Prod release if QA is green. QA should be diligent in **regression testing after**

**Week 1** because the first deployments might reveal unexpected issues (e.g., maybe a certain OutSystems report fails because of a database change). They should also pay attention to performance tests - e.g., did adding that index improve the query as expected, or any page load times changed? If QA notices a trend (like repeated bugs due to developers forgetting something), they should raise it in retrospectives so the team can address via training or checklist. In essence, QA serves as the safety net verifying that despite all our automation, the _functional behavior_ and _data integrity_ meet expectations.

## Operations / Release Engineering (Ops/DBA)

**Role:** The Ops team members or DBAs are responsible for the reliability of deployments and the health of database environments. In this context, they maintain the CI/CD infrastructure (Azure DevOps, Octopus) and often execute or monitor the actual deployments, especially for higher environments.

- **Primary Responsibilities: Deployment management and environment upkeep.** Ops takes charge of running the deployment pipeline and ensuring it's configured correctly. For each release to Test or Prod, an Ops engineer (or DBA) will follow the **Deployer Checklist** [77](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=9,OPS/) : before deploying, confirm a backup or snapshot is in place, ensure users are notified or a maintenance window is scheduled (if required), and check that no conflicting deployments or maintenance are happening (a "lockout calendar" for the DB) [77](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=9,OPS/) . During deployment, Ops monitors the script execution in Octopus - watching for errors or long-running steps. They verify the number of rows affected by significant operations (for example, if we expected a post-deploy script to update 1000 rows and the log says 100,000, that's a flag). After deployment, Ops ensures that Integration Studio refresh has occurred

(or coordinates with dev to do it) and that the application is smoke tested before declaring success. They often are the ones to communicate "Deployment successful" or "rolled back" to stakeholders.

- **Pipeline & Tooling Maintenance:** Ops also maintains the Azure DevOps build agents and Octopus configurations. If the build pipeline needs an update (say, adding the forbidden-drop scan script), Ops implements that (in collaboration with leads) 82 108 . They manage credentials and security:

e.g., rotating the SQL service account passwords in Octopus, making sure the deployment agent has least privilege needed (for example, maybe the deployment uses a SQL user with db_owner on target DB only, nothing more). Ops will also set up new publish profiles or environment variables as we add environments or change settings. Essentially, they are custodians of the CI/CD infrastructure - ensuring it's running smoothly, updated, and properly logging everything.

- **Backups and Recovery:** A critical Ops duty is ensuring that robust backups exist in case something goes wrong in production. Before the cutover and each Prod deployment, Ops/DBA should confirm a recent backup is available (and ideally do a fresh one right before a major release, as mentioned). In a worst-case scenario, if a deployment fails disastrously, Ops would be the one to restore the backup or assist in recovery. They also help test our rollback procedures occasionally to make sure we can meet RTO/RPO targets. Ops might also manage any database snapshots for quick rollback in nonprod environments if we use that for test resets.
- **Monitoring & Performance:** With frequent deployments, monitoring is key. Ops should ensure that database monitoring tools are in place (for CPU, memory, slow queries, deadlocks, etc.) to catch any performance regression introduced by a change. For instance, if a new index is added, does it help or hinder performance? If a deployment's data migration runs long, did it cause blocking or timeouts? Ops will review system logs and maybe capture baseline metrics to compare. If issues are found (like a spike in deadlocks after a release), Ops will alert the dev team to investigate. They might also assist in performance tuning by providing execution plan insights or enabling trace flags if needed.
- **Security & Compliance:** Ops/DBA also ensures that the new process complies with any security guidelines. For example, if the company requires separation of duties, perhaps only Ops (not devs) have Prod deployment permissions - which Octopus can enforce via role-based access. Ops would handle the secret management (ensuring connection strings in Octopus are encrypted, etc.). If any GDPR or PII concerns exist with data changes, Ops ensures that audit logs are kept. They also maintain user accounts/permissions on the database; e.g., if new schemas or objects are introduced that apps need permissions on, Ops might apply those (though we often automate grants in postdeploy scripts).
- **Troubleshooting Support:** When deployments fail, Ops are the first to triage on the infrastructure side. For instance, if Octopus fails to connect to a SQL instance, Ops checks network or firewall issues. If a script fails due to a SQL error, devs diagnose it, but Ops may help by providing logs, or if a partial deployment occurred, Ops can use their DB skills to examine the state and assist in either rolling forward or back. They maintain the **Troubleshooting Runbooks** for known failure scenarios (like "what to do on a DACPAC data loss block" or "how to kill a long-running script") 109 . They contribute these to the team knowledge base so that we handle incidents consistently.

In summary, Ops ensures that the machinery of deployment runs efficiently and safely, allowing devs to focus on writing changes with confidence that the pipeline will catch issues and deploy properly.

## Product Managers (PM) / Project Managers

**Role:** PMs or project managers oversee timelines, scope, and coordinate cross-functional aspects. They are not hands-on with the tech, but their support is vital for a smooth transition.

- **Primary Responsibilities: Timeline management and coordination.** The PM ensures that the cutover plan (Week 0 and Week 1 activities) fits into the overall project timeline. For instance, they made sure that development work was light around the cutover weekend to allow time for training and setup. If any features had database changes needed, they coordinated to possibly complete them before freeze or delay them until after cutover as needed. During Week 1, the PM monitors if any deliverables are at risk due to the team learning curve and adjusts expectations with stakeholders. They are essentially the _air traffic controller_: ensuring no major launches collide with the cutover, communicating to business stakeholders that a new DB deployment process is being implemented (perhaps explaining any minor delays or risks associated).
- **Facilitating Communication:** PMs help keep everyone aligned. They might schedule extra check-ins or stand-ups during the transition to make sure issues are heard. They ensure QA, Ops, Dev, business are all on the same page regarding deployment schedules (e.g., making sure a UAT tester knows that at 4pm the DB will be updated and they should refresh their environment, etc.). If something goes wrong (say a deployment fails and we need to use a maintenance window), the PM communicates impacts (like downtime extended) to stakeholders so developers can focus on fixing. They also capture any decisions or process changes in documentation (or delegate that). For example, if we decide to adjust the PR cutoff time based on the retro, the PM updates the official project plan or team agreement doc.
- **Risk Management:** A key role of the PM is to track and mitigate risks. In such a transition, risks include: team not fully ready by cutover (mitigated by training and practice - PM ensured those happened), potential production issues (mitigated by scheduling cutover in a low-traffic period and having rollback plans - PM ensures business sign-off on a possible maintenance window), or slower velocity as team learns (mitigated by reducing scope for that sprint and focusing on technical debt learning). The PM likely worked with the team to define the **success metrics** (like zero prod incidents in week 1, velocity back to normal by week 4) and will monitor those 8 . If metrics are not met, they convene discussions on how to course-correct (additional training? more resources?).
- **Enforcing Governance from a Process Side:** While leads enforce technical governance, PMs ensure process governance. For example, if our governance model says every change needs a linked ticket and approvals, the PM audits that those are happening (maybe in retros they check: "hey, we had two hotfixes directly in DB, why?" and logs a process deviation). They also schedule the **release trains** in broader terms. Perhaps outside of Dev daily deploys, there's a plan like "We will do UAT deployments every Friday, and Production every second Monday." The PM coordinates these with business releases, ensuring features align with train schedules. If a certain big feature needs a special release, they evaluate the impact on the train model and negotiate adjustments.
- **Stakeholder Updates:** Last but not least, PMs communicate upwards - to product owners, clients, or management - about how the cutover is going. They might prepare a brief for an executive like "We successfully cut over to the new database DevOps process. So far, 10 schema changes have been deployed with no incidents. Developers are adapting well, with only minor delays in the first few days which are improving 8 . We anticipate normal development speed by next sprint." This keeps confidence high in the initiative. If external teams rely on our database, the PM also informs them of any new protocols (like if they should no longer directly edit the DB, or how to request schema changes through us, etc.). Essentially, PMs handle all the non-coding aspects that nonetheless are crucial for project success.

Each role above contributes to the shared goal: a smooth, safe transition to the new workflow and ongoing efficient operations. We've explicitly clarified roles to prevent confusion - e.g., Devs know Ops will handle backups, QA knows to test after refresh, PM knows what to communicate if a deployment is postponed. **When everyone performs their role and collaborates, the cutover and subsequent releases will go much more smoothly** 110 **.**

_(Note: In some teams one person may wear multiple hats - e.g., a Tech Lead might also be the PM for the transition, or a senior dev might fulfill some Ops tasks if no dedicated Ops. In such cases, ensure that person understands all responsibilities of each role they cover.)_

# Governance Model: Release Trains, Quality Gates & Idempotency

With the new process, we enforce a governance model to maintain consistency and reliability. Key aspects of this model include a **release train schedule**, structured checklists and gates at each step, and policies to ensure deployments are repeatable (idempotent) and safe. This section details how we govern the database DevOps pipeline.

## Release Train Timing & Cadence

We have adopted a **Release Train** approach for database deployments, meaning deployments happen at fixed, agreed times rather than ad-hoc whenever a developer feels like it 35 . In Week 1 (and likely continuing afterward for Dev environment) we use a **daily train**: code cutoff at 2:00 PM, deployment at 3:00 PM, refresh at 3:30 PM 37 43 . This cadence imposes discipline - developers plan their merges by the cutoff, reviewers know when to be available, and everyone knows when to expect new changes in the dev environment. It prevents the chaos of multiple people deploying conflicting DB changes throughout the day, which could break the integration environment unpredictably.

For higher environments (Test/UAT, Prod), we plan less frequent trains, typically **weekly or on-demand with approvals**. For instance, we might designate _Thursdays at 4 PM_ for UAT deployments (giving QA Friday to test) and _Mondays at 8 AM_ for Production (ensuring Ops and dev leads are fresh and available to monitor). We avoid Fridays for Prod releases in this early stage to have more support on hand if something goes wrong. The exact schedule can be adjusted based on our product release calendar but will be communicated clearly to all stakeholders. The PM/Dev Lead will publish a calendar of "Train departures" so everyone aligns their work.

**Off-Cycle Releases:** We strongly discourage off-cycle or unscheduled DB deployments, especially to Prod. If an urgent hotfix is needed (e.g., a critical bug requiring a schema change), it must get approval from both **Ops and the Dev Lead** to run outside the train 27 87 . The reasoning is that off-cycle changes are riskier (less time to review/test, and might happen with fewer people around if at night). If one is done, a postmortem is required to document why and how to avoid needing it in the future 27 87 . This governance ensures that the "train schedule" remains reliable and not undermined frequently.

During Week 1, we also implemented **"Narrated ops"** where leads/seniors talk through deployments and capture FAQs 47 . This was a one-time thing to bootstrap knowledge, but it set the expectation that even after Week 1, communication during deployments remains high (e.g., on Slack: "Deployed DB, now refreshing OS, all good.").

## Checklists and Quality Gates

At every stage of the pipeline, we have checklists and gates to enforce quality:

- **Developer Checklist (Pre-PR):** Before even making a PR, developers must go through an internal checklist. This includes: _Did I run a local build and fix errors? Did I follow naming standards? Did I write or update unit tests if any? Is my post-deployment script idempotent? Did I double-check that adding this column won't break a view or OutSystems aggregate?_ We have a documented **Developer PR Checklist** that covers naming, data types, FK indexes, idempotent seeds, etc., which every developer is expected to mentally (or physically) check off 94 . This reduces trivial findings in code review and improves first-pass quality.
- **Pull Request Template & Gates:** As discussed, the PR itself has required fields (description, linked work item, etc.), and it won't allow merge until certain conditions are met: at least one approval, and **all automated checks pass**. The automated checks (gates) include: **CI Build = Passed**, **Forbidden Drop Scan = Passed**, and any others we add (like linting) 111 . The _forbidden-drop scanner_ specifically scans the generated deployment script for operations we ban, such as DROP TABLE , DROP COLUMN , or altering columns to a smaller size (data loss) 61 . If any are found, the PR pipeline fails and outputs the details of the forbidden operation. In Dev/Test, this is a warning that must be addressed or overridden; in Prod pipeline, it's an outright block 72 . To get around it (for legitimate cases like dropping a truly obsolete table), a lead would need to temporarily allow it via configuration or a special approved refactor procedure. This guard has already caught a few "oops I accidentally dropped instead of renamed" in testing. Another check we have is ensuring a **RefactorLog entry for renames** - essentially if the diff shows a DROP+CREATE for something that looks like a rename, that triggers scrutiny; our checklist and leads enforce 100% refactorlog coverage for renames 112 113 .
- **Deployment Checklist (Ops):** For each deployment event, there's an Ops checklist to follow [77](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=9,OPS/) . Key items: _Pre-deployment:_ confirm backup done, ensure all pending PRs are merged or explicitly deferred, notify team ("deployment starting, please don't publish any OutSystems modules until done"), check that the target DB is in expected state (e.g., no one left uncommitted schema changes). _During:_ monitor for any errors, keep an eye on performance (if an unexpected lock or slow query appears, be ready to intervene or extend timeouts). _Post-deployment:_ verify the Octopus step shows success for all scripts, then do smoke tests (Ops might run a quick query or have a script that hits a health-check endpoint of the app). Then proceed to Integration Studio refresh and confirm all extensions updated properly (perhaps maintaining a list of which ones were expected to be refreshed). Finally, announce completion ("Deployment completed and verified") in the channel or email. This Ops checklist is a crucial governance doc - it makes sure no step (like forgetting to refresh, or forgetting to do a backup) is missed. We will refine it as needed in retrospectives.
- **Promotion Gates:** For promoting to Prod, we may have additional gates: e.g., requiring that the release was deployed and tested in UAT, and QA sign-off is recorded. Octopus can enforce this via a manual intervention step where QA or product owner clicks "Proceed" after testing. Another gate is change management approval - if our org requires a CAB approval for prod changes, the PM/Lead ensure that is obtained and documented (like a change request ID perhaps noted in the release). We also ensure _no emergency changes made it to Prod without being in the project_: an Ops checklist item for Prod is to run a _Drift Report_ (SqlPackage's DeployReport action) comparing the Prod DB to the latest project DACPAC _before_ deploying, to confirm they match except for the intended changes. If drift is found (someone tweaked prod outside process), we halt and reconcile. This is to enforce that prod schema is only changed through approved means.

All these gates and checklists might sound heavy, but they establish trust in the process. They catch mistakes when the cost is low (during build or in Dev) rather than when the cost is high (in Prod).

## Idempotency & Repeatable Deployments

A core principle of our DevOps approach is **idempotency** - a deployment can be applied multiple times and yield the same end state without side effects. This is particularly important for automation and rollback strategies. We enforce idempotency in several ways:

- **State-based DACPAC Deployment:** By nature, the DACPAC publish is idempotent _on schema_: if you run it against the same schema twice, the second time it finds no differences and does nothing (other than maybe update some internal version stamp) 45 . This property relies on not making sneaky changes outside the project. If someone manually added a column in an env, then running the DACPAC might treat it as an unexpected difference (extra column) and try to drop it to conform to the project. To guard against that, as mentioned, we avoid out-of-process changes, and our pipeline drift check would catch such discrepancy before applying.
- **Idempotent Scripting (Pre/Post):** All custom scripts we write must be idempotent. This is part of the PR checklist (e.g., if you write a INSERT in PostDeploy for seed data, make it INSERT IF NOT EXISTS or use a MERGE ) 54 . If you write an update script, perhaps include logic "only update if not already updated" or safely handle re-run. We have even introduced a practice of doing a **double deployment test** in Dev: after deploying, run the deployment again immediately (with the same DACPAC) and ensure it reports success with 0 changes applied. This "double-run" acts as an idempotency proof 114 74 . In our metrics, we aim for near 100% of deployments being clean on a double-run (PostDeploy scripts can sometimes be tricky, but we strive for it) 115 .
- **Publish Profile Settings:** Certain settings help idempotency. For example, we ensure "Generate DROP statements for objects that are in target but not in source" is on (so that removing something from the project does drop it - except blocked by data loss flag in prod). We use

"BlockOnPossibleDataLoss" in higher env to prevent unintended drops. For Dev, we might allow drops but we do include "Treat verification errors as warnings" in Dev so that if something doesn't match exactly, it can still proceed for flexibility; but in Prod, any drift or diff must be accounted for. These settings ensure we don't accidentally skip something.

- **Idempotent Deploy Environment:** We ensure that each environment's starting state is known and that each deployment brings it to a known state. We discourage manual data patches outside of deployments because those could make a deployment non-idempotent (e.g., if a post-deploy script assumes some data state that a manual patch violated). If such things happen, we incorporate them into scripts.
- **Testing Roll-forward & Rollback (in lower env):** We test scenarios like applying two deployments in a row, and sometimes even simulate a rollback by applying an old DACPAC over a new schema in Dev to see what happens. This flushes out issues like "Oh, if we re-run the old DACPAC it might drop a new table" - which is expected, but we consider how we'd handle that in Prod (we probably wouldn't run an old DACPAC in Prod; we'd restore backup instead). Nonetheless, practicing these in Dev/UAT gives us confidence and uncovers if any script is not handling reruns.
- **Monitoring Idempotency Metrics:** We actually track a few metrics around this as part of our quality KPIs: e.g., _"PostDeploy idempotency coverage 95%"_ meaning 95% of our releases had post-deploy scripts that could run twice without issues 115 . If we find scripts that aren't (maybe a developer forgot a guard on a delete or something), we address it. We also track if any release had to use a special override (like disabling BlockOnPossibleDataLoss) - those are flagged and require lead signoff with a plan (like we allowed a drop after confirming it was safe).

In essence, idempotency is about being **able to run the same process repeatedly with trust**. This makes our CI/CD robust - if a deployment partially fails, we can fix and run it again, confident it will pick up where it left off or do no harm if it already applied something. It also aids in disaster recovery: re-deploying the last known good DACPAC should ideally bring a restored database (from backup) up to current state without issues. Not all operations can be inherently idempotent (e.g., data migrations that insert data could doubleinsert if not coded carefully), hence our emphasis on coding them carefully.

## Governance Reviews and Audits

As part of the governance model, we will conduct periodic reviews:

- **Weekly Team Retro:** As mentioned, end of Week 1 we did a retrospective 46 , and we'll likely do weekly for the first month, then bi-weekly/monthly. In these, we evaluate process adherence (did we follow the train schedule? Were any reviews rushed? Any incidents?). We adjust policies accordingly. For example, if pull requests are often coming in last-minute, maybe we set an earlier cutoff or remind devs in daily standup. If OutSystems refreshes are chaotic, maybe assign a dedicated person each day to coordinate it. The idea is the governance model is a living thing we refine together.
- **Metrics & Reporting:** The lead/architect will gather metrics such as: PR lead time (from PR open to merge), deployment frequency, number of production issues due to schema, number of times the forbidden-drop scanner blocked something (and why), etc. Some metrics noted in planning include _"forbidden-drop blocks (by env) - target 100% blocked in Prod, 100% flagged in UAT"_ 116 , or refresh SLA (how quickly after DB deploy was OS refreshed). If metrics show problems (e.g., refresh taking too long or some drift issues), governance might tighten (like making refresh part of the automated pipeline or introducing stricter checks).
- **Documentation & Audits:** We treat the documentation (this guide and related playbooks) as part of governance. It should always reflect the latest agreed process. The Dev Lead or an appointed "Librarian" will update the docs when decisions are made (like adding a new standard). Auditors (internal or external) can look at our process docs and pipeline logs to ensure we meet any regulatory requirements for change management. For example, an ISO audit might want to see that all prod changes are approved and logged - our PR reviews and Octopus release logs provide that trail. The PM/Lead ensure those records are kept (for instance, we might archive PRs and pipeline results for X months).

In summary, the governance model is about **safety, consistency, and continuous improvement**. By having set trains and checklists, we reduce variance in how deployments are done. By enforcing review gates, we catch issues early. By insisting on idempotency and tracking it, we make our process resilient. All of this is especially important as we transition from a less formal process (manual DB changes) - it might feel strict, but it is what will allow us to move fast _without breaking things_.

# Common Pitfalls and Troubleshooting Guide

Despite careful planning, the team will inevitably encounter some hiccups. This section highlights common pitfalls observed in similar transitions, along with troubleshooting tips and solutions. Being aware of these will help you recognize and resolve issues faster, minimizing downtime or frustration.

## Pitfall: Skipping the OutSystems Refresh

**Symptom:** After deploying a schema change, parts of the OutSystems app break or show errors like "column not found" or new columns not appearing in aggregates. This often happens when the Integration Studio refresh step was missed or incomplete.

**Solution:** Always perform the **Integration Studio refresh and republish** immediately after a DB deployment 67 68 . If this was skipped and an issue arises, do the refresh as soon as possible. The error should resolve once OutSystems is back in sync. If you did refresh but still see issues, try republishing the consumer modules - sometimes a module doesn't realize the extension updated until you refresh dependencies and republish it 117 . Also check that the refresh was done on the correct environment's database (Dev vs Test vs Prod). A related gotcha is forgetting to update **all** extensions that need refresh. If multiple external entities were impacted, each extension module must be refreshed (we once refreshed one and not another, causing partial updates). We now use a mapping list of which OutSystems extension corresponds to which DB schemas, so we refresh all relevant ones after a deployment.

## Pitfall: Deployment Blocked by "Possible Data Loss"

**Symptom:** The DACPAC deployment fails with an error like _"Deployment blocked due to possible data loss"_. This typically occurs when dropping a column/table or reducing column length - the publish profile by default aborts to protect data 55 .

**Solution:** This is a _safety feature_, not a bug. It means the tool detected a destructive change. In Dev environment, you might choose to override it (we allow BlockOnPossibleDataLoss=False in Dev for flexibility), but in higher environments you should respect it. Ask: _Was this drop intentional and safe?_ If yes (e.g., dropping an unused table), you can proceed by temporarily disabling the block or using the Allow data loss publish option _with caution_. Usually, the better approach is to **plan the data change explicitly**: backup data, script out what's needed, then drop. For example, if removing a column in Prod, maybe first copy its data to a log table, then drop with BlockOnPossibleDataLoss off, knowing you have the backup if needed 55 . If the data loss was _not_ intended (you didn't realize your change would drop data), then stop and revise the change. Perhaps use a view or keep the column as deprecated instead of dropping immediately. In summary, treat this error as a big red flag - coordinate with leads on how to handle it 118 . Many times, the resolution is a phased deployment (don't drop now, do it in next release after other changes).

## Pitfall: Build Errors (SQL71501, SQL71006, etc.)

**Symptom:** Visual Studio build fails with cryptic error codes like SQL71501 (unresolved reference) or SQL71006 (batch separator issue), etc. This prevents you from even getting to deployment.

**Solution:** These are common during development and usually have straightforward fixes. **SQL71501 Unresolved Reference** means something in your project references an object it can't find 65 . Check if you spelled the object name exactly as in the project (names are case-sensitive in SSDT's comparison). If it's a cross-database reference (e.g., you refer to another DB), you need to add a database reference or a SQLCMD variable for that DB name. If it's a new object you added but didn't include in the project, add the file to the project. **SQL71006 Only one statement per batch** typically means you forgot a GO between two statements in a script 63 . Split them with a GO on its own line. Often this happens in PostDeploy if you have multiple unrelated statements. **Syntax errors** near unexpected tokens can be due to missing commas or parentheses in CREATE scripts 119 - check the lines around where it points, and ensure proper comma placement (no trailing comma on last column, etc.). **Refactor Log Conflicts (SQL71558)** happen if you tried to rename something manually that SSDT also is renaming, or if you have duplicate rename entries 120 . You may need to edit the refactorlog to remove duplicates or follow the correct rename procedure. For any build error: read the message carefully (SSDT errors can be verbose but they indicate the line and cause), fix what seems obvious, then rebuild. If stuck beyond ~15 minutes, reach out on #database-help Slack with the error text 121 . Often, a senior has seen the error and can point you to the fix quickly. We also have a "Troubleshooting Flowchart" in the appendix that lists these common errors and fixes 122 .

## Pitfall: Merge Conflicts in SSDT Project

**Symptom:** When merging branches, you get Git conflicts, often in the .sql files for tables or in the refactorlog. Example: two developers added different columns to the same table - the CREATE TABLE file now has conflicts. Or two renames causing refactorlog conflicts.

**Solution:** Merging in a database project requires careful attention. For table scripts, open both changes and manually merge the definitions: ensure all intended columns are present and in a sensible order (the order in the script doesn't affect the DB except for new columns end up in that order, but keep it logical). If both added columns, include both, perhaps one after the other. For refactorlog, each entry has a GUID; conflicts arise if two renames happened with the same entry ID or on the same object. Sometimes you can keep both entries if they're distinct renames. If they conflict on the same object, it means two branches tried to rename the same object differently - that needs functional resolution (pick one name or sequence the renames). In general, **coordinate schema changes on the same objects** to avoid heavy conflicts. Use small, frequent merges so conflicts are easier (if two devs work for weeks on the same table, conflicts pile up; instead integrate often). We have an advanced playbook on merge conflict resolution with examples (like merging two column adds vs true conflicts) [123](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A4/) . If in doubt, get both developers and perhaps a lead to review the conflict together - it's important not to accidentally drop a change. Also, after resolving, do a full build and maybe schema compare to ensure the merged result is correct. Git won't know SQL context, so our eyes must verify it.

## Pitfall: OutSystems Type Mismatch Warnings

**Symptom:** After refreshing an external entity, OutSystems shows a warning or error that the type of an attribute changed (e.g., from Text to Integer, or length changed) and it might be incompatible.

**Solution:** This can happen if a column's data type or length was altered in the DB. OutSystems might not automatically handle a type narrowing or certain type changes. For example, changing an INT to BIGINT in DB - OutSystems might flag that but it's usually fine (BIGINT can fit into an OutSystems Integer if within range). If it's safe (widening type), you can accept the change. If it's a breaking change (like changing a Text to Integer), any OutSystems logic using that field will break until adjusted. The approach is: _treat it as a breaking schema change_. Possibly revert the DB change and do a phased approach (add new column of new type, migrate data, update OS to use it, then drop old). If the change is minor (e.g., extended a varchar from 50 to 100), OutSystems might just need a refresh and republish; usually no issues. Always test the app after such changes - e.g., if a type changed, test any screen or action that uses that attribute to ensure no runtime casting issues. It's wise to search in the OutSystems code for usages of that attribute to see if anything assumes a certain format. In summary, heed OutSystems warnings: they are telling you to check if your app logic can handle the schema change. Often the resolution is updating the OutSystems code accordingly and redeploying the app modules.

## Pitfall: Long-Running Deployment or Timeout

**Symptom:** The pipeline runs the deployment and seems to hang, or eventually fails with _"Timeout expired"_ or _"Execution Timeout"_. This often means a script is taking too long (default DACPAC timeout is 30 or 60 seconds) or waiting on a lock.

**Solution:** First, determine if it's actually still running or stuck. On SQL Server, you can check active processes (e.g., sp_who2 or Activity Monitor) to see if a deployment session is blocked or just churning on a huge update. If a large data migration is running, it might simply need more time. For Dev/Test, we can increase the timeout limit in the publish profile for that deployment 124 . If it's blocked by another process (someone left a table open or an uncommitted transaction), you may need to kill that blocking session 125 . Octopus or SqlPackage allows a /p:CommandTimeout to be set; Ops can rerun with a higher timeout once identified. However, if something is unexpectedly slow (say an index creation on a 10 million row table, or a data update on huge table without indexing), it points to a need for optimizing the deployment plan. Perhaps break the operation into chunks, or do it offline. For the immediate issue: if partial changes have applied, you might have to resume or complete them manually (e.g., if half the data updated, finish the rest with a script). Always aim to get the database to a consistent state, even if deploy timed out - incomplete changes can leave it inconsistent. If uncertain, involve a DBA or senior: check what ran and what didn't. For prevention: for any script likely to run long, test the runtime on a backup or subset beforehand, and consider adding indexes or splitting work (our Month-1 optimization covers strategies for large data changes like batching with TOP(N) loops). Another tactic is enabling SQL's online index build for large indexes (if Enterprise edition) or doing heavy changes in off-peak hours with a maintenance window.

## Pitfall: Partial Deployment and Unknown State

**Symptom:** A deployment fails midway (for example, after applying schema changes but before or during post-deploy scripts). The database is left in a partial state - some changes applied, others not. This can be scary because neither the old nor new version is fully present.

**Solution:** This scenario is why we love transactions - often DACPAC will execute many schema changes in a single transaction, so if one fails, it rolls back all. But certain things (like some post-deploy scripts) might run outside the main transaction. If you suspect a partial application, first **assess what was done**. Check the target DB: were new columns created? Were any tables dropped or partially modified? If a post-deploy script failed, the schema might be updated but data migration incomplete 126 . The course of action: decide whether to **roll forward or roll back** 127 . Roll forward means fix the issue and complete the deployment. Roll back means undo what was done (which could mean restoring a backup if data was changed significantly). We have detailed _Deployment Recovery_ playbooks, but general guidance: If the schema changes themselves succeeded and only data script failed, it's often best to fix the script and run it manually to finish the job 128 . The system might be usable except that new features aren't fully there, so finishing it gets to consistency. If the failure happened in a schema change (like an ALTER that half applied - though typically it wouldn't half-apply, it'd either commit or not), you may need to manually reconcile (rare). Communicate with the team - if in Test or Prod, tell folks the system is in a "limited mode" until resolved. For example, "the new column is there but data isn't fully populated yet, so feature X might not work fully."

Then either complete the deployment or revert. Worst case (especially in Prod), if you're not confident in forward-fixing quickly, opt to restore the backup (downtime but guaranteed consistency) 129 . Then redeploy from scratch. We treat partial deployments as **SEV incidents** - seniors and leads jump on a call, use the "Deployment Recovery Decision Tree" 130 to decide next steps. There's no shame in rolling back to known good state if unsure. But in Dev/Test, we often practice by deliberately simulating partial failures to refine our recovery steps. Over time, these scenarios get rarer as we learn to anticipate which scripts might fail and handle them.

## Pitfall: Breaking Changes Not Coordinated (Consumer Impact)

**Symptom:** After a deployment, certain OutSystems modules or other applications start failing due to missing tables/columns or changed behavior. This indicates a breaking change to the schema was deployed without coordinating the necessary application changes.

**Solution:** Ideally, avoid this by using backward-compatible changes or feature flags. But if it happens: identify which change caused the break (error messages will help, e.g., an OutSystems error log "invalid object name" points to a missing table). If a column was removed that OutSystems still uses, the OutSystems module will error until updated. The fix might be to quickly apply a hotfix to the OutSystems application to remove references to that column (if possible) or, if not possible quickly, consider restoring the column (perhaps temporarily add it back via a quick DB script) and then plan a proper removal later. We had a rule to **never drop something still in use by any consumer** (OutSystems or otherwise) in the same release - always two-step it. If broken, it means that was violated, so treat as high priority to fix. If an external system was using a view or stored proc that changed, coordinate with that system's owner to update their side. Sometimes a view change can cause subtle bugs (e.g., column order or names changed). Monitoring and quick communication help - if an issue is reported, don't assume it's unrelated; verify if it could stem from the DB changes. In one instance, a report query in OutSystems used SELECT \* and a new column confused the logic. The solution: fix the query to specify columns (reinforcing our no SELECT \* rule). If truly stuck, one could rollback the database change (like re-add the dropped column) until the app is ready. The governance model tries to prevent this via the PR template and reviews (reviewer would ask "is anything using this column you drop?"), but human error can happen. When it does, **make it a learning incident**: add that scenario to the checklist ("Search OutSystems code for usage of X before dropping X").

## Pitfall: Static Data Mismatch or Missing Data

**Symptom:** After deploying, certain reference (static) data is missing or incorrect. E.g., an OutSystems static entity that was supposed to be moved to the external DB now shows empty or incomplete values. Or default lookup records weren't inserted as expected.

**Solution:** This likely points to an issue in our data seeding process. Check the PostDeploy script if it was supposed to insert data - did it run? Was it conditional incorrectly? Perhaps it didn't run because the project thought the data already existed (maybe you had it in dev but not in prod). If data is missing in Prod, you might need to manually insert critical reference data to quickly restore functionality (e.g., if a "Status" lookup table is empty and the app expects entries). Then fix the script for next time. To troubleshoot: run the same merge or insert script in SSMS to see if it throws an error (maybe a foreign key prevented insertion, etc.). Also confirm the target environment's expectation - OutSystems static entities previously auto-populated their data in the OS platform DB, but when moving to external, that doesn't happen automatically. We had to explicitly migrate those values. If one static entity was overlooked, it might simply be empty externally. The fix is to populate it (we can extract values from OutSystems static entity definition and load into the new table). For future, ensure **static-to-lookup migrations** are complete and tested. Another example: if an ID mapping changed (say static IDs were 1,2,3 but new table uses different keys), the app might misbehave. We should ideally preserve IDs for such static data to avoid having to update foreign references. In any case, static/reference data issues are usually easily fixable by adding the missing data and redeploying a corrected seed script. The key is to catch it early - QA should notice if a dropdown list is suddenly empty (indicating lookup data didn't load). That's why verifying seed data is part of our checklist.

## Getting Help

For any issue not immediately understood, remember the team resources: consult this guide's troubleshooting section (we tried to capture many scenarios), search the internal knowledge base or Microsoft's documentation (error messages often have MSDN pages or Q&A threads), and ask colleagues. We have the Slack channels (#database-help for non-urgent how-tos, #database-emergency for prod issues) - **use them** 131 . In the early weeks, it's expected to use these channels a lot; over time, fewer issues will arise as everyone becomes proficient 132 . Don't let a deployment issue fester alone - a quick post like "Hey, my build is giving SQL71006, anyone know why?" can get a response from someone who's seen it. We also established an office hours session daily in Week 1 where a senior or lead sits on a call ready to help any dev live - that may continue if helpful.

Finally, any production incident must be **immediately escalated** to the on-call and Dev Lead (follow our incident management protocol, paging if necessary). We have zero tolerance for unreported prod issues. It's better to over-communicate (even a false alarm) than to silently try fixes that could worsen things. The ops dashboards and monitoring will also help catch things (like slow queries or errors spiking after a release), but human reporting is crucial.

By anticipating these pitfalls and knowing the fixes, the team can handle issues with confidence. Most of these happened in other teams' transitions; we've learned from them. As we encounter new ones, we'll add to this list so our knowledge base grows. Troubleshooting is part of the learning curve - it will get easier each time.

# Upskilling Roadmap: Building SQL, SSDT, and CI/CD Proficiency

Transitioning OutSystems developers into database DevOps experts is a journey. We recognize that team members have varying starting points - some are junior developers new to SQL, others are senior but maybe haven't used SSDT or Octopus. This roadmap lays out how to go from novice to proficient, ensuring everyone acquires the needed skills in stages. It's as much about _learning_ as doing.

## Stage 1: Foundation (Week 0 to Week 1) - "Crawl"

**Goal:** Establish fundamental skills and confidence with the basic workflow. By end of Week 1, every developer should be able to make a simple schema change and deploy it through the pipeline with guidance.

- **Tools & Setup Mastery:** In Week 0, focus on getting comfortable with Visual Studio + SSDT. Do the official tutorial on creating and deploying a SQL project 133 if you haven't - it's a quick way to see the end-to-end in a sandbox. Practice connecting VS to our dev database and publishing (in a test DB). The idea is to remove any intimidation around "Visual Studio" or "SQL Server". Make sure you know how to do things like Schema Compare, where to find build output (for errors), how to add a new file, etc. If you've never used Git beyond OutSystems, this is a good time to learn branching and PR basics (we provided a Git/DevOps 101 refresher to some devs).
- **SQL and Schema Basics:** Ensure you understand core SQL DDL (Data Definition Language) statements: CREATE/ALTER TABLE, constraints, indexes, etc. If terms like "foreign key", "index cardinality", "execution plan" are new, take the week to read up. We recommended some Pluralsight courses and the team had a lunch & learn on "SQL for OutSystems devs" which covers normalization, common data types, etc. The measure of success is that you can read a table creation script and understand each part, and you could write a simple one from scratch. Also, learn or refresh T-SQL for writing any post-deployment scripts (basic UPDATE / INSERT , conditions).
- **First Change in Dev:** As soon as cutover happens, each dev should implement a **"Hello SSDT" change** in the real project (or a training branch). For example, add a practice column to a small table, go through PR, have it reviewed, deploy to Dev, do the refresh 134 . This hands-on experience in Week 1 is invaluable. Don't worry if it's a dummy change that might be reverted - the point is to exercise the muscle. Leads will try to ensure everyone gets a chance at a change early (even pairing up if needed). By Friday Week 1, every junior dev is expected to have done such a change independently (with review) 135 . This is a KPI we set to gauge onboarding progress.
- **Shadowing and Pairing:** In Stage 1, juniors should shadow seniors on one or two more complex changes to see how they approach it. E.g., watch a senior add a foreign key and a post-deploy script. Meanwhile, seniors pair with juniors on the juniors' first changes to guide them. This knowledge transfer accelerates learning. We also had an open forum for any "silly questions" on Slack - encouraging asking anything, like "what's a refactorlog?". Addressing these early prevents misunderstandings.
- **Knowledge Check:** End of Stage 1 (around end of Week 1 or early Week 2), we might do a quick quiz or review session. Example questions: _How do you make sure a column addition doesn't break existing data? Where do you add a default? How to handle a rename safely?_ If someone struggles to answer, leads will know they need extra help or resources in the next stage. But by this point, everyone should be familiar with the basic workflow steps: editing SSDT, committing, PR, build, deploy, refresh - and why each is important.

## Stage 2: Automation & Intermediate Skills (Weeks 2-4) - "Walk"

**Goal:** Eliminate reliance on hand-holding by getting comfortable with the automation and expanding to intermediate scenarios. By end of Week 4, devs should be independently using the CI/CD pipeline and handling most common schema changes (including ones requiring multi-step handling), while mid-levels/ seniors start driving process improvements.

- **CI/CD Pipeline Familiarity:** Now that everyone has seen the pipeline in action, Stage 2 is about truly understanding it. Developers (especially those interested in DevOps) should learn the mechanics of our Azure DevOps and Octopus setup. For instance, know where to find the build logs and deployment logs, how to read a DACPAC deployment report. We'll grant devs read access to Octopus logs so they can self-serve checking what happened in a deployment. This fosters autonomy - e.g., if your change didn't show up in the app, you can go see the Octopus log to confirm it deployed and the refresh happened. Some may even try deploying to a personal local DB using SqlPackage to see the process in detail. The more you know the pipeline, the more you can leverage it (for example, learning that you can do a "Schema Compare" from DACPAC to DB as a report could be useful in reviews).
- **Intermediate Schema Operations:** Practice and master slightly more complex changes. For example: **Adding a NOT NULL column** with considerations (Stage 1 touched on it, now do it with real data - maybe add a NOT NULL with default and see how SSDT handles it). **Updating data in deployments:** e.g., a story requires backfilling some values - learn to write a proper post-deploy script with a WHERE clause, test it on dev data, and ensure it's idempotent. **Foreign key additions:** do one and see how to handle existing orphan data (maybe you discover some rows violate the FK - you'd need to clean or script a default parent for them). These scenarios will appear in real work, so Stage 2 is encountering them with some safety nets. We'll have "playbooks" written for these (like how to do a phased not null) and possibly have seniors assign practice tasks ("hey try adding a NOT NULL to this table in a branch, see if it deploys or errors, and figure out why"). By week 4, juniors should handle ~80% of such tasks on their own (with code review still) 91 , and mid-levels comfortably even more.
- **Handling Merge Conflicts & Multi-Dev Collaboration:** By Stage 2, multiple devs will be touching the DB project concurrently. Learning to manage branches and avoid/resolve conflicts is key. We might simulate a conflict: two people edit the same table on purpose and then resolve it together, to demystify it. Also, using features like **Git pull with rebase** frequently to keep your branch updated can help. Leads will coach on strategies: e.g., if working on adjacent features, communicate ("I'm going to modify Orders table, anyone else? If yes, let's sequence or one of us waits.").
- **Reviewing and Owning Quality:** As devs gain confidence, we encourage them to start reviewing each other's PRs (at least for juniors to review peers' small changes). This not only offloads leads but also teaches the reviewer. A junior reviewing another's PR might catch something simple and feel more ownership of quality. Of course, leads will still do the official approval, but this spreads knowledge. By end of month 1, some juniors can handle reviewing trivial changes (like adding a column with no tricky aspects) - this was a target milestone 93 . Mid-level devs by this time should be actively reviewing junior PRs daily 136 .
- **Expanding OutSystems Knowledge:** Since our work ties to OutSystems, Stage 2 is also ensuring devs learn any OutSystems-side tweaks. For example, how to create a new external entity in Integration Studio, how to update OutSystems logic if a schema changes (maybe using the "Find References" in Service Studio to update all uses of a renamed column). We might have a training on advanced Integration Studio usage (like what to do if the extension module says it has pending changes, or how to script Integration Studio with automation). The better the devs understand OutSystems integration, the less reliant they are on a small subset to do those tasks.
- **Begin Process Improvement Participation:** With basics in hand, team members can contribute ideas to improve the workflow. Stage 2 is where we expect suggestions: maybe a dev says "I found a tool to analyze our SSDT code for best practices, can we integrate it?" or "the PR template could have a checkbox for 'did you refresh OutSystems in dev after deploying?'". Mid-levels are especially encouraged to think this way 137 . We might run a mini-retro mid-sprint to gather these. This involvement means by the time we reach month's end, the process is truly _team-owned_, not just leadimposed.

## Stage 3: Advanced Practices (Month 2 and beyond) - "Run"

**Goal:** Develop expertise in complex scenarios and optimization. Senior devs/Leads will introduce advanced topics (performance tuning, zero-downtime deployments, etc.), and every team member will deepen their specialization (some might focus on performance, some on pipeline engineering, etc.). By a few months in, the team should handle even major schema overhauls confidently and continuously improve the process.

- **Complex Schema Changes & Refactoring:** In Stage 3, we tackle scenarios like **splitting a table** (vertical or horizontal partitioning) or **merging tables**. These require careful planning: using views or temporary redundancy to migrate gradually 138 . We have advanced playbooks for such refactors (e.g., how to split a table with minimal downtime by introducing a view that unions them for legacy reads). Seniors might lead one such refactor as a "teaching case," with a mid-level developer shadowing to learn the pattern. Another advanced scenario is **changing a column's data type or size in a huge table** - here we'd teach techniques like adding a new column, backfilling in batches, then switching usage (the "parallel table" or "expand/contract" pattern) [139](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A4/) 140 . Over time, devs become comfortable that even the scary changes are doable with the right approach.
- **Performance and Optimization:** We dive deeper into performance tuning of database operations. Devs should learn to interpret an execution plan for a slow query, know how to add a covering index, and be aware of things like index maintenance and statistics. Perhaps a senior or DBA will hold a session on **index design and SARGability** (how query patterns affect index usage) 141 . We also consider writing some automated performance tests: e.g., using a known heavy query with and without an index to see improvement, to quantify and justify performance changes. By this stage, developers should proactively think about performance when designing changes (like "this new query might need an index, I'll add it now").
- **CI/CD Mastery and Pipelines as Code:** Some experienced devs might start contributing to improving the pipeline itself. For instance, writing custom scripts (maybe a PowerShell in the pipeline) to automatically generate documentation or to integrate with OutSystems API for an automated refresh checklist. We might move our Azure DevOps pipeline to YAML (if not already) so it's versioned as code, and devs can propose changes via PR (with Ops oversight). We also consider adding automated _database tests_ - e.g., a Post-Deployment test that runs a query to ensure a critical view still returns results, etc. These can be integrated into the pipeline as additional quality gates. Empowering devs to tune the pipeline fosters DevOps culture - it's not just Ops' responsibility, devs can enhance it too.
- **Cross-Team Collaboration:** As our team becomes expert, we may coordinate with other teams or enterprise DBAs. Stage 3 might involve publishing some of our newly minted best practices for wider use, or collaborating if another app's team needs to interface with our DB. Developers should be able to have technical discussions with DBAs on equal footing, talking about things like locking, indexing strategies, etc. The Dev Lead may engage the DBA team to do a performance review after a month of changes, to see if any queries regressed. This is a chance for devs to learn even more from seasoned DBAs. Also, we should align with any enterprise standards (maybe the company has rules about indexing or about using certain features) - by Stage 3 we ensure we're compliant or have exceptions documented.
- **Continuous Learning and Certification:** We encourage developers to pursue deeper learning.

Microsoft has certifications for database development (like "DP-300: Azure Database Admin" or older MCSE for SQL) - not required, but some might find it motivating to study for those. We also recommend books like _"SQL Server Execution Plans"_ or _"Database DevOps"_. The company might provide subscriptions to Pluralsight, etc. We suggest a learning goal of one advanced topic per developer per week or sprint - e.g., one dev picks "learn about SQL partitioning", another "learn about writing a custom DACPAC deployment contributor" - and then share a summary with the team. By continuously learning, we ensure our knowledge stays fresh and we can tackle upcoming challenges (like maybe moving to Azure SQL or implementing Always Encrypted, etc., in the future).

**Milestones & Checkpoints:**

- By end of **Week 1**, as noted, juniors independently added a basic change 48 . Everyone comfortable with core process.
- By end of **Month 1**, team velocity back to normal; junior devs handle ~80% common tasks with little help

91 ; mid-levels can run the daily train themselves 100 ; seniors feel the team is ready for more complex tasks (maybe already did one moderate refactor by then as a test). We expect zero critical incidents in Prod in this first month (maybe a few minor hiccups that were fixed). If achieved, that's a huge win - we leveled up without hurting deliverables.

- By **Month 3**, most devs are effectively mid-level in DB skills. They can design a schema change considering all angles (data, app, performance). They can resolve merge conflicts routinely, and no one's intimidated by the process. Seniors by now might be focused on optimizing process (like writing more playbooks, or building internal tools e.g., a script to auto-check for missing indexes in PRs). We also expect by 3 months in, some devs can help onboard any _new hires_ into this process because they lived through it. That's a sign of success - the knowledge is spread enough that newcomers can learn from team, not just docs.
- Ongoing beyond: the team should strive to become not just consumers of DevOps process but innovators.Perhaps we become an example for other OutSystems teams integrating with databases. Maybe we'll present our journey in a company tech talk or write a case study. This reinforces the achievement and also cements knowledge (teaching others is a great way to solidify what we learned).

This roadmap isn't rigid; people will progress at different speeds. But it provides a structure to ensure no one is left behind and everyone has opportunities to grow. **The enterprise has invested in this DevOps shift not just to improve deployments, but to improve _us_ as engineers.** Embrace the learning - becoming fluent in database DevOps will make you a stronger developer overall, able to handle full-stack challenges that span database to application. We'll support each other through this journey.

# Decision Frameworks: Direct Table Bindings vs. Views (Contracts)

When integrating OutSystems (or any app) with the external database, a key architectural decision is whether to bind the app directly to database tables or to introduce an abstraction layer (like database **views** or stored procedures, which we'll collectively call "contracts"). The direct approach is simpler, but the contract approach can provide stability and security at the cost of complexity. We have established guidelines to help choose the right approach for a given scenario 106 .

## Default Approach: Direct Entity Bindings

Our default is to have OutSystems modules directly use tables via Integration Studio (external entities). This yields better performance (no extra layer) and simplicity - OutSystems can do CRUD directly on the tables as if they were its own. For many cases, this is fine, especially if _the schema is stable and under our full control_. For example, a table like Country (list of countries) is very stable - we can safely have OutSystems reference it directly. Direct binding is easiest to maintain when changes are straightforward or infrequent; the OutSystems integration will catch any changes upon refresh.

**Benefits of Direct Binding:**

- Simpler development - one less layer to manage. OutSystems aggregates and queries run directly on thetable, utilizing indexes etc., no need to maintain parallel logic.
- Less overhead - no need to sync view definitions or worry about view-based limitations (OutSystems mighttreat a view as read-only if not carefully crafted to be updatable).
- Clear source of truth - OutSystems shows the real schema, so developers see exactly what's in the DB,which can be easier for understanding relationships.
- Sufficient for low-churn, low fan-out tables - if only one app uses the table, and it doesn't change often,direct is simplest.

We usually start with direct binding unless there are compelling reasons to add a layer.

## When to Introduce a "Contract" (View/Stored Proc)

We consider adding a database view or stored procedure as an interface between the DB and OutSystems when one or more of the following factors apply, _especially if multiple factors are present (2 or more is our threshold)_ 106 :

- **High Fan-Out Usage:** The table is consumed by many places - perhaps multiple OutSystems modules or even other systems. Example: a central Customers table used by 5 applications. If we directly bind all 5 to the table, a schema change (like splitting the table or renaming a column) would break all 5 at once. A view can act as a stable interface - we could change underlying table structure but keep the view's columns the same (or at least deprecate them gradually) so consumers are none the wiser. If fan-out is high, a contract reduces the ripple effect of changes.
- **High Schema Churn:** If the table's schema is expected to evolve rapidly (perhaps during initial development or due to changing requirements), a view can provide a buffer. For example, during an ongoing refactor, we might maintain a view with the "old" schema for OutSystems to use, while we rework the base tables. This way OutSystems doesn't need to refresh and change every sprint. Once things stabilize, we could even retire the view if desired. Conversely, if a table is very stable, no need for a view - it would be over-engineering.
- **Security/Compliance Shape:** Sometimes not all columns in a table should be exposed to the app (maybe sensitive data). A view can select only allowed columns or apply a filter (e.g., hide softdeleted records). OutSystems roles can also control access, but if data segregation is needed, a view could enforce it at the source. If multiple roles should see different data shapes, we might even have multiple views. If compliance requires an audit whenever certain data is accessed, a stored proc might be used instead to log that access. Generally, if security requirements can't be met with OutSystems' platform alone, a contract layer might help.
- **Performance Shape:** Sometimes the way the app needs the data is complex (like joining multiple tables, or aggregating). We can encapsulate a complex query in a view or stored proc, so the app just selects from that view for simplicity and to ensure consistent logic. Also, if you need to ensure certain indexes or query hints, a view (indexed view perhaps) or proc might allow that. Example: if an OutSystems screen needs data from 3 tables joined and filtered, and that's a hot path, a view could pre-join and maybe index the combined result. However, caution: OutSystems can often handle joins itself in Aggregates efficiently; we typically only do this if OutSystems is limited (like can't easily express a query, or if we want to reduce data transfer by pre-aggregating).
- **Ownership Boundary:** If the database is shared across teams or products, a contract defines clear boundaries. For instance, our team owns tables A and B, another team owns C, but we both share some via views. If that other team changes table C, if we only ever accessed it through a view, they can coordinate changes by updating the view's definition if needed (assuming they own the view too). In an enterprise, sometimes DBAs mandate that applications only touch views/stored procs and not tables directly, to allow them flexibility in backend changes. In our scenario, we largely own the

DB and app together, but if that changes or if a table becomes a shared resource, using a contract may be prudent.

- **Evolving Keys or Structure:** If a table's primary key might change (say from an INT to GUID, or from single column to composite) or if we anticipate normalization changes (splitting one table into two linked by new keys), a view can mask that from the application. For example, initially the app sees a view vOrder that flattens perhaps Order and Customer into one, but later we change schema and adjust the view. Or if we move a table to another database, a view (maybe a synonym or crossdb view) can keep the interface. Essentially, if you foresee "we might change how data is stored or keyed but want the app to not care," a contract is useful.

**Concrete Example:** Suppose we have a UserProfile table with columns \[UserId, Name, Email, Department, Manager, etc.\] and multiple apps use it. If we plan to normalize Department and Manager into separate tables for flexibility, doing so directly means all apps need to adjust their queries and references (and OutSystems would need to re-import relationships). Instead, we could create a view vUserProfile that still presents \[UserId, Name, Email, DepartmentName, ManagerName\] as one flat structure. OutSystems uses vUserProfile as if nothing changed. Under the hood, we split the table, but update vUserProfile to join User, Department, Manager tables. No OutSystems refresh needed (assuming the view's schema remains the same columns). Over time, we might update the OutSystems modules to use a new pattern or direct new tables if needed, but the view gave us flexibility in timing.

**Costs of Using Contracts:**

- Additional complexity to maintain (views need to be updated when underlying schema changes anyway;we are pushing the problem one layer but not eliminating it, though it can centralize fixes).
- Performance overhead if not careful (views can sometimes prevent optimizer from using indexes onunderlying tables effectively, especially if layered views on views).
- OutSystems limitations: OutSystems external entities from views are read-only by default (since a view maynot map cleanly to a table). If the app needs to write, you either instead use an _Instead-of Trigger_ on the view (complicated) or avoid using a view for writes. We generally use views for read scenarios. Alternatively, use stored procedures for writes if needed (OutSystems can call stored procs but it's not as seamless as using entities).
- Potential false sense of security: a poorly planned view can still break apps if changed; it's not magic. It justmeans we have one place to adapt the interface. But if an underlying change can't be masked (e.g., you drop a column that the view used to provide, the view has to drop it too - then OutSystems breaks anyway unless you keep dummy data), so sometimes you still need a multi-release strategy (like keep column in view returning null or fixed value for a while to satisfy consumers until they remove usage).

**Guideline Summary:** We _prefer direct bindings_ unless there is a strong justification for a contract. If **2 or more** of the factors (fan-out, churn, security, perf, cross-team, key changes) are true for a particular part of the schema, then lean towards introducing a stable interface (view/proc) 106 .

For example: Our Orders table is used by both the OutSystems app and a reporting service (two different consumers) AND we expect to heavily refactor it (high churn) - that's two factors, so a view might be wise. Conversely, our AuditLog table is internal, only our app uses it, and it's append-only (low churn, low fanout) - direct binding is fine.

**Using Views/Contracts Wisely:**

If we do use a view, we should document it clearly as an interface. Perhaps treat it like an API: version it if needed (like vUserProfile_v1 , vUserProfile_v2 if we ever drastically change it, keeping old for backward compat temporarily). Also, tests should be in place to ensure the view returns expected results, since logic may reside in it. If using stored procedures (e.g., to insert into multiple tables in one call), ensure OutSystems can call them and handle results.

One particular pattern: **Expanding/Contracting refactor** - use a view during transition. Example: We want to split a table into two. Step 1, create new table, create view that unions or joins them to look like old one. Point OutSystems to view (if possible). Or if OutSystems was already on table, perhaps do nothing at first. Step 2, gradually migrate data, update view as needed. Step 3, once stable, optionally switch OutSystems to use new structure or keep them on view indefinitely. This is advanced but we mention it because contract usage often comes with phased refactoring patterns.

**Static vs. Direct:** A related decision (though slightly different) is static data usage - which we address separately in the next section. But note, one could consider using a view to present static data uniformly, though usually simpler is moving static data into a lookup table and treat it like any other table (the next section covers that pattern).

In conclusion, **use contracts (views) selectively**: if they solve a genuine problem or risk. Do not create a view for every table by default - that adds maintenance overhead for little gain. But when you foresee a volatile or widely used piece of schema, design a contract for it from the start to isolate consumers from changes. In our team, we actually identified a couple of areas in planning: for example, the external database is also accessed by a Data Warehouse ETL. Instead of pointing the ETL directly at our 20+ tables, we gave them a set of views to use. This way, if we change our schema for the app, we can adjust the ETL view definitions to still provide continuity (like combining tables, renaming columns, etc.). The same thinking applies with OutSystems: if future changes could break a lot of modules, consider shielding them with a view. Use the checklist above to decide case by case.

We will maintain a **Decision Log** (in our documentation repo) where any deviation from direct binding is recorded, including rationale and any special considerations. This transparency ensures everyone knows where we've put abstraction layers and why.

# Pattern: Transitioning Static Entities to External Database Lookups

OutSystems "Static Entities" are a convenient way to handle small sets of reference data (essentially enumerations) within the platform. However, since we are moving to an external database model, static entities that were stored in the OutSystems metamodel need to be reconsidered. The platform does **not** support linking static entities to external data directly [142](https://www.outsystems.com/forums/discussion/85655/using-static-entities-while-using-external-database/#:~:text=Hello%20Sourav%2C) . Our strategy is to transition these to normal lookup tables in the external SQL database, so they can be managed with SSDT and included in deployments.

**Why Migrate Static Entities?**

- **Single Source of Truth:** We want all persistent data in the external DB under version control. If some reference data stays in OutSystems static entities, we'd have two places to manage data definitions - one in code (SSDT) and one in OutSystems. This can lead to inconsistency and manual steps to sync them.
- **Data Volume and Operations:** If a static entity has more than trivial number of entries or might be updated by end-users (some static lists might get new entries over time), OutSystems static entities are not ideal (they require a code deployment to change, as they are part of the application). Externalizing them allows runtime data changes if needed and easier maintenance.
- **Cross-System Sharing:** If the static data is needed outside OutSystems (say by integration or reports), having it in the SQL DB accessible to all is preferable.
- **Limitations:** As noted, OutSystems cannot treat an external table as a "static entity" with designtime constants [142](https://www.outsystems.com/forums/discussion/85655/using-static-entities-while-using-external-database/#:~:text=Hello%20Sourav%2C) . You can't join a static and external entity directly either. Converting to an external lookup table resolves such limitations by making everything a normal entity.

## Transition Pattern: Static Entity -> External Lookup Table

For each static entity we identify to migrate, follow this playbook (we'll use an example of a static entity called Status with values like "New", "InProgress", "Done"):

- **Create a New Lookup Table in SSDT:** In the database project, create a table, e.g., StatusLookup with appropriate columns. Typically, static entities in OutSystems have an _Id_ (often an integer or text code), _Label_ (the name), and maybe additional attributes (like a sort order or an external code). We will mimic that structure. For instance: StatusLookup table with StatusId (primary key, maybe int), Name (nvarchar), and any other relevant fields. Decide on the primary key type - OutSystems static entities often use integer identifiers (starting at 0 or 1). We might preserve the same values to ease transition. If OutSystems static had a specific Id (they often start at 1 and increment), we can use the same.
- **Seed the Table Data:** Add the initial data as part of deployment. In SSDT, we do this via a postdeployment script with INSERT or MERGE statements for each static entry 143 . For example, insert (1, 'New'), (2, 'InProgress'), (3, 'Done') into StatusLookup 144 . Make sure these inserts are idempotent - e.g., use IF NOT EXISTS (SELECT 1 FROM StatusLookup WHERE StatusId=1) around each insert or a single MERGE that upserts 54 . This way if it runs twice or the table already has data (say we deployed to Dev once), it won't duplicate rows. Also, if static values might ever change (label text updates), consider the MERGE logic to update the name if the Id exists. For pure static that rarely changes, it's less of an issue, but good practice.
- **Deploy and Populate in Dev:** Deploy this change to Dev environment. Now StatusLookup table exists in Dev DB with the static entries (the post-deploy script inserted them). Verify the data is correct. This table is now under source control.
- **Import External Entity in OutSystems:** Using Integration Studio, connect to the Dev database and import the new StatusLookup table as an external entity (probably into the same extension module that houses other external entities, or create a new extension if appropriate). This external entity will appear just like any table entity in Service Studio, except it's read from external. Now, here's a crucial step: **update the OutSystems application logic to use this external entity instead of the old static entity**. This might involve:
- Changing data types of attributes: If some OutSystems entities had attributes of type Static Status, you might convert them to simple (likely Integer) that reference the external status table's Id. Or, perhaps better, you replace those with references to the external entity (OutSystems can't directly enforce foreign keys to external, but you can manage logically). In UI, any dropdowns that used static entity values need to query the external table instead.
- Replace any usage of the static entity list (like in expressions or default values) with equivalent references to the external data. For example, where code used Status.New static reference, now you might have a Site Property or an static variable that fetches the "New" Id from StatusLookup . Since static entities allowed design-time constant references (like compile-time constants), losing that is a consideration. One strategy: keep an OutSystems Site Property or Static Entity just to hold the default values of the IDs (like a mapping). But ideally, it can all be dynamic: you query StatusLookup for name "New" if needed.
- There's also an OutSystems feature: you can **convert a static entity to a normal entity** (which would make it part of OutSystems DB) [145](https://success.outsystems.com/documentation/outsystems_developer_cloud/building_apps/data_management/data_modeling/static_entities/#:~:text=To%20convert%20a%20Static%20Entity,then%20select%20Convert%20to%20Entity) , but that's not what we want - we want it external. So we likely create new logic around the external table usage. Possibly we mark the old static as deprecated (prefix name with "OLD_" or something) to ensure no one uses it.
- This step may be involved depending on how widely the static was used. We might do it gradually: perhaps first just start referencing the external data in new logic while old logic still references static, then eventually replace everywhere and remove static. But ideally, if not too widespread, do a full find-and-replace in the module code.
- **Coordinate the Switch:** During cutover to this new approach, there's a chicken-and-egg problem: OutSystems static data existed in OutSystems internal DB, now external table holds it too. We need to ensure consistency. Ideally, do a one-time migration of any delta: if static data had been modified (though static entities usually only change via new deployments of app, not at runtime, so they should be the same as design). The initial insertion script in SSDT should match exactly the static entity values that were in OutSystems. If we trust the design values, then no live migration needed, just ensure nobody added static items via logic (OutSystems doesn't allow insert into static at runtime). So the data should match. The moment we switch the app to use external, it should see the same values.
- We should also decide whether to keep the same IDs. If OutSystems static "New" was ID 0 or 1, we used that in external. This way any code that stored those IDs (like in entity records) still line up. If we changed keys (like used different numbering), any existing references (say an OutSystems entity had a static reference saved) would break. So preserving IDs is recommended. We might explicitly set the identity insert on in the seed script to use specific IDs 143 .
- Example: OutSystems had Status static with values (Id:1 "New", 2 "InProgress", 3 "Done"). Our StatusLookup uses the same IDs and text. Then if an OutSystems entity Order had an attribute Status (static) which stored "2" for InProgress, if we now change that attribute to an integer referencing external, the value 2 still means InProgress, and if we join to StatusLookup , it finds "InProgress". So things line up. We might have to adjust the data model slightly, but conceptually, it can align.
- **Deploy to Test/Prod:** Once dev is verified, propagate the change to higher environments via pipelines: deploy the new table and seed data to Test and Prod using DACPAC. Important: do this **before** the OutSystems code that relies on it goes live, or simultaneously. Ideally sequence such that: the external table and data exist in Prod (no one using it yet, that's fine), then deploy the new OutSystems version that starts using it. This avoids any break where OutSystems is looking for data that isn't there. Since static data is often loaded at platform startup, having it ready beforehand is wise. The OutSystems deployment (with static->external changes) needs to happen after the DB deployment.
- Ensure that the OutSystems Prod environment's extension is refreshed to include the new table and published _before_ the main modules use it. Alternatively, orchestrate in one maintenance window: Deploy DB change to Prod, refresh/publish extension, deploy OutSystems app update - all in one sequence with minimal gap. This might require some downtime or careful toggling if the old code and new code can't run concurrently. But if done quickly, minimal user impact (maybe just a scheduled maintenance downtime to implement this change).
- **Retire the Old Static Entity:** Once the new mechanism is working end-to-end and thoroughly tested, the old static entity in OutSystems can be removed or hidden to avoid confusion. OutSystems doesn't allow deletion of static entity if something references it; by now hopefully nothing does. We can convert it to a normal entity if needed to get data out (but probably not needed) or simply leave it empty and not used. Perhaps mark it as deprecated in documentation to ensure no future dev accidentally uses it. Over time, remove it from the module in a cleanup release.
- **Validation:** After cutover, verify that all places that should use the external lookup are indeed using it. This includes UI dropdowns showing values (they now should show data from external table, confirm they do). Also check any Create/Update logic - if previously the static ID was auto-managed by OutSystems, now if we manually assign an ID or fetch it, ensure nothing is missed. Usually, we still use OutSystems to handle it (like if an Order's status default is "New", previously you set default to static New. Now you might set default to an expression that fetches the Id of "New" from

StatusLookup , or simply default 1 if that's known "New"). We might have introduced a small risk of hardcoding IDs in logic if not careful, so better to have a small function or lookup to get ID by name if needed.

**Alternative Approaches:**

An alternative to fully migrating static to external is a hybrid: keep the static entity but regularly sync it with an external table via a timer (as one forum answer suggested) [146](https://www.outsystems.com/forums/discussion/85655/using-static-entities-while-using-external-database/#:~:text=You%20cannot%20add%20an%20entity,it%20as%20a%20static%20entity) . However, that means two sources and adds complexity, so we avoid that unless necessary. Another approach is OutSystems's "Convert to Entity" which makes it a normal OutSystems entity (in OutSystems DB) [147](https://success.outsystems.com/documentation/outsystems_developer_cloud/building_apps/data_management/data_modeling/static_entities/#:~:text=Static%20Entities%20,then%20select%20Convert%20to%20Entity) and then use OutSystems integration to copy it to external, but again double handling. We prefer single source in external.

**Example Outcome:** After doing this for, say, 5 static entities, we end up with 5 new lookup tables in the external DB, each seeded with values. OutSystems now treats them like any other external entity: you can aggregate them, join to them, etc. We include those in our SSDT project so any changes to reference data (like adding a new status) can be done by adding an insert to the post-deploy script in source control, and deploying it (or in some cases, we might allow an app screen to add a row - then that data lives in DB and we'd pull it back to Dev environment via Import or manually adding to script to source control to not lose it). This is a process detail: if reference data can be user-edited, we need to handle capturing those changes. Usually static data is not user-edited, though, it's a dev task.

**Idempotency & Merging Data:** For reference, our post-deploy uses MERGE for static tables to make sure new values are inserted, changed labels are updated, and removed ones maybe are left or flagged (we usually don't delete static values without careful consideration). This way if one dev adds a new static option and deploys, it appears; if another removes one, we might just not include it and then either manually decide to remove from DB or keep it. It's safer not to auto-delete reference data that might be in use.

**Conclusion:** After migrating, we have reduced friction - now adding a new static option is just a schema change like any other (update the seed script and OutSystems will pick it up on refresh), rather than an OutSystems model change that is outside the DB deployment pipeline. It also means our DB and app stay in sync through the pipeline rather than someone forgetting to update static data in one environment. It's a bit of effort to migrate but mostly one-time per static entity.

This pattern ensures that come cutover Monday, developers aren't confused why some data lives in OutSystems vs external. Everything (schema and reference data) is managed via the SSDT project and deployed consistently.

Having covered all requested topics - from the initial onboarding to advanced practices - this whitepaper provides a modular, role-tailored guide. It can be referenced by a new junior hire (to follow the roadmap and checklists), by a QA lead (to see their role in the new workflow), or by a dev lead planning a safe deployment. Following these guidelines, the OutSystems team will not only transition smoothly to SSDT/ Azure DevOps/Octopus, but will also cultivate a stronger engineering culture around database changes. By the first Monday of cutover, we'll be ready with tools in hand and safety nets deployed; by the end of the first week, the team will be confidently riding the daily release train; and in the months to follow, we'll continuously improve and perhaps even set an example for other teams.

Good luck with the cutover - with preparation, teamwork, and the practices in this guide, we're on track for a **successful transition to state-based database DevOps** .

**Checklist Recap (for Monday Readiness):**

- \[ \] SSDT project set up and building without errors (Dev env).
- \[ \] All developers have run through a test deployment and refresh in Dev.
- \[ \] PR process in place - template, reviewers assigned, branch policies enabled.
- \[ \] CI/CD pipelines configured (Azure DevOps build, Octopus deployment) and tested on a sample change.
- \[ \] Integration Studio connections ready; all external entities imported and up-to-date.
- \[ \] Communication channels (Slack #database-help, etc.) active; on-call schedule for any issues defined.
- \[ \] Backups taken and rollback plan documented for cutover weekend.- \[ \] Go/No-go review done - team leads confirm all above items OK.

By ticking all boxes, you minimize surprises. Here's to a smooth Week-0 to Week-1 and beyond, as we elevate our database development to the next level.

**Sources:** The practices and recommendations above are drawn from a combination of our internal transition guides and industry best practices. Key reference materials include the _External Database Cutover Survival Guide for OutSystems Teams_ [148](file://file_0000000021fc620aa31dd727c8deded7#:~:text=Pre,Why/) 149 , the comprehensive _Onboarding & Operational Guide: OutSystems to SQL Server DevOps Transition_ 10 150 , the developer-focused _OutSystems Developer's Playbook for SQL Server DevOps_ 151 152 , and our internal notes/strategy documents [153](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A9/) 110 , among others. These provided insights on everything from daily workflows and anti-patterns to role definitions and advanced deployment patterns, which have been consolidated here for easy reference.

Onboarding & Operational Guide_ OutSystems to SQL Server DevOps Transition.docx

1

2

3

6

9

10

11

12

13

14

15

16

17

19

20

21

29

34

36

37

38

39

40

41

42

43

46

48

52

60

78

79

80

81

83

84

85

86

89

90

91

[9](file://file_00000000173461fd85268df92fdd32e9#:~:text=,112/)

[2](file://file_00000000173461fd85268df92fdd32e9#:~:text=,112/)

93

95

96

97

98

99

100

101

102

103

104

105

107

135

136

137

150

[file://file_00000000173461fd85268df92fdd32e9](file://file_00000000173461fd85268df92fdd32e9/)

| 4 5 31 32 33 44 45 49 50 51 56 69<br><br>Playbook for SQL Server DevOps.docx [file://file_000000002528622faca48f46d66deb72](file://file_000000002528622faca48f46d66deb72/) | 71  | 75  | 76  | 117 133 143 144 151 152 The OutSystems Developer's |
| --- | --- | --- | --- | --- |
| 7 8 18 22 23 24 25 26 28 30 35 54 | 55  | 57  | 62  | 63 64 65 66 67 68 70 118 119 120 121 122 124 125 |

126 127 128 129 130 131 132 [148](file://file_0000000021fc620aa31dd727c8deded7#:~:text=Pre,Why/) 149 External Database Cutover Survival Guide for OutSystems Teams.docx

[file://file_0000000021fc620aa31dd727c8deded7](file://file_0000000021fc620aa31dd727c8deded7/)

27 47 53 [58](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A4/) 59 61 72 73 74 [77](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=9,OPS/) 82 87 88 94 106 108 109 110 111 112 113 114 115 116 [123](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A4/) 134 138 [139](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A4/) 140

- [153](file://file_0000000050b861fb80c0a4f2babae4ef#:~:text=,S%3A9/) notes.md

[file://file_0000000050b861fb80c0a4f2babae4ef](file://file_0000000050b861fb80c0a4f2babae4ef/)

- [146](https://www.outsystems.com/forums/discussion/85655/using-static-entities-while-using-external-database/#:~:text=You%20cannot%20add%20an%20entity,it%20as%20a%20static%20entity) Using static entities while using external database | OutSystems <https://www.outsystems.com/forums/discussion/85655/using-static-entities-while-using-external-database/>

[145](https://success.outsystems.com/documentation/outsystems_developer_cloud/building_apps/data_management/data_modeling/static_entities/#:~:text=To%20convert%20a%20Static%20Entity,then%20select%20Convert%20to%20Entity) [147](https://success.outsystems.com/documentation/outsystems_developer_cloud/building_apps/data_management/data_modeling/static_entities/#:~:text=Static%20Entities%20,then%20select%20Convert%20to%20Entity) Static Entities - ODC Documentation [https://success.outsystems.com/documentation/outsystems_developer_cloud/building_apps/data_management/data_modeling/ static_entities/](https://success.outsystems.com/documentation/outsystems_developer_cloud/building_apps/data_management/data_modeling/static_entities/)
