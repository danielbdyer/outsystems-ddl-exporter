# Upskilling OutSystems 11 Developers for SQL Server 2019 - SSDT, Schema Versioning & Azure DevOps CI/CD

## Background: OutSystems 11 and SQL Server 2019

OutSystems 11 is a low-code application platform that typically runs on a SQL database (SQL Server or Oracle) to store application data and metadata. In our case, the OutSystems environment is backed by SQL Server 2019. OutSystems developers usually work at a higher abstraction, designing entities and logic in Service Studio while the platform handles the underlying SQL schema. However, OutSystems also allows integration with external SQL databases when needed. For example, one can configure a **Database Connection** in the OutSystems Service Center (administration) and then use **Integration Studio** to create an extension that maps external tables or views into OutSystems as if they were entities[\[1\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=A%20database%20connection%20can%20be,Connections%E2%80%9D%20is%20available%20in%20Administration)[\[2\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Using%20OutSystems%20Integration%20Studio%2C%20an,to%20access%20the%20external%20tables%2Fviews). This means OutSystems applications can directly read/write to tables in an external SQL Server database - useful for leveraging existing corporate data or performing complex operations outside the OutSystems data model.

**Why the need for new database skills?** In a scenario where OutSystems-first developers must work with an external SQL Server 2019 database (or advanced aspects of the platform DB), they face tasks that OutSystems would normally abstract away: designing relational schema changes, writing SQL scripts, and deploying those changes across environments (Dev/Test/Prod) in sync with application releases. Most of our developers have basic SQL knowledge (e.g. writing queries) but are **new to SQL Server Data Tools (SSDT)** and formal **schema versioning** practices. Without proper tooling, database changes might be applied manually, which is error-prone and hard to track. Indeed, it's common in some teams to automate application deployments but handle database updates manually "because it's tricky," which leads to inconsistent environments[\[3\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=I%20have%20seen%20projects%20that,deal%20with%20all%20this%20complexity). Our goal is to dramatically improve our developers' fluency in database development by introducing modern **DevOps practices for the database**, centered on SSDT, DACPAC deployments, and Azure DevOps pipelines. The following sections provide a deep-dive into these elements and an upskilling roadmap (beginner → intermediate → advanced) to reduce friction for our team.

## Embracing SSDT and Schema Versioning

**SQL Server Data Tools (SSDT)** is Microsoft's toolkit for database development in Visual Studio. It allows us to **design and deploy SQL database schemas using a project-based approach**, similar to how one would develop application code[\[4\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=SQL%20Server%20Data%20Tools%20,projects%20with%20the%20SqlPackage%20CLI). In essence, you create a _SQL Database Project_ in Visual Studio and define all your database objects (tables, views, stored procedures, etc.) as create scripts within that project. This introduces a **declarative model** for the database schema that can be version-controlled and integrated into CI/CD pipelines. SSDT treats the database schema like code: you get features like offline editing with IntelliSense, schema validation on build, refactoring support, and source control integration for team collaboration[\[5\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=Developers%20can%20use%20the%20familiar,SQL%20Database%20and%20SQL%20Server)[\[6\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=Transact,SQL%20Database%20and%20SQL%20Server).

A core concept in SSDT-based development is the **Data-tier Application Package**, or **DACPAC**. A DACPAC is a single file that contains the entire schema of the database (and optionally some reference data) in a portable form[\[7\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=DACPAC%20is%20an%20abbreviation%20of,project%2C%20is%20enabled%20to%20version). When you build an SSDT project, it produces a DACPAC file as the artifact. This DACPAC encapsulates all the SQL schema elements from your project and can be used to **deploy those schema changes** to any target SQL Server 2019 instance or Azure SQL Database[\[8\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=A%20DACPAC%20file%20is%20generated,Necessary%20changes%20are). The deployment process uses a _model comparison_ approach: the DACPAC's content is compared to the target database's current schema, and a script of _necessary changes_ is generated to update the target to match the DACPAC (i.e., to reach the latest schema version)[\[9\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=changes%2C%20being%20easily%20applied%20to,schema%20reach%20the%20latest%20version). This ensures that only the incremental differences are applied (in other words, deployment is **state-based** and computes a differential script[\[10\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Key%20feature%20of%20DACPAC%20deployment,and%20error%20free%20as%20possible)).

**Why use SSDT & DACPAC for schema versioning?** Adopting SSDT brings multiple benefits for our team: - **Schema Consistency Across Environments:** By deploying the same DACPAC to Dev, Test, Prod, we ensure each environment's schema is uniform[\[11\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=1,errors%20appear%20in%20an%20SQL). No more "works on Dev but not on Prod" due to missing table changes. - **Version Control and Change Tracking:** The database project can be managed in Git (Azure Repos). Developers can branch and merge schema changes just like application code, with full history. SSDT + DACPAC effectively enables treating "database as code," so changes are tracked, code-reviewed, and can be rolled back if needed[\[12\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Server%20Data%20Tools%20,becomes%20a%20piece%20of%20cake)[\[11\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=1,errors%20appear%20in%20an%20SQL). - **Automation and CI/CD Integration:** Schema changes can be built and tested automatically, then deployed through pipelines. DACPACs are naturally suited for CI/CD - you can compile and deploy them with automated tools, fitting into our Azure DevOps pipeline for frictionless, repeatable deployments[\[11\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=1,errors%20appear%20in%20an%20SQL)[\[13\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=The%20Revolutionary%20Impact%20of%20DACPAC,reduce%20downtime%20and%20deployment%20difficulties). Microsoft's guidance even calls DACPAC _"the simplest way to deploy a database"_ in pipelines[\[14\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=DACPAC). - **Reduced Errors and Safer Deployments:** SSDT build process will catch syntax errors or invalid object references at design time. The DACPAC deployment will warn of conflicts or data loss (for example, dropping a column) before execution. This systematic approach, along with built-in scripting of changes, greatly reduces the chance of manual error during deployments[\[15\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=deployments%2C%20which%20it%20fits%20into,errors%20appear%20in%20an%20SQL)[\[13\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=The%20Revolutionary%20Impact%20of%20DACPAC,reduce%20downtime%20and%20deployment%20difficulties). In short, it brings rigor to database changes comparable to our application code practices. - **Faster Onboarding for Devs:** While initially new to our OutSystems developers, SSDT's visual editors and familiar Visual Studio interface can actually make SQL schema work more approachable. It provides a friendly environment to edit tables or write scripts with IntelliSense and designers[\[16\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=Developers%20can%20use%20the%20familiar,supported%20SQL%20platforms%2C%20such%20as)[\[17\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=objects%20like%20SQL%20Server%20Management,the%20SQL%20Server%20Object%20Explorer). Over time, this can lower the learning curve compared to hand-maintaining SQL upgrade scripts.

By embracing SSDT and DACPAC-based schema versioning, our team will be able to **move faster and with more confidence** when making database changes. The next sections outline how to get started (for beginners) and then provide guided steps for common tasks, followed by more advanced considerations as the team matures in these practices.

## Getting Started: Basic Upskilling for SSDT and Database DevOps

This section covers the foundational steps and knowledge areas to bring our OutSystems-first developers up to a _beginner/intermediate_ level with database schema development and DevOps. Initially, the focus is on understanding the tools and performing simple, controlled schema changes.

**1\. Set Up the Tools - Visual Studio and SSDT:** Each developer should install **Visual Studio 2019 or 2022** (Community edition is fine) with the _Data storage and processing_ workload, which includes SQL Server Data Tools. SSDT is essentially a Visual Studio feature that may require a separate install or extension in VS 2019; in VS 2022 it can be added via the installer[\[18\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=The%20core%20of%20SQL%20Server,SSDT%29%20for%20Visual%20Studio). Ensure the developers also have access to a SQL Server 2019 instance for development - this could be the shared development database or a local SQL Express/LocalDB for experimentation. (Microsoft provides SQL Server Developer Edition for free for development use[\[19\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=To%20complete%20the%20deployment%20of,on%20Windows%20or%20in%20containers).) Having a local instance can be useful for quick testing of deployments, though in an OutSystems context, developers might deploy to a shared dev DB that OutSystems is connected to.

**2\. Understand SQL and Database Fundamentals:** While our team has _working SQL knowledge_, we should reinforce a few key database concepts to ensure everyone speaks the same language: - **Data Definition Language (DDL):** Creating or altering tables, defining primary/foreign keys, indexes, etc. - developers should be comfortable reading and writing basic DDL statements (CREATE/ALTER/DROP). SSDT will generate these, but understanding them helps in reviewing changes. - **Relational Design Basics:** Briefly upskill on normalization, data types, and relationships. OutSystems abstracts this through Entities and Relationships, but when working directly one must manually design these aspects. Emphasize thinking about nullability, defaults, and referential integrity when creating new tables or columns. - **Transactions and Deployment Safety:** Explain that schema changes, especially those affecting data (e.g., dropping a column), need careful consideration. In practice, DACPAC deployment will wrap changes in a transaction by default for safety. Nonetheless, developers should learn to plan for preserving data (we will cover advanced migration scripts later).

We can leverage internal experts to host short knowledge-sharing sessions or recommend concise online modules for the above. Microsoft's documentation and SQL tutorials can be good self-study material. At this stage, the aim is not to turn everyone into a DBA overnight, but to establish a comfort level with terminology and the SSMS/SSDT environment.

**3\. Create Your First SQL Database Project:** Hands-on practice is crucial. Have each developer (or pair with a mentor) create an SSDT project for our database: - Launch Visual Studio and select **File > New > Project**, then choose **SQL Server Database Project** (targeting SQL Server 2019 platform)[\[20\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=as%20using%20the%20schema%20comparison,tools)[\[21\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=In%20the%20New%20Project%20dialog,style%20%28preview). Give it a name (it could correspond to the database name or a functional name). - **Import the Existing Schema:** Since the database likely already exists (perhaps initially set up by an expert or containing current production schema), use the Schema Compare tool or _"Import from Database"_ wizard to pull in the current schema. Visual Studio's SQL Server Object Explorer or Schema Compare can connect to the SQL Server 2019 instance, read all tables, views, etc., and import them into the project as create scripts. This establishes a baseline. After import, the project will have a folder structure (e.g., Tables, Stored Procedures) populated with .sql files for each object. - **Build the Project:** Do a Build in Visual Studio. On successful build, it will produce a **.dacpac** file (visible in the project's \\bin\\Debug or \\bin\\Release folder). This verifies that the project is self-consistent. (Any errors would indicate issues like unresolved references or syntax errors that need fixing now.) The build output confirms we have a DACPAC artifact representing the database. _"If your database project is configured properly it will produce a DACPAC file which becomes the main artifact containing the definition of the entire database,"_ as one CI/CD guide notes[\[22\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=This%20task%20will%20build%20the,Configuration%20is%20set%20to%20release).

**4\. Source Control the Database Project:** Just like our OutSystems application modules are managed (OutSystems has its own platform for module versioning and deployment), our new database project should be placed under source control (e.g., an Azure DevOps Git repository). Commit the initial project files (the imported create scripts, the .sqlproj file, etc.). From here on, every schema change will be done by editing this project and committing the change - this gives us traceability. Developers should practice typical Git workflows with the database project: branching for a feature that requires a DB change, committing the updates, and merging via pull requests. This is where internal experts can enforce best practices (code review each database change for correctness and quality, just as they would review code). It might be helpful to define some conventions (for instance, naming conventions for new tables/columns, or how to structure folders in the project for complex DBs) and document these for the team.

**5\. Basic Schema Editing in SSDT:** Now that the project is set up, developers can try making a simple schema change in SSDT to get familiar with the process: - For example, **add a new table**: In Solution Explorer, right-click the Tables folder > Add New Item > Table. A T-SQL template appears for creating a table. They can fill in columns, data types, primary key, etc. (There's also a visual table designer if preferred, but editing the script is good practice). Similarly, one could add a new column to an existing table by editing its create script. - **Validate locally:** Build the project again to ensure the change doesn't introduce errors. Optionally, deploy the change to a local dev database for immediate testing: Visual Studio allows publishing the project to a database. This can be done by right-clicking the project and choosing Publish, then providing a connection to (for instance) a local SQL instance or the Dev environment database. This will apply the new table/column to that database via the DACPAC mechanism. Developers can check in SQL Server Management Studio (SSMS) that the new table/column appears. - This local publish is mainly for quick feedback and testing. In a team setting with a shared Dev DB, you might skip direct publishes from VS and instead rely on the automated pipeline to deploy changes (to avoid one developer inadvertently changing the shared dev DB outside of source control). That said, encouraging exploration in a sandbox database is fine for learning.

At this stage, our OutSystems developers should grasp the basics: how an SSDT project represents the DB schema, how to make and test changes in it, and the importance of using the project (not manual DB edits) as the single source of truth. They are essentially at the intermediate level, able to perform simple schema versioning. Next, we introduce **just-in-time guides** for the typical workflows they will need, integrating this with Azure DevOps for full CI/CD automation.

## Just-in-Time Guides for Common Tasks

When working on real deliverables, developers will often need quick, focused guidance on specific tasks - from making a schema change to deploying it safely. This section provides **step-by-step "cheat sheet" guides** that can be consulted at the moment of need. Each guide assumes the basics (above) are understood, and it emphasizes practical execution. The guides are ordered roughly in the sequence a feature might flow (design -> build -> deploy -> integrate with OutSystems), but each can be looked up individually as needed:

- **Guide: Implementing a Schema Change (e.g., new table or column)** - _How to modify the database schema using SSDT._
- **Plan the Change:** Determine what needs to be changed or added. For example, adding a new table to support a feature, or altering a column to a different data type. Involve an internal DB expert if the change affects existing data or has performance considerations (for instance, altering a large table).
- **Edit in the SSDT Project:** Open the database project in Visual Studio. To add a new database object, right-click the appropriate folder (Tables, Views, etc.) and choose **Add > New Item**. Select the object type (Table, etc.) and fill out the template script with the new schema definition. For altering an existing object (like adding a column), open its .sql file in the project and modify the CREATE statement (since SSDT stores the "CREATE Table" statement representing the desired final state).
- **Reference OutSystems Naming/Usage:** If this database is accessed by OutSystems, ensure the naming aligns with any OutSystems queries or entity naming conventions. For example, OutSystems Integration Studio might prepend schema names or you might choose to use a specific schema/catalog if using the Multiple Catalogs feature. Keep names descriptive and avoid SQL reserved words.
- **Build and Verify:** Build the project (Debug or Release configuration). Fix any errors. Common issues might include forgetting a comma between columns, or if you added a foreign key, the referenced table might need to exist or be included in project (order doesn't matter for build, but the ref must resolve). The successful build produces an updated DACPAC which now includes your new changes.
- **(Optional) Test Locally:** If feasible, publish the project to a local dev database to see the change applied. Alternatively, you can generate a _Publish Script_ (in VS publish dialog, choose "Generate Script") - review the SQL script to ensure it does what you expect (e.g., a CREATE TABLE statement for your new table, or an ALTER TABLE for your changed column). This script review is a good practice to catch unintended drops or modifications.
- **Guide: Setting Up Continuous Integration (CI) Pipeline in Azure DevOps** - _Automating the build of the database project and artifact creation._  
    In Azure DevOps (ADO), we create a build pipeline so that every commit (or at least every merge to main) of the database project triggers a build and produces a DACPAC artifact. This ensures rapid feedback and an auditable build output. For simplicity, you can use a classic pipeline or a YAML pipeline - we'll outline the key tasks, which apply to either approach:

_An example Azure DevOps build pipeline for an SSDT project, containing tasks to build the database project, copy the DACPAC file, and publish it as an artifact._

- **Build Task:** Use the **Visual Studio Build** task (or in YAML, an MSBuild task) to build the solution containing the .sqlproj. This will compile the database project. If everything is configured correctly, this step will output a DACPAC file - _"the main artifact containing the definition of the entire database"_[\[22\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=This%20task%20will%20build%20the,Configuration%20is%20set%20to%20release). Make sure the build agent has the required VS/SSDT installed or use the hosted VS2019/VS2022 agent image which includes MSBuild and SQLProj support. In YAML, this might look like:  

- \- task: VSBuild@1  
    inputs:  
    solution: '\*\*\\\*.sqlproj'  
    platform: 'Any CPU'  
    configuration: 'Release'
- (This finds the .sqlproj and builds it.)

- **Copy DACPAC Artifact:** After build, add a **Copy Files** task to pick up the generated .dacpac (e.g., from the project's bin\\Release folder) and copy it to the pipeline's staging directory. For example, copy \*\*\\bin\\Release\\\*.dacpac to \$(Build.ArtifactStagingDirectory). This prepares the DACPAC for publishing.
- **Publish Artifact:** Use the **Publish Build Artifacts** task to store the DACPAC (and any other outputs) as a pipeline artifact. Name the artifact e.g. "drop". This artifact will be used by the release/deployment pipeline. In YAML:  

- \- task: PublishBuildArtifacts@1  
    inputs:  
    PathtoPublish: '\$(Build.ArtifactStagingDirectory)'  
    ArtifactName: 'drop'

- **(Optional) Schema Differences Report:** For extra insight, we can include a step to generate a schema changes report between builds. A tool like **Colin's ALM Corner - Dacpac Schema Compare** can produce an HTML summary of what changed since the last DACPAC[\[23\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=DacPac%20Schema%20Compare)[\[24\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=You%20will%20find%20two%20reports,messed%20around%20with%20it%20manually). This is optional but helpful for developers to verify their changes. It requires installing that extension in the pipeline and adding the task. If used, ensure **Allow Scripts to Access OAuth Token** is enabled on the build agent (so it can fetch the previous artifact for comparison)[\[25\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=You%20must%20enable%20%C2%A0the%20Allow,as%20you%20can%20see%20below). The result will show a diff of schema changes which can be attached to the build summary[\[24\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=You%20will%20find%20two%20reports,messed%20around%20with%20it%20manually).
- **Trigger and Verification:** Set the pipeline to trigger on pushes to your main branch (or feature branches as needed). Once set up, test it by committing a change. The build should run and you should see the artifact DACPAC published. Verify that the artifact is downloadable and indeed contains the updated schema (you can examine a DACPAC by unpacking it or using Schema Compare against a database, though this is more for curiosity).
- **Guide: Setting Up Continuous Deployment (CD) - Database Release Pipeline** - _Automating deployment of the DACPAC to SQL Server 2019 through Azure DevOps._  
    With CI producing a DACPAC artifact, the next step is a release pipeline that takes that artifact and deploys it to target environments (Dev, Test, Prod). We can configure this with Azure Pipelines release stages or as a multi-stage YAML pipeline. Key steps for the release (CD) process:
- **Define Pipeline Stages per Environment:** For example, Stage 1: Dev, Stage 2: Test, Stage 3: Prod. In classic Release pipelines, you link the artifact from the CI build and create these stages. In YAML, you'd define multiple stages with conditionals or triggers. Typically, Dev might deploy continuously on each build (or on demand), while Test/Prod stages could be manual or require approvals.
- **Database Deployment Task:** Azure DevOps provides a built-in **Azure SQL Database Deployment** task (SqlAzureDacpacDeployment@1) which is ideal if deploying to Azure SQL. However, since our target is SQL Server 2019 (which might be on-prem or an Azure VM), we can still use DACPAC deployment via **SqlPackage**. Options:
  - If the SQL Server is accessible from Azure Pipelines (e.g., through a VPN or it's an Azure SQL), you can use the Azure SQL deployment task by providing a server name, database name, and credentials[\[26\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=To%20deploy%20a%20DACPAC%20to,snippet%20to%20your%20YAML%20file). This task essentially runs SqlPackage under the hood. Microsoft states: _"The simplest way to deploy a database is by using a DACPAC... package and deploy schema changes"_[\[14\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=DACPAC). In YAML, it looks like:  

  - \- task: SqlAzureDacpacDeployment@1  
        inputs:  
        azureSubscription: '&lt;ServiceConnectionName&gt;'  
        ServerName: '&lt;server_name&gt;'  
        DatabaseName: '&lt;db_name&gt;'  
        SqlUsername: '&lt;username&gt;'  
        SqlPassword: '&lt;password&gt;'  
        DacpacFile: '\$(Pipeline.Workspace)/drop/yourproject.dacpac'
  - Note: Despite the name "Azure SQL", this task can target a SQL Server if network allows, or you might use a similar **SqlDacpacDeploy** task. If using on-prem servers, you likely need a self-hosted build agent that can reach the DB, or ensure firewall rules allow Azure DevOps agent IPs.
  - Alternatively, use a **PowerShell** task to run SqlPackage.exe manually. SqlPackage is a command-line tool that comes with SQL Server or can be downloaded. For example:  

  - SqlPackage.exe /Action:Publish /SourceFile: "\$(Pipeline.Workspace)\\drop\\your.dacpac" /TargetServerName: "YOUR_SQL_SERVER" /TargetDatabaseName: "YOUR_DB" /TargetUser:"user" /TargetPassword:"pass" /p:BlockOnPossibleDataLoss=false
  - (The BlockOnPossibleDataLoss parameter, if true, will prevent deployment if data loss might occur - you may set it to false only if you have planned for that data change or handled it via script).
- **Configuration per Environment:** Use pipeline variables or variable groups to store environment-specific info (server names, creds). Never hard-code credentials; use Azure DevOps **Service Connections** or **Secret variables** to store passwords securely. For on-prem, consider setting up an **Azure DevOps self-hosted agent** on a machine that has network access to the SQL Server - this way deployment can run behind the firewall. If using Azure SQL, set up an Azure Service Connection and grant it appropriate rights.
- **Run Deployment and Monitor:** When the pipeline runs, the deployment task will compare the DACPAC to the target DB and apply changes. It's a good practice to **review the deployment report** that SqlPackage outputs. This can usually be found in the pipeline logs or can be turned on to write an output script. It will list the SQL operations executed (e.g., "Creating table X", "Altering column Y"). Have the team habitually check this, especially for Prod, to ensure no unexpected drops or modifications. Successful completion means the target DB is now updated to the new schema version.
- **Post-Deployment Verification:** After deployment to an environment, developers or DBAs should run smoke tests. For instance, if a new table was added, try inserting and querying it via an OutSystems screen or an SSMS query. If data migration was expected (not handled by DACPAC alone), confirm the data is in the expected state. In automated pipelines, you could include a basic post-deployment test step (like running a T-SQL script to verify a critical table exists or row count matches, etc., and fail the pipeline if not).

Additionally, integrate this DB deployment pipeline with the OutSystems deployment process. For example, when releasing a new OutSystems module version, coordinate so that the database stage (via Azure DevOps) runs before or in parallel, ensuring the database is ready by the time the OutSystems application is live. Azure DevOps can even trigger OutSystems deployments if using the OutSystems Deployment API or the OutSystems pipeline integration[\[27\]](https://www.outsystems.com/forums/discussion/69187/trigger-pipeline-ci-cd-pipeline-with-azure-devops-deploying-main-application/#:~:text=,present%20on%20your%20target%20environment), but at minimum, communicate the order of deployment in runbooks: database changes usually should be applied before the app tries to use them (e.g., create a new table first, then the OutSystems code that uses it).

- **Guide: Updating OutSystems to Use the Latest Schema** - _Refreshing external entities or adjusting OutSystems modules after a DB change._  
    Once the external SQL Server schema is updated (through the above pipeline in, say, the Dev environment), OutSystems needs to be made aware of those changes:
- **Refresh External Entities:** In Integration Studio, open the extension module that contains the external table mappings. Use the **"Refresh"** or **"Import Table"** functionality to update the entity definition. For example, if you added a new column to a table, the Integration Studio wizard will show that difference and allow you to import the updated schema for that entity. Similarly, if a new table was added and you want to expose it, you'll need to go through the "Connect to External Table or View" wizard again to add that table to the extension[\[28\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Using%20OutSystems%20Integration%20Studio%2C%20an,to%20access%20the%20external%20tables%2Fviews)[\[29\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=After%20selecting%20a%20connection%2C%20a,selected%20from%20the%20available%20databases). After refreshing or adding, **publish** the updated extension module to the OutSystems environment. This step regenerates the integration layer so that OutSystems knows about the new/changed database structure.
- **Adjust OutSystems Application Logic:** Now, in Service Studio, you may need to adjust application modules that use the external entities. For a new table, you might create new screens or logic to handle it; for a changed column, you need to verify any aggregates or SQL queries in OutSystems that touched the old column. OutSystems will typically mark references as outdated if the external entity changed - you should see warnings and need to refresh references in consuming modules. Do that and test the app's functionality against the new DB schema.
- **Just-in-Time Learning:** If developers are not familiar with Integration Studio, note that it's the OutSystems tool for managing integrations, including external databases. They should consult OutSystems documentation on how to use it, but the core idea is covered: define the connection in Service Center (admins likely set this up once) and then use Integration Studio to manage the entities. OutSystems documentation provides detailed steps on this process[\[1\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=A%20database%20connection%20can%20be,Connections%E2%80%9D%20is%20available%20in%20Administration)[\[2\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Using%20OutSystems%20Integration%20Studio%2C%20an,to%20access%20the%20external%20tables%2Fviews).
- **Deployment Between Environments:** One caveat from experience - when you move the OutSystems application (and its extension module) from Dev to Test to Prod via OutSystems Lifetime or deployment plans, the external database connection might differ (e.g., Dev points to Dev DB, Prod to Prod DB). Ensure the extension is configured without hard-coding a database name so it can bind to the correct connection in each environment[\[30\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Despite%20having%20the%20correct%20database,not%20in%20the%20current%20one)[\[31\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Keeping%20only%20the%20tables%E2%80%99%20name%2C,database%20connection%20in%20each%20environment). OutSystems recommends leaving the "catalog" name blank in the entity definitions to avoid cross-environment issues[\[32\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Changing%20the%20table%20name%2C%20as,the%20extension%20to%20different%20environments). This is just something to watch out for to reduce friction during promotion of releases.
- **Guide: Handling Data Migration or Complex Changes** - _Advanced guide (for later, but worth having handy) on schema changes that involve data transformation._  
    Not all changes are as simple as "add a new table" or "add a column." Sometimes we need to e.g. split a column into two, migrate data from one table to another, or other refactoring that DACPAC can't automatically know about. For such cases:
- **Pre-Deployment and Post-Deployment Scripts:** SSDT projects support special scripts that run before or after the main schema changes. Use a pre-deployment script for preparatory steps (e.g., copy data from a column before a table is rebuilt) and a post-deployment script for cleanup or data seeding. These scripts are included in the project and will execute in the context of the target database during deployment. For example, if you need to migrate data out of a column before dropping it, you might use a pre-deployment script to save that data to a temp table, then let the DACPAC drop the column, and finally a post-deployment script to perhaps reinsert or adjust the saved data into a new structure. This requires T-SQL coding, which our more advanced developers or DB experts can help with.
- **Reference/Lookup Data:** If your application relies on certain static lookup data (for instance, a table of statuses or configurations), consider versioning that data as part of deployment. SSDT doesn't automatically include data changes (it's schema-focused), but you can manage it via post-deployment insertion scripts or use a tool like the SSDT data population feature. This ensures, for example, if a new code "XYZ" must exist in a lookup table for the app to work, your deployment pipeline inserts it. This avoids manual data fix steps outside the pipeline.
- **Testing Migrations:** Emphasize that any time data movement is involved, it should be tested on a lower environment with production-like data volume, if possible. Developers should practice writing idempotent scripts (scripts that can run multiple times without harm) for these cases, or scripts that check for conditions (e.g., only populate data if it doesn't exist).

These guides should be used as living documents - the team can extend them as we encounter new scenarios. They represent a "just in time" help system: e.g., when a dev is about to set up a pipeline, they refer to the CI/CD guide; when adding a new DB column for a feature, refer to the schema change guide, etc. The goal is to empower developers to perform these tasks with low friction, knowing they have a reference to follow.

## Advanced Practices and Considerations

Once the team is comfortable with the basics of SSDT and the CI/CD pipeline, we can elevate our practices to an advanced level. This ensures our database DevOps is robust, scalable, and aligned with both OutSystems and enterprise best practices. Here are some advanced elements to consider:

**Branching and Release Strategy for DB Changes:** In code, we might use feature branches, pull requests, etc. For database changes, a similar strategy should be in place. Encourage developers to isolate their schema changes in feature branches and merge when ready. However, be cautious: if multiple branches make conflicting DB changes (e.g., two branches edit the same table), merging can be tricky. It's best to communicate major schema changes early and perhaps serialize their integration. Use pull request reviews with our internal experts to catch issues (e.g., "You added a nullable column but didn't provide a default - is that intended?"). One approach is to have a **DB architect** or experienced SQL dev as an approver on all database PRs for oversight.

**Continuous Integration for OutSystems and DB Together:** Our focus is on the DB side, but note that OutSystems itself can be integrated with external version control and CI to an extent[\[33\]](https://success.outsystems.com/documentation/how_to_guides/devops/how_to_build_an_outsystems_continuous_delivery_pipeline/#:~:text=How%20to%20build%20an%20OutSystems,as%20governance%20over%20which). In an ideal DevOps process, you might trigger the OutSystems app build/deployment and the DB deployment in coordination. OutSystems provides LifeTime APIs and even an OutSystems Azure DevOps extension for automated deployments[\[34\]](https://devblogs.microsoft.com/devops/top-stories-from-the-vsts-community-2017-11-24/#:~:text=Top%20stories%20from%20the%20VSTS,an%20OutSystems%20Integration%20that). For advanced pipeline integration, consider using the Azure DevOps pipeline to orchestrate both: e.g., first deploy DACPAC to Test DB, then call OutSystems Deployment API to deploy the application to Test environment. This ensures a synchronized release. This is an extensive topic on its own, but worth exploring as we mature (it can drastically reduce the manual steps in releasing an OutSystems app that depends on DB changes).

**Performance and Optimization:** As the team's SQL skills improve, they should also learn about indexing strategies and query performance. OutSystems queries (especially if using Aggregates in Service Studio) will generate SQL under the hood; when dealing with large data in an external DB, developers might need to create indexes to support those queries. Using SSDT, they can add indexes or even filtered indexes as needed. They should also understand how to interpret execution plans in SQL Server for any custom queries they write (e.g., in Advanced SQL nodes in OutSystems). We could arrange an advanced session on SQL performance tuning once basics are solid, to avoid any new DB changes inadvertently slowing the system.

**Error Handling and Rollback Plans:** With automated deployments, if something goes wrong, what's the plan? For instance, deploying a change that accidentally drops a needed column could be disastrous. Our pipeline should be configured to **stop on any SQL errors**, but logical issues might slip through. For critical changes, take backups of the database (or at least the affected table data) before deployment - this might be a manual DBA step or a script in the pipeline (for example, export data to storage). Know that DACPAC doesn't provide automatic rollback if a change is committed; you'd either restore a backup or deploy a fixing script. In advanced use, one could implement a "blue-green" database deployment or use feature flags to enable new features only after verifying DB updates. These are complex in relational DB context, but the concept is to minimize downtime and risk.

**Environment Drift and Manual Changes:** It's important to instill discipline that **all changes go through SSDT and the pipeline**. If someone makes a hotfix directly in production DB (e.g., a quick add of an index or urgent patch), that change must be back-ported into the SSDT project; otherwise, our source of truth diverges. Using the schema compare tool against Prod can help detect drift. Some teams schedule regular drift checks. Our internal DB experts can periodically compare the Prod schema to the latest project DACPAC to ensure they match (or better yet, use the pipeline to do this by generating a report). We should document a procedure for emergency manual changes: who can do them, and how to reconcile afterward.

**Security and Permissions:** In an OutSystems context, the platform usually manages DB user accounts for its own usage. With an external integrated DB, OutSystems likely connects with a configured connection string (perhaps using a service account). When we deploy schema changes, the deployment user needs sufficient privileges (ALTER schema, etc.). It's best to have a dedicated **DB deployment user** with DDL rights on the target database. Store its credentials securely (in Azure DevOps service connection or Key Vault). Also, consider role-based security inside the database for the app's runtime if not already handled (though OutSystems will handle end-user security at the app level). Our developers should also be aware of SQL injection if they write any dynamic SQL in queries - treat inputs properly, etc., though in OutSystems much of that is handled by the platform.

**Testing and Quality Gates:** As we progress, we can add more quality checks. For example, implement **static code analysis** for SQL - there are tools (like SSDT has some Code Analysis rules or third-party analyzers) that can warn about bad practices (SELECT \* usage, missing primary keys, etc.). We can integrate such analysis in the build (failing the build on severe warnings). We could also introduce **automated database tests** using tSQLt (a SQL unit testing framework) for any stored procedures or complex logic in the DB. These tests could run in a pipeline against a temporary instance. This is advanced and likely beyond initial scope, but worth knowing as a future direction for truly mature DevOps.

**Documentation and Knowledge Sharing:** As the team gains experience, encourage them to document their lessons. Perhaps maintain an internal **wiki** or knowledge base for common scenarios ("How to rename a column with minimal downtime", "Checklist before deploying to production", etc.). This can include OutSystems-specific notes, e.g., "If you rename a table that OutSystems uses, you'll need to update the extension and all references - avoid doing this unless absolutely necessary." Our internal experts can spearhead this, but even newer developers can contribute as part of their learning (writing docs is a great way to solidify understanding).

**Leverage Internal Experts:** We have senior staff engineers who are experienced in these areas - we should formally designate them as mentors or reviewers for the database DevOps initiative. For example, set up a weekly office hour where anyone can bring questions about SSDT or SQL; have them do paired programming with less experienced devs on tricky tasks; have them review pipeline configurations for best practices (like ensuring no sensitive info is inadvertently logged, etc.). This not only transfers knowledge but also builds confidence in the newer developers as they have a safety net.

By covering these advanced topics, we aim to continuously improve and not stop at just "it works." Over time, our OutSystems-first developers will become well-rounded engineers comfortable in both low-code and traditional code worlds, able to navigate SQL Server optimally while still delivering rapid solutions in OutSystems. This comprehensive skillset is a big win for the team and the organization.

## Documentation and Learning Resources

To support this deep upskilling, below is a curated list of documentation, tutorials, and other resources. These can be used for self-study or referenced when implementing specific parts of our solution:

- **Official OutSystems Docs - External Databases Integration:** _OutSystems 11 documentation on how to connect to external SQL databases._ This covers configuring database connections in Service Center and using Integration Studio to import tables[\[1\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=A%20database%20connection%20can%20be,Connections%E2%80%9D%20is%20available%20in%20Administration)[\[2\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Using%20OutSystems%20Integration%20Studio%2C%20an,to%20access%20the%20external%20tables%2Fviews). It's essential reading to understand the OutSystems side of the equation (how the platform handles external data sources).
- **OutSystems Community and Articles:** The OutSystems community has posts like _"External Databases in OutSystems Environments"_ by Sara Pedrosa, which provide practical tips (e.g., avoiding including the database name in entity definitions to ease multi-environment deployments)[\[30\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Despite%20having%20the%20correct%20database,not%20in%20the%20current%20one)[\[31\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Keeping%20only%20the%20tables%E2%80%99%20name%2C,database%20connection%20in%20each%20environment). Such articles can give context-specific advice that official docs may not cover.
- **Microsoft Learn - SQL Server Data Tools (SSDT):** Microsoft's documentation on SSDT explains the project-based approach and how it integrates into Visual Studio[\[4\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=SQL%20Server%20Data%20Tools%20,projects%20with%20the%20SqlPackage%20CLI). The "What are SQL Database Projects?" and "Create and deploy a SQL project" tutorial[\[35\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=The%20development%20cycle%20of%20a,local%20development%20without%20additional%20effort) are highly recommended for step-by-step guidance in creating a project, adding objects, and setting up a basic pipeline (the MS tutorial uses GitHub Actions, but the concepts apply equally to Azure DevOps).
- **SQL Server Data Tools - Project Deployment and CI/CD:** Look into Microsoft's article _"Azure SQL Database deployment"_ on Azure DevOps Docs, which specifically notes that using DACPAC is the simplest method for DB deployment in pipelines[\[14\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=DACPAC) and provides YAML examples. This can reinforce our pipeline setup knowledge and is useful if we need to troubleshoot pipeline tasks.
- **Blogs and Community Content on Database DevOps:** A wealth of community blogs exist. Radoslav Gatev's blog series on _CI/CD of SSDT Projects_ (2019) is a great example. It walks through setting up pipelines and even advanced concepts like automated schema comparison reports[\[23\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=DacPac%20Schema%20Compare). Another modern source is Firas Sboui's _"SQL Schema Versioning, Build, and Deployment with DACPAC"_ (2024) which eloquently highlights the advantages of DACPAC in preventing common errors and ensuring consistency[\[7\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=DACPAC%20is%20an%20abbreviation%20of,project%2C%20is%20enabled%20to%20version)[\[11\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=1,errors%20appear%20in%20an%20SQL). These readings help solidify _why_ we are doing this and provide insight into real-world usage.
- **Training Courses:** If developers prefer a structured course, consider recommending Pluralsight or Microsoft Learn paths on DevOps for Databases, or an internal workshop. OutSystems University (online training) may not cover external DB in depth, but there might be Advanced courses that touch on Integration Studio usage and best practices. Microsoft's SQL workshops or SQLBits conference videos can also be good for deeper SQL knowledge.
- **Internal Knowledge Base:** We will create an internal Confluence (or similar) space for **Database DevOps**. This will include our project-specific conventions, the guides from this document (kept up-to-date as we refine our process), troubleshooting tips, and links to external docs. All developers should bookmark this. New hires can be onboarded using this as a mini curriculum.

By combining these resources with hands-on practice, our team can quickly climb the learning curve. Remember that the goal is **better fluency and lower friction** - that means no question should go unanswered. If something is confusing (be it SSDT publish settings or an OutSystems integration quirk), we document it or find an answer via these resources. In time, the team will not only be consuming documentation but contributing to it.

**In summary**, this deep research has outlined the critical elements for success: adopting SSDT and DACPAC for managing our SQL Server 2019 schema, automating deployments via Azure DevOps pipelines, and methodically upskilling a team of OutSystems developers in these new skills. We started from fundamentals and built up to advanced practices, emphasizing documentation and learning at each step. By following this plan and utilizing the guides and resources provided, our predominantly OutSystems-first developers can become confident in database changes, resulting in faster development cycles and more reliable releases. The combination of OutSystems rapid development and robust database DevOps will dramatically move our capability forward - allowing us to deliver features quickly **and** with the assurance that our data layer is well-managed, versioned, and aligned with best practices.[\[13\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=The%20Revolutionary%20Impact%20of%20DACPAC,reduce%20downtime%20and%20deployment%20difficulties)[\[14\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=DACPAC)

[\[1\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=A%20database%20connection%20can%20be,Connections%E2%80%9D%20is%20available%20in%20Administration) [\[2\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Using%20OutSystems%20Integration%20Studio%2C%20an,to%20access%20the%20external%20tables%2Fviews) [\[28\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Using%20OutSystems%20Integration%20Studio%2C%20an,to%20access%20the%20external%20tables%2Fviews) [\[29\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=After%20selecting%20a%20connection%2C%20a,selected%20from%20the%20available%20databases) [\[30\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Despite%20having%20the%20correct%20database,not%20in%20the%20current%20one) [\[31\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Keeping%20only%20the%20tables%E2%80%99%20name%2C,database%20connection%20in%20each%20environment) [\[32\]](https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574#:~:text=Changing%20the%20table%20name%2C%20as,the%20extension%20to%20different%20environments) External Databases in OutSystems Environments | by Sara Pedrosa | Noesis Low-Code Solutions | Medium

<https://medium.com/noesis-low-code-solutions/external-databases-in-outsystems-environments-122e97fa2574>

[\[3\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=I%20have%20seen%20projects%20that,deal%20with%20all%20this%20complexity) [\[22\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=This%20task%20will%20build%20the,Configuration%20is%20set%20to%20release) [\[23\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=DacPac%20Schema%20Compare) [\[24\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=You%20will%20find%20two%20reports,messed%20around%20with%20it%20manually) [\[25\]](https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/#:~:text=You%20must%20enable%20%C2%A0the%20Allow,as%20you%20can%20see%20below) CI/CD of SSDT Projects: Part 2, Creating Azure DevOps pipelines

<https://www.gatevnotes.com/continuous-integration-continuous-deployment-of-ssdt-projects-creating-azure-devops-pipelines/>

[\[4\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=SQL%20Server%20Data%20Tools%20,projects%20with%20the%20SqlPackage%20CLI) [\[5\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=Developers%20can%20use%20the%20familiar,SQL%20Database%20and%20SQL%20Server) [\[6\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=Transact,SQL%20Database%20and%20SQL%20Server) [\[16\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=Developers%20can%20use%20the%20familiar,supported%20SQL%20platforms%2C%20such%20as) [\[17\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=objects%20like%20SQL%20Server%20Management,the%20SQL%20Server%20Object%20Explorer) [\[18\]](https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17#:~:text=The%20core%20of%20SQL%20Server,SSDT%29%20for%20Visual%20Studio) SQL Server Data Tools - SQL Server Data Tools (SSDT) | Microsoft Learn

<https://learn.microsoft.com/en-us/sql/ssdt/sql-server-data-tools?view=sql-server-ver17>

[\[7\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=DACPAC%20is%20an%20abbreviation%20of,project%2C%20is%20enabled%20to%20version) [\[8\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=A%20DACPAC%20file%20is%20generated,Necessary%20changes%20are) [\[9\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=changes%2C%20being%20easily%20applied%20to,schema%20reach%20the%20latest%20version) [\[10\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Key%20feature%20of%20DACPAC%20deployment,and%20error%20free%20as%20possible) [\[11\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=1,errors%20appear%20in%20an%20SQL) [\[12\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=Server%20Data%20Tools%20,becomes%20a%20piece%20of%20cake) [\[13\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=The%20Revolutionary%20Impact%20of%20DACPAC,reduce%20downtime%20and%20deployment%20difficulties) [\[15\]](https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1#:~:text=deployments%2C%20which%20it%20fits%20into,errors%20appear%20in%20an%20SQL) SQL Schema Versioning, Build, and Deployment with DACPAC | by Firas Sboui | Medium

<https://medium.com/@firaassboui/sql-schema-versioning-build-and-deployment-with-dacpac-b106a603ded1>

[\[14\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=DACPAC) [\[26\]](https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops#:~:text=To%20deploy%20a%20DACPAC%20to,snippet%20to%20your%20YAML%20file) Deploy to Azure SQL Database - Azure Pipelines | Microsoft Learn

<https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-sqldb?view=azure-devops>

[\[19\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=To%20complete%20the%20deployment%20of,on%20Windows%20or%20in%20containers) [\[20\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=as%20using%20the%20schema%20comparison,tools) [\[21\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=In%20the%20New%20Project%20dialog,style%20%28preview) [\[35\]](https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17#:~:text=The%20development%20cycle%20of%20a,local%20development%20without%20additional%20effort) Create and Deploy a SQL Project - SQL Server | Microsoft Learn

<https://learn.microsoft.com/en-us/sql/tools/sql-database-projects/tutorials/create-deploy-sql-project?view=sql-server-ver17>

[\[27\]](https://www.outsystems.com/forums/discussion/69187/trigger-pipeline-ci-cd-pipeline-with-azure-devops-deploying-main-application/#:~:text=,present%20on%20your%20target%20environment) \[Trigger Pipeline\] CI/CD Pipeline with Azure DevOps - OutSystems

<https://www.outsystems.com/forums/discussion/69187/trigger-pipeline-ci-cd-pipeline-with-azure-devops-deploying-main-application/>

[\[33\]](https://success.outsystems.com/documentation/how_to_guides/devops/how_to_build_an_outsystems_continuous_delivery_pipeline/#:~:text=How%20to%20build%20an%20OutSystems,as%20governance%20over%20which) How to build an OutSystems continuous delivery pipeline

<https://success.outsystems.com/documentation/how_to_guides/devops/how_to_build_an_outsystems_continuous_delivery_pipeline/>

[\[34\]](https://devblogs.microsoft.com/devops/top-stories-from-the-vsts-community-2017-11-24/#:~:text=Top%20stories%20from%20the%20VSTS,an%20OutSystems%20Integration%20that) Top stories from the VSTS community - 2017.11.24 - Azure DevOps ...

<https://devblogs.microsoft.com/devops/top-stories-from-the-vsts-community-2017-11-24/>
